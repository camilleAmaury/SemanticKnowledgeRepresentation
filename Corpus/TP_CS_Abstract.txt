(2 + 1)-dimensional non-linear optical waves through the coherently excited resonant medium doped with the erbium atoms can be described by a (2 + 1)-dimensional non-linear Schrodinger equation coupled with the self-induced transparency equations.
 For such a system.
(Objective) In order to increase classification accuracy of tea-category identification (TCI) system.
[Purpose] This study verified that the smoothness of reaching movements is able to quantitatively evaluate the effects of two-and three-dimensional images on movement in healthy people.
 In addition.
 Camera trapping is a widely applied method to study mammalian biodiversity and is still gaining popularity.
 It can quickly generate large amounts of data which need to be managed in an efficient and transparent way that links data acquisition with analytical tools.
 We describe the free and open-source R package camtrapR.
 Species distribution modelling can be useful for the conservation of rare and endangered species.
 Freshwater mussel declines have thinned species ranges producing spatially fragmented distributions across large areas.
 Spatial fragmentation in combination with a complex life history and heterogeneous environment makes predictive modelling difficult.
 A machine learning approach (maximum entropy) was used to model occurrences and suitable habitat for the federally endangered dwarf wedgemussel.
 While phylogenies have been getting easier to build.
2008 is the year of creation of the Bitcoin network consisting of a decentralized database.
2-dimensional (2D) nanosheets such as graphene.
3D digital visualization technology is a new research field along with the rapid development of computer technology.
 It is a multi-scale technology which consists of computer graphics.
3D finite-element (FE) mesh generation is a major hurdle for marine controlled-source electromagnetic (CSEM) modeling.
 In this paper.
3D human pose estimation from a single image is an important problem in computer vision with a number of applications.
3D mesh segmentation is considered an important process in the field of computer graphics.
 It is a fundamental process in different applications such as shape reconstruction in reverse engineering.
3D meshes are deployed in a wide range of application processes (e.
3D Model Reconstruction is of the most important part in the field of Reverse Engineering.
 It has now become feasible to use this method to create a 3D model of existing product.
3D modeling is used in Computer Graphics in various fields.
 Since the growth of gestures.
3D models of humans are commonly used within computer graphics and vision.
3D segmentation methods and their evaluation are important problems in computer graphics.
 Many 3D segmentation techniques are available in the literature.
5G mobile networks are promising to offer mobile users unrivaled experiences with infinite networking capability at any period and from anywhere.
A (3 + 1)-dimensional coupled nonlinear Schrodinger system is investigated.
A (3+1)-dimensional B-type Kadomtsev-Petviashvili equation.
A 3-D network-on-chip (NoC) enables the design of high performance and low power many-core chips.
 Existing 3-D NoCs are inadequate for meeting the ever-increasing performance requirements of many-core processors since they are simple extensions of regular 2-D architectures and they do not fully exploit the advantages provided by 3-D integration.
A basic question in computational geometry is how to find the relationship between a set of points and a line in a real plane.
 In this paper.
A behavioural theory consists of machine-independent postulates characterizing a particular class of algorithms or systems.
A blind digital image watermarking scheme based on spatial domain is presented and investigated in this paper.
 The watermark has been embedded in intermediate significant bit planes besides the least significant bit plane at the address locations determined by pseudorandom address vector (PAV).
 The watermark embedding using PAV makes it difficult for an adversary to locate the watermark and hence adds to security of the system.
 The scheme has been evaluated to ascertain the spatial locations that are robust to various image processing and geometric attacks JPEG compression.
A central idea in distance-based machine learning algorithms such k-nearest neighbors and manifold learning is to choose a set of references.
A clause is not-all-equal satisfied if it has at least one literal assigned by T and one literal assigned by F.
 Max NAE-SAT is given by a set U of boolean variables and a set C of clauses.
A combination of 3-D vision and X-ray radiography is proposed to enable low-cost.
A common way to learn is by studying written step-by-step tutorials such as worked examples.
A complementation operation on a vertex of a digraph changes all outgoing arcs into non-arcs.
A complete 3D visualization method for virtual liver lesion model was proposed by taking patients' abdomen CT slices as data source.
A complexity of business dynamics often forces decision-makers to make decisions based on subjective mental models.
A comprehensive ontology can ease the discovery.
A comprehensive radiometric characterization of raw-data format imagery acquired with the Raspberry Pi 3 and V2.
1 camera module is presented.
 The Raspberry Pi is a high-performance single-board computer designed to educate and solve real-world problems.
 This small computer supports a camera module that uses a Sony IMX219 8 megapixel CMOS sensor.
 This paper shows that scientific and engineering-grade imagery can be produced with the Raspberry Pi 3 and its V2.
1 camera module.
 Raw imagery is shown to be linear with exposure and gain (ISO).
A computer vision-based autonomous fire suppression system with real-time feedback of fire size and spray direction is presented in this paper.
 The system has been developed for use in a firefighting robot for close-range.
A conceptually new algorithm of sea ice concentration retrieval in polar regions from satellite microwave radiometry data is discussed.
 The algorithm design favorably contrasts with that of known modern algorithms.
 Its design is based on a physical emission model of the sea surface - sea ice - snow cover - atmosphere system.
 No tie-points are used in the algorithm.
 All the calculation expressions are derived from theoretical modeling.
 The design of the algorithm minimizes the impact of atmospheric variability on sea ice concentration retrieval.
 Beside estimating sea ice concentration.
A Continuous Monitoring System (CMS) model is presented.
A correspondence is a set of mappings that establishes a relation between the elements of two data structures (i.
 sets of points.
A critical aspect of dimensionality reduction is to assess the quality of selected (or produced) feature subsets properly.
 Feature subset assessment in machine learning refers to split a given feature subset into a training set.
A crucial operation in the maintenance of data quality in relational databases is to remove tuples that mutually describe the same entity (i.
A crucial problem in modern software engineering is maturity assessment of organizations developing software.
A crucial step in the use of DNA markers for biodiversity surveys is the assignment of Linnaean taxonomies (species.
A damage detection methodology is proposed by integrating a nonlinear recursive filter and a non-contact computer vision based algorithm to measure structural dynamic responses.
 A phase-based optical flow algorithm inspired by the motion magnification technique is used to measure structural displacements.
A darknet monitoring system is developed to grasp malicious activities on the Internet in an early stage and to cope with them.
 The darknet monitoring system consists of network sensors deployed widely on the Internet.
 The sensors capture incoming unsolicited packets.
 A goal of this system analyzes captured malicious packets and provides effective information for protecting good Internet users from malicious activities.
 To provide effective and reliable information.
A database is a structured collection of records or data that is stored in a computer so that it can be consulted by a program to answer queries.
 Records retrieved through queries become information that can be used to make decisions.
 A database consists of one or more tables containing records of values for fields that pertain to the attributes of the object being represented by the table.
 Relational databases contain multiple tables that are linked by means of key fields.
 A database management system is the computer program that manages the database and queries the data to produce reports of information.
 Examples of simple databases and how they are produced are described in this article.
A design approach for determining the optimal flow pattern in a landscape lake is proposed based on FLUENT simulation.
A design of an advanced-reliability piezoelectric transducer of acoustic emission (AE) is suggested.
 The advanced reliability is achieved by eliminating the main reasons that lead to failures; duplicating/backing the main operating systems; and imparting redundant or extended technical capabilities to the transducer design that make it possible to compensate for partial or complete performance loss of the antenna group.
 It is shown that given such a design approach.
A deterministic Boltzmann model equation solver called dugksFoam has been developed in the framework of the open source CFD toolbox OpenFOAM.
 The solver adopts the discrete unified gas kinetic scheme (Guo et al.
A dimension splitting method (DSM) with Crank-Nicolson time discrete strategy for a three-dimensional heat equation is proposed.
 The basic idea is to simulate the three-Dimensional problem by numerically solving a series of two-dimensional problems in parallel fashion.
 Convergence and error estimation for the DSM scheme are derived in the paper.
 Numerical experiments demonstrate the feasibility and efficiency of the DSM scheme.
 Copyright (c) 2016 John Wiley & Sons.
A Distributed Denial of Service (DDoS) attack is an austere menace to extensively used Internet-based services.
 The in-time detection of DDoS attacks poses a tough challenge to network security.
 Revealing a low-rate DDoS (LR-DDoS) attack is comparatively more difficult in modern high speed networks.
A dynamic computer-vision control system incorporating open-source software technologies (Python.
A family of conservative.
A fast adaptive parallel factor (PARAFAC) decomposition algorithm is proposed for a class of third order tensors that have one dimension growing linearly with time.
 It is based on an alternating least squares approach in conjunction with a Newton-type optimization technique.
 By preserving the Khatri-Rao product and exploiting the reduced-rank update structure of the estimated subspace at each time instant.
A flexible and universal domain decomposition parallel scheme is proposed for the unconditionally stable finite-difference time-domain (FDTD) method.
 The leapfrog alternating direction implicit FDTD (ADI-FDTD) method is employed to eliminate the restriction of the Courant-Friedrichs-Lewy stability condition.
 The proposed domain decomposition parallel implementation of the leapfrog ADI-FDTD method is more flexible with process allocation and requires fewer data communications.
 A buffer region is introduced to decouple the interactions between neighboring subdomains at each time step.
 Electromagnetic simulations are presented to demonstrate the applicability.
A four-dimensional visualization approach.
A fully autonomous intracranial device is built to continually record neural activities in different parts of the brain.
A function transformation is presented to change a kind of nonlinear coupled system into a set of two elliptic equations of the first kind.
 Then new infinite sequence complexion two-soliton solutions to a kind of nonlinear coupled system are constructed by new solutions and Backlund transformation of elliptic equation of the first kind.
A generalized variable-coefficient KdV equation with perturbed and external-force terms is investigated in this Letter.
A genome sequence assembly provides the foundation for studies of genotypic and phenotypic variation.
A good skin detector that is capable of capturing skin tones under different conditions is important for human-machine interaction applications.
 In a general situation.
A good virtual Learning Space (LS) should convey pertinent learning information to the visitors at the most adequate time and locations to favor their knowledge acquisition.
 Considering the consolidation of the internet and the improvement of the interaction.
A gradient-statistic-based diagnostic measure is developed in the context of the generalized linear mixed models.
 Its performance is assessed by some real examples and simulation studies.
A graph-theoretic formulation for the sensitivity analysis of multi-domain dynamic systems is presented in this article.
 In this formulation.
A ground surface roughness measurement method is proposed to address current problems in the use of machine vision technology to measure roughness: the calculations are complex.
A handover authentication protocol ensures secure and seamless roaming over multiple access points.
 A number of such protocols are proposed.
A handy system is presented for creating in real time a natural television image composed of a real scene and a computer graphics object.
 The light positions and colors (RGB values) in the real scene are estimated and then applied to the object by using simple equipment.
 Objective and subjective experiments demonstrated the effectiveness of this system.
 An enhanced algorithm is also presented that improves the accuracy of light estimation.
A heuristic procedure based on novel recursive formulation of sinusoid (RFS) and on regression with predictive least-squares (LS) enables to decompose both uniformly and nonuniformly sampled 1-d signals into a sparse set of sinusoids (SSS).
 An optimal SSS is found by Levenberg-Marquardt (LM) optimization of RFS parameters of near-optimal sinusoids combined with common criteria for the estimation of the number of sinusoids embedded in noise.
 The procedure estimates both the cardinality and the parameters of SSS.
 The proposed algorithm enables to identify the RFS parameters of a sinusoid from a data sequence containing only a fraction of its cycle.
 In extreme cases when the frequency of a sinusoid approaches zero the algorithm is able to detect a linear trend in data.
A high degree of uncertainty associated with the emission inventory for China tends to degrade the performance of chemical transport models in predicting PM2.
5 concentrations especially on a daily basis.
 In this study a novel machine learning algorithm.
A high level of concern is placed on the storage.
A high performance license plate recognition system (LPRS) is proposed in this work.
 The proposed LPRS is composed of the following three main stages: (i) plate region determination.
A high-resolution (7 km) non-hydrostatic global mesoscale simulation using the Goddard Earth Observing System (GEOS-5) model is used to visualize the flow and fluxes of carbon dioxide throughout the year.
 Carbon dioxide (CO2) is the most important greenhouse gas affected by human activity.
 About half of the CO2 emitted from fossil fuel combustion remains in the atmosphere.
A high-resolution drought forecast model for ungauged areas was developed in this study.
 The Standardized Precipitation Index (SPI) and Standardized Precipitation Evapotranspiration Index (SPEI) with 3-.
A improvement of the expansion methods namely the improved tan(Phi(xi)/2)-expansion method for solving the Tzitzeica type nonlinear evolution equations is proposed.
 In this work.
A key aspect of realizing the future smart grid communication solution is a balanced approach between the network performance and the network security during the network deployment.
 A high security communication flow path is not useful when the network path cannot support capacity and reachability requirements.
 The deployment phase in communication network can facilitate an optimal network path by focusing on both the network performance and the network security at the same time.
 In this paper.
A key issue in fruit export is classification and sorting for acceptable marketing.
 In the present work.
A KP-like nonlinear differential equation is introduced through a generalised bilinear equation which possesses the same bilinear form as the standard KP bilinear equation.
 By symbolic computation.
A large body of research into bilingualism has revealed that language processing is fundamentally non-selective; there is simultaneous.
A large number of computer applications(like Computer Graphics.
A linear transformation method is proposed to handle the vector autoregression with mixed frequency time series data.
 Temporally aggregated observations impose linear constraints on the distribution of latent variables.
A literature review has identified the absence of a robust framework that guides the development of streamlined and valid multiple linear regression (MLR) predictive models for construction engineering applications.
 A reliable MLR model requires an appropriate set of input variables that can satisfy the underlying assumptions of best linear unbiased estimators (BLUE).
 In this research.
A look back at Skeletal Radiology in 2016 reveals a sizable number of publications that significantly advanced the state of knowledge about diseases of the musculoskeletal system.
 This review summarizes the content of some of the most intriguing papers of the year.
A majority of RFID authentication scenarios involve a single tag that is identified independent of other tags in the field of the reader.
A MapReduce algorithm can be described by a mapping schema.
A Mathematica application providing the user with a graphical interface (GUI) is presented and published.
A measurement system was developed using micro-distance imaging technology to measure droplet sizes in the field.
 Legible droplet images were acquired with a single-lens reflex camera.
A meminductor is a new type of memory device.
 It is of importance to study meminductor model and its application in nonlinear circuit prospectively.
 For this purpose.
A method for the computation of inverses of a given matrix is derived.
A method has been developed to determine the fat content in different cold meat products by image processing using the camera of a mobile phone.
 Salchichon.
A method to visualize polytopes in a four-dimensional euclidian space (x.
A methodology based on Artificial Neural Networks (ANN) and an Analog Ensemble (AnEn) is presented to generate 72 h deterministic and probabilistic forecasts of power generated by photovoltaic (PV) power plants using input from a numerical weather prediction model and computed astronomical variables.
 ANN and AnEn are used individually and in combination to generate forecasts for three solar power plants located in Italy.
 The computational scalability of the proposed solution is tested using synthetic data simulating 4450 PV power stations.
 The National Center for Atmospheric Research (NCAR) Yellowstone supercomputer is employed to test the parallel implementation of the proposed solution.
A methodology is presented for obtaining and processing data from the Web of Science (WoS) for bibliometric purposes.
 The R statistical software is used as a tool for creating functions that facilitate bibliometric work.
 In addition.
A microscope vision system to retrieve small metallic surface via micro laser line scanning and genetic algorithms is presented.
 In this technique.
A microstructure-based modeling method is developed to predict the mechanical behaviors of lithium ion battery separators.
 Existing battery separator modeling methods cannot capture the structural features on the microscale.
 To overcome this issue.
A model has been developed to simulate electromigration degradation in an interconnect segment in two dimensions using finite differences.
 The model was deployed on a parallel computer to statistically assess the lifetimes.
 The simulation takes into account the diffusion paths for electromigration mass transport along the grain boundaries and the capping layer.
 The microstructure is generated with a Monte Carlo algorithm.
A model that accurately predicts.
A modeling language for hybrid systems HydLa and its implementation HyLaGI are described.
 HydLa is a constraint-based language that can handle uncertainties of models smoothly.
 HyLaGI calculates trajectories by symbolic formula manipulation to exclude errors resulting from floating-point arithmetic.
 HyLaGI features a nondeterministic simulation algorithm so it can calculate all possible qualitative different trajectories of models with uncertainties.
A modified Gaussian current density is put forward.
A multi-craft asteroid survey has significant data synchronization needs.
 Limited communication speeds drive exacting performance requirements.
 Tables have been used in Relational Databases.
A multiphysics simulation technique based on the finite element method is developed for the reliability analysis of interconnects.
 The multiphysics simulation characterizes multidisciplinary.
A multiphysics-based co-simulation technique is developed based on the finite element method for the characterization of 3-D high-frequency integrated circuits (ICs) with integrated microchannel cooling.
 The multiphysics in the cosimulation includes full-wave electromagnetic.
A multistage graph is center problem of computer science.
A multitude of contemporary applications now involve graph data whose size continuously grows and this trend shows no signs of subsiding.
 This has caused the emergence of many distributed graph processing systems including Pre-gel and Apache Giraph.
A multi-view content generation method for three-dimensional (3D) display without depth-reversed area is demonstrated.
 The viewing zone periodicity of an autostereoscopic display is utilized to eliminate the depth-reversal by adjusting the light field alignment in the 3D light field.
 The conditions that a multi-view sequence should satisfies to form a depth-reversal-free sequence are given and detailed processes for converting the multi-view sequence into a depth-reversal-free sequence are presented.
 Experimental results show that the depth-reversed area is well eliminated.
A network traffic detection model based on swarm intelligent optimization neural network algorithm is proposed in this paper.
 QAPSO algorithm is used to optimize the basis function center and base function width of RBF neural network.
A networking laboratory is an essential tool for teaching communications engineering.
A neural network technique known as Kohonen unsupervised training is coupled with a modified version of the particle swarm optimization technique in an effort to develop an algorithm capable of finding multiple optimal solutions for a given problem.
 The results of five example problems of increasing difficulty validate the Alternative Analysis Networking algorithm's functionality.
A new approach for the unified treatment of frictional contact and orthotropic plasticity at finite strains using semi-smooth Newton methods is presented.
 The contact discretization is based on the well-known mortar finite element method using dual Lagrange multipliers to facilitate the handling of the additional Lagrange multiplier degrees of freedom.
 Exploiting the similarity of the typical inequality constraints of plasticity and friction.
A new compact combinatorial coordinate system is presented to address not only the voxels (truncated octahedra) of the body-centered cubic grid but also the lower-dimensional cells - faces.
A new data encryption scheme is proposed based on the position substitution.
A new data-driven model has been developed to determine 1D/2D particle size distribution (PSD) from measured FBRM chord-length distribution (CLD) data.
 The structure of the model consists of three steps: first.
A new disturbance automated reference toolset (DART) was developed to monitor human land surface impacts using soil-type and ecological context.
 DART identifies reference areas with similar soils.
A new experimental.
A new full-precision algorithm to solve the Debye scattering equation has been developed for high-performance computing of powder diffraction line profiles from large-scale atomistic models of nanomaterials.
 The Debye function was evaluated using a pair distribution function computed with high accuracy.
A new generation of computer games has taken over during the last few years.
A new image analysis algorithm based on mathematical morphology and pixel classification for grapevine berry counting is presented in this paper.
A new kind of cloud service.
A new lightning shielding failure evaluation method based on the electro-geometric method aided by 3D graphics technology is introduced.
 The approach is based on the collection surface method.
 Shielding devices and protected equipment are equally considered as targets of lightning strikes when generating collection surfaces.
 The collection surfaces are then projected to a 2D bitmap.
A new method for obtaining random bijective S-boxes based on discrete chaotic map is presented.
 The proposed method uses a discrete chaotic map based on the composition of permutations.
 The obtained S-boxes have been tested on the number of criteria.
A new method for obtaining strong S-boxes based on chaotic map and Teaching-Learning-Based Optimization (TLBO) is presented in this paper.
 Our method presents eight rounds; each round contains two transformations: row left shifting and columnwise rotation.
 The vectors for the transformations are different from one round to another.
A new method to obtain second-order reductions for ordinary differential equations which are polynomial in the derivatives of the dependent variable is presented.
 The method is applied to obtain reductions and new solutions to several well-known equations of mathematical physics: a lubrication equation.
A new nonlinear dimensionality reduction method called kernel global-local preserving projections (KGLPP) is developed and applied for fault detection.
 KGLPP has the advantage of preserving global and local data structures simultaneously.
 The kernel principal component analysis (KPCA).
A new quantum dialogue protocol is designed by using the continuous-variable two-mode squeezed vacuum states due to its entanglement property.
 The two communication parties encode their own secret information into the entangled optical modes with the translation operations.
 Each communication party could deduce the secret information of their counterparts with the help of his or her secret information and the Bell-basis measurement results.
 The security of the proposed quantum dialogue protocol is guaranteed by the correlation between two-mode squeezed vacuum states and the decoy states performed with translation operations in randomly selected time slots.
 Compared with the discrete variable quantum dialogue protocols.
A new scanning system named Vertex picker has been developed to rapid collect alpha decay events.
A new secure key distribution scheme based on the dynamic chaos synchronization of two cascaded semiconductor laser systems (CSLSs) subject to common chaotic injection and random phase-modulated optical feedback is demonstrated.
 In this scheme.
A new vision-based system is designed for the identification of chatter vibration.
 Chatter vibration is a major obstacle in CNC machining as it causes bad surface finishes and increases tooling damage and costs.
 This research proposes the use of machine vision for the detection of chatter vibration frequencies during high-speed milling operations.
 The vision system utilizes digital image processing and texture analysis to determine the frequencies of the chatter marks on the surface of a machined workpiece.
 The developed system provides a solution to machinists at ease without any expensive measuring equipment.
 The vision system was verified and compared with other chatter detection methods by identifying and creating tests based on the dynamics of a machine tool.
A next generation B factory and the detector counterpart.
A nondestructive method was developed for assessing total viable count (TVC) in pork during refrigerated storage by using hyperspectral imaging technique in this study.
 The hyperspectral images in the visible/near-infrared (VIS/NIR) region of 400-1100 nm were acquired for fifty pork samples.
A non-destructive methodology was developed to automatically detect and quantify bruise volumes in the equatorial region of apples.
A nonlinear Schrodinger equation with self-consistent sources can be used to describe the interaction between an high-frequency electrostatic wave and an ion-acoustic wave in two-component homogeneous plasmas.
 In this paper.
A nonrelational.
A notion of a universal construction suited to distributed computing has been introduced by Herlihy in his celebrated paper Wait-free synchronization (ACM Trans Program Lang Syst 13(1):124-149.
A notion of asymmetric quantum dialogue (AQD) is introduced.
 Conventional protocols of quantum dialogue are essentially symmetric as the users (Alice and Bob) can encode the same amount of classical information.
 In contrast.
A novel depth from defocus (DFD) measurement system is presented.
A novel dimensionality reduction method named nonlocal and local structure preserving projection (NLLSPP) is proposed and used for process monitoring.
 NLLSPP can simultaneously preserve the nonlocal structure (i.
A novel dynamic radio-cooperation strategy is proposed for a Cloud Radio Access Network (Cloud-RAN) consisting of multiple Remote Radio Heads connected to a central Virtual Base Station (VBS) pool.
 In particular.
A number of computer vision problems such as facial age estimation.
A number of methods.
A number of security protocols have been designed for mobile transactions using Near Field Communication technology in the last few years.
A parallel matheuristic algorithm is presented as a spin-off from the exact Branch-and-Fix Coordination (BFC) algorithm for solving multistage stochastic mixed 0-1 problems.
 Some steps to guarantee the solution's optimality are relaxed in the BFC algorithm.
A parallel time integration method for nonlinear partial differential equations is proposed.
 It is based on a new implementation of the Paraexp method for linear partial differential equations (PDEs) employing a block Krylov subspace method.
 For nonlinear PDEs the algorithm is based on our Paraexp implementation within a waveform relaxation.
 The initial value problem is solved iteratively on a complete time interval.
 Nonlinear terms are treated as source terms.
A parallel version of the NEMO complex ocean circulation model has been implemented for the Black Sea basin; the results of circulation numerical modeling with a high spatial resolution are presented.
 Analysis of the spatial variability is performed for the reconstructed hydrophysical fields in 2005-2008.
 The resulting simulated spatial variability characteristics of the sea surface temperature are compared with available satellite observational data.
A parallelized numerical contour integral based method is proposed for counting interior eigenvalues in a given region on the complex plane.
 The proposed method is derived from descriptor system of power system linearized model and complex analysis theory.
A parameterized string (p-string) is a generalization of the traditional string over two alphabets: a constant alphabet and a parameter alphabet.
 A parameterized match (pmatch) exists between two p-strings if the constants match exactly and if there exists a bijection between the parameter symbols.
 Historically.
A patient-specific algorithm.
A periodic processes scheduling problem appears for flexible manufacturing systems.
A phase-only computer-generated holography (CGH) calculation method for stereoscopic holography is proposed in this paper.
 The two-dimensional (2D) perspective projection views of the three-dimensional (3D) object are generated by the computer graphics rendering techniques.
 Based on these views.
A PIC/MCC simulation model for the analysis of low-temperature discharge plasmas is represented which takes the common leapfrog and the velocity Verlet algorithm for the particle integration.
A plethora of Multi Agent Systems (MAS) development methodologies exists and all compete for prominence.
 This paper advocates unification of best of breed activities from these methodologies and examines two existing approaches for unifying access to them.
 It proposes an alternative approach that focusses on the use of domain knowledge through ontologies as offering the best potential for unifying access to them.
 The reliance on ontologies will provide flexibility in the process and workproducts use within the methodology.
 The focus on domain knowledge will reduce the number of mandatory methodological tasks and at the same time create scope for reuse with respect to both system designs and components.
 The paper will further sketch and argue for a full software development lifecycle for MAS where ontologies expressing domain knowledge are the central artifacts.
A portable real-time facial recognition system that is able to play personalized music based on the identified person's preferences was developed.
 The system is called Portable Facial Recognition Jukebox Using Fisherfaces (FRJ).
 Raspberry Pi was used as the hardware platform for its relatively low cost and ease of use.
 This system uses the OpenCV open source library to implement the computer vision Fisherfaces facial recognition algorithms.
A positive credit history and rating help consumers with good payment history to get lower interest rates.
A poster must be sufficiently informative to convey a suitable message which reflects the genuine characteristics of the topic.
 A social poster design deals with social issues.
A potential problem for persisting large volume of streaming logs with conventional relational databases is that loading large volume of data logs produced at high rates is not fast enough due to the strong consistency model and high cost of indexing.
 As a possible alternative.
A practical issue is present in sustaining and rehabilitating the ecologically vulnerable post-mining area in which the environmental condition varies spatially and therefore influenced by multiple factors.
 This paper attempts to integrate the ecological vulnerability assessment and rehabilitation treatment to assist land managers in revealing vulnerable features along with developing treatments of vulnerability mitigation.
 Using a post-mining site in a mountainous area in western China as study area.
A preconditioned Navier-Stokes (NS) method is developed for multiphase flow with application to the water entry problem of moving bodies.
 The method employs a dual time-preconditioned technique with multiblock and parallel computing to improve the computational productivity.
A preference-based approach is proposed for Grid computing with regard to preferences given by various groups of virtual organization (VO) stakeholders (such as users.
A prerequisite for simulating the biophysics of complex biological tissues and whole organisms are computational descriptions of biological matter that are flexible and can interface with materials of different viscosities.
A previous bioinformatics analysis identified the Mycobacterium tuberculosis proteins Rv2125 and Rv2714 as orthologs of the eukaryotic proteasome assembly chaperone 2 (PAC2).
 We set out to investigate whether Rv2125 or Rv2714 can function in proteasome assembly.
 We solved the crystal structure of Rv2125 at a resolution of 3.
A previous knowledge of the domains of dependence of a Hamilton-Jacobi equation can be useful in its study and approximation.
 Information of this nature is.
A probabilistic relational database is a probability distribution over a set of deterministic relational databases (namely.
A project manager balances the resource allocation using resource leveling algorithms after assigning resources to project activities.
A prominent software security violation-buffer overflow attack has taken various forms and poses serious threats until today.
 One such vulnerability is return-oriented programming attack.
 An return-oriented programming attack circumvents the dynamic execution prevention.
A prototype of home management system (HMS) controlled by mobile device was developed by project teams at Computing Education Academy (CEA) at Fairfield University by integrating Arduino with AppInventor.
 CEA was established at Fairfield University in 2013 to expose high school students to key computing concepts and basic computer programming through hands-on activities and to motivate students to enter post-secondary education and career opportunities in computing.
 CEA class modules include Scratch.
A quantitative and qualitative increase in production has been obtained in most fields through the development of CPUs and real-time systems based on them.
 Such is the case in the industrial sector where the automation process relieved partly or wholly the human activities needed in the manufacturing process.
 This is mainly due to time sharing in embedded real-time systems and to pseudo-parallel execution of tasks in the implementation of a single central processing unit.
 The present article presents the validation of the nHSE (Hardware Scheduler Engine) scheduler implemented in hardware by using a FPGA Xilinx Virtex-7.
A quantum key distribution protocol with traditional Bob has been proposed recently by Boyer et al.
 using single-particle state.
 In this paper.
A randomized on-line algorithm is given for competitiveness less than 1901 against the previously best known competitiveness of IN uses a new approach and defines a potential in the 2-server problem on the line.
A real-time image filtering technique is proposed which could result in faster implementation for fingerprint image enhancement.
 One major hurdle associated with fingerprint filtering techniques is the expensive nature of their hardware implementations.
 To circumvent this.
A recent biological study shows that the extremely good efficiency of fruit flies in finding food.
A recommendation system for software engineering (RSSE) is a software application that provides information items estimated to be valuable for a software engineering task in a given context.
 Present the results of a systematic literature review to reveal the typical functionality offered by existing RSSEs.
A rectangular phase screen transmission model is usually applied to the simulation of optical propagation processes in atmospheric turbulence.
 In this paper.
A reference architecture for supporting secure big data analytics over Cloud-enabled relational databases.
A relaxation oscillator is a kind of oscillator based on a behavior of physical system's return to equilibrium after being disturbed.
 The relaxation-oscillation equation is the primary equation of relaxation and oscillation processes.
 The relaxation oscillation equation is a fractional differential equation with initial conditions.
 In this paper.
A research has been carried out on the dynamic analysis and simulation of parallel robots due to the situation where parallel robots tend to break down on account of large stress during movements.
 In order to study the force condition of the robot on the move.
A review and comparative analyses of methods for restricting the range of molecular interactions within the concept of atom-atom potentials are presented.
 Emphasis is placed on the problem of calculating the electrostatic energy in models with periodic boundary conditions.
 Numerous calculations of the thermodynamic and structural characteristics of water using parallel Monte Carlo computations have shown that the use of functional forms simulating the electric potentials of screened charges provides very good results.
A roadmap for a semi-algebraic set S is a curve which has a non-empty and connected intersection with all connected components of S.
A robust and efficient object tracking algorithm is required in a variety of computer vision applications.
 Although various modern trackers have impressive performance.
A robust multi-objective optimization method for truss optimum design is presented.
 In the robust design.
A sample of a nanomaterial contains a distribution of nanoparticles of various shapes and/or sizes.
 A scanning electron microscopy image of such a sample often captures only a fragment of the morphological variety present in the sample.
 In order to quantitatively analyse the sample using scanning electron microscope digital images.
A seamless and secure handover is always one of the important design goals of the cellular networks.
 The handover scheme of 4G Long Term Evolution (LTE) wireless networks is complex due to the presence of two possible different types of base stations.
 In LTE communication systems.
A secure and safe authentication on a vehicular ad-hoc network (VANET) is essential for network security.
 A safe data transmission requires integrity.
A secure two-way identity authentication scheme of WiFi environment is raised from the perspective of security and the load capacity mobile terminal according the increasing serious of wireless network security.
 The scheme is based on one-time password authentication technology and the digital signature is realized by using the elliptic curve cryptosystem.
 It is known that the scheme can resist the common network attacks by analyzing the security of this scheme.
A series of experiments were conducted in a flume to study bed-load transport.
 The motion of bed-load particles was captured by a series of images taken by a high-speed camera.
A novelparticle motion tracking method was developed to automatically detect all the moving particles and calculate the instantaneous particle velocities.
 The instantaneous bed load transport rate was calculated based on particle velocity and the volume of moving particles.
 To verify this method.
A set of algorithms for simultaneous localization and mapping in industrial television systems is discussed.
 A probabilistic model of this problem is described with the FastSLAM algorithm as an example.
 The possibility of using a sigma-point Kalman filter for estimating the movement of spatial landmarks.
A Shannon cipher system for memoryless sources in which distortion is allowed at the legitimate decoder is considered.
 The source is compressed using a secured rate distortion code.
A significant challenge faced by the mobile application industry is developing and maintaining multiple native variants of mobile applications to support different mobile operating systems.
A significant challenge in fitting metamodels of large-scale simulations with sufficient accuracy is in the computational time required for rigorous statistical validation.
 This paper addresses the statistical computation issues associated with the Bootstrap and modified PRESS statistic.
A simple and accurate scheme for modeling microstructures is proposed with the help of element trimming.
A single semester molecular biology laboratory has been developed in which students design and execute a project examining transcriptional regulation in Saccharomyces cerevisiae.
 Three weeks of planning are allocated to developing a hypothesis through literature searches and use of bioinformatics.
 Common experimental plans address a cell process and how three genes that encode for proteins involved in that process are transcriptionally regulated in response to changing environmental conditions.
 Planning includes designing oligonucleotides to amplify the putative promoters of the three genes of interest.
 After the PCR.
A slow-paced persistent attack.
A smart camera is a vision system capable of extracting application-specific information from the captured images.
 The paper proposes a decentralized and efficient solution for visual parking lot occupancy detection based on a deep Convolutional Neural Network (CNN) specifically designed for smart cameras.
 This solution is compared with state-of-the-art approaches using two visual datasets: PKLot.
A smart grid is delay sensitive and requires the techniques that can identify and react on the abnormal changes (i.
A Software Product Line is a set of software systems of a domain.
A software project encounters many changes during the software development life cycle.
 The key challenge is to control these changes and manage their impact on the project plan.
A sparse wireless sensor network for forest fire detection is considered.
 It is assumed that two types of malicious nodes can exist in this network.
 The malicious behaviors are assumed to be concealed through some statistical behavior.
 A lightweight centralized trust-based model is proposed to detect malicious or misbehaving nodes.
 We assume that all nodes contribute to this process through the gathering of statistical data related to communication with their neighbors.
 These data are periodically sent to a base station.
A spatiotemporal mining framework is a novel tool for the analysis of marine association patterns using multiple remote sensing images.
 From data pretreatment.
A stereovision based methodology to estimate the position.
A street lighting system designed with features to detect critical functional problems using data fusion concepts is presented.
 With measurements of rms voltage.
A substantial amount of datasets stored for various applications are often high dimensional with redundant and irrelevant features.
 Processing and analysing data under such circumstances is time consuming and makes it difficult to obtain efficient predictive models.
 There is a strong need to carry out analyses for high dimensional data in some lower dimensions.
A suitable piece of software is presented to connect Abaqus.
A system is presented for emulate a mouse from the movement of the head and eyelids.
 The position of the head is used for controlling the horizontal and vertical displacement of the cursor.
A systematic review allows us to identify.
A tangent vector field on a surface is the generator of a smooth family of maps from the surface to itself.
A team consisting of an unknown number of mobile agents.
A test oracle determines whether a test execution reveals a fault.
A three-party scheme for secure quantum communication.
A trie is one of the data structures for keyword search algorithms and is utilized in natural language processing.
A variable-coefficient nonisospectral modified Kadomtsev-Petviashvili equation with two Painlev.
A variety of compartment models are used for the quantitative analysis of dynamic positron emission tomography (PET) data.
 Traditionally.
A variety of operating systems and application programs on the internet have weak points in terms of security.
 Intelligent attacks on such weak points have been spreading rapidly.
A visual breakout prediction method for mold monitoring is proposed based on temperatures measured by thermocouples in the mold combined with the use of computer vision technology.
 This mold temperature rate thermography allows the characteristics of abnormal temperature regions to be captured and extracted.
A Web-based knowledge database and computing platform for nonlinear differential equations is presented.
A wide variety of large-scale data have been produced in bioinformatics.
 In response.
A wide variety of predictive analytics techniques have been developed in statistics.
A wireless sensor network (WSN) consists of energy-limited autonomous tiny devices spatially distributed so as to monitor their environments.
 They have a very wide application domain ranging from military applications to health care services.
A zero dimensional thermodynamic model simulation is developed to simulate the combustion characteristics and performance of a four stroke homogeneous compression combustion ignition (HCCI) engine fueled with gasoline.
 This model which applies the first law of thermodynamics for a closed system is inclusive of empirical model for predicting the important parameters for engine cycles: the combustion timing and mass burnt fraction during the combustion process.
 The hypothesis is the increasing intake temperature can reduce the combustion duration and the fuel consumption at wide range of equivalence ratio.
 The intake temperature were increased from 373-433 K with increment of 20 K.
 The engine was operated over a range of equivalence ratios of 0.
5 at constant engine speed of 1200 rpm and intake pressure of 89.
Aberrant expression of miR-15a-5p has been frequently reported in some cancers excluding lung cancer.
 The role and its molecular mechanism of miR-15a-5p in lung cancer have not been reported.
 In this study.
Able to directly study the internal rule by starting from the complicated phenomenon.
Access control is critical for many applications of the Internet of Things (IoT) since the owner of an IoT device (and application) may only permit one user to access a subset of the resources of the device.
 To provide access control for an IoT network.
Access control model-based security plays very crucial role in security.
 Cloud computing is one of the emerging and challenging fields.
 In the implementation explained in this paper.
According to a recent Gartner report.
According to compression sensing reconstruction algorithm of Orthogonal Matching Pursuit (OMP) algorithm the problem of each iteration can't select the optimal atomic.
According to consciousness involvement.
According to dichromatic reflection model.
According to many published literature.
According to the tree structure description of the multibody system.
Accumulating evidence indicates that long noncoding RNAs (lncRNAs) and circular RNAs (circRNAs) involve in germ cell development.
Accumulating evidence suggests that long noncoding RNAs (IncRNAs) are crucial regulators of the Epithelial-Mesenchymal-Transition (EMT).
 TGF-beta signaling is a major inducer of EMT and can facilitate lung cancer metastasis.
Accumulating evidence suggests that ribosomal proteins may have extraribosomal functions in various physiological and pathological processes.
Accurate 3D measuring systems thrive in the past few years.
 Most of them are based on laser scanners because these laser scanners are able to acquire 3D information directly and precisely in real time.
Accurate and timely knowledge is critical in intelligent transportation system (ITS) as it leads to improved traffic flow management.
 The knowledge of the past can be useful for the future as traffic patterns normally follow a predictable pattern with respect to time of day.
Accurate and timely traffic flow prediction is crucial to proactive traffic management and control in data driven intelligent transportation systems ((DITS)-I-2).
Accurate condition assessment of in-service infrastructure systems is critical for system-wide prioritization decisions.
 Current protocols require lengthy inspections and expensive equipment to examine large infrastructure systems.
 Furthermore.
Accurate determination of the protons energy in radiotherapy planning can be achieved if the stopping power of protons to specific body tissue are known.
 The Bethe-Bloch equation greatly aid the understanding of how to calculate the stopping power from the physics point of view.
Accurate image registration is a vital step in many computer vision processes.
Accurate measurement of cardiomyocyte contraction is a critical issue for scientists working on cardiac physiology and physiopathology of diseases implying contraction impairment.
 Cardiomyocytes contraction can be quantified by measuring sarcomere length.
Accurate prediction of software defects is of crucial importance in software engineering.
 Software defect prediction comprises two major procedures: (i) Design of appropriate software metrics to represent characteristic software system properties; and (ii) development of effective regression models for count data.
Accurate rainfall-runoff modeling during typhoon events is an essential task for natural disaster reduction.
 In this study.
Accurate scale estimation and occlusion handling is a challenging problem in visual tracking.
Accurate segmentation of each bone of the human skeleton is useful in many medical disciplines.
 The results of bone segmentation could facilitate bone disease diagnosis and post-treatment assessment.
Accurate service-life prediction of structures is vital for taking appropriate measures in a time-and cost-effective manner.
Accurately and reliably tracking the undulatory motion of deformable fish body is of great significance for not only scientific researches but also practical applications such as robot design and computer graphics.
Achieving efficient security solutions for wireless sensor networks (WSN) is a daring challenge due to vulnerable nature of wireless medium.
 Routing is a major threat to security.
 An adversary can inject bogus routing to en-route nodes causing false decision and drain the sensors energy in the network system.
 To identify and prune the adversaries.
Achieving security in distributed systems of constrained devices (like Wireless Sensor Networks) requires methods that can be performed while very limited computational.
Acquiring relevant business concepts is a crucial first step for any software project for which the software experts are not domain experts.
 The wealth of information buried within an organization's written documentation is a precious source of concepts.
Acting as a focus of network security field.
Action recognition.
Activated microglia cells (MCs) are able to release a large amount of inflammatory cytokines after ischemic stroke.
Active appearance models (AAMs) are one of the most popular and well-established techniques for modeling deformable objects in computer vision.
 In this paper.
Active Contour Models (ACM) have been widely used for segmentation in many computer vision applications.
 These models are defined by an energy functional attached to an initial curve that evolves under some constraints to extract desired objects in the image.
 New models are proposed.
Actual software project feasibility assessment is one of the foremost challenging and vital activities in estimating economics of software before its actual development.
Ad hoc networks are subject to multiple challenges.
Adaptive algorithms are prevalently applied in the design of nonlinear active noise control (ANC) systems.
 The most important nonlinearity in ANC is the saturation effect produced by the electro-acoustical sensors and transducers.
 The dominant saturation nonlinearity is in the transducers.
Adaptive and intelligent instructional systems are used to deal with the issue of learning personalisation in contexts where human instructors are not immediately available.
Adaptive Educational Hypermedia systems (AEHS) have provided new perspectives for access to information and enhance learning.
 However AEHS have advantages and positive impact on learning.
Adaptive learning game studies experienced rapid growth due to its learning benefits and popularity among younger generations.
 Computer programming is considered a challenging subject by learners and teachers.
Adding the number of computing nodes is a common approach to achieving higher performance in a parallel computing system.
Addressing the documented shortage of students in the United Sates choosing to enter computing and engineering fields.
Address-resolution protocol (ARP) is an important protocol of data link layers that aims to obtain the corresponding relationship between Internet Protocol (IP) and Media Access Control (MAC) addresses.
 Traditional ARPs (address-resolution and neighbor-discovery protocols) do not consider the existence of malicious nodes.
Adsorption processes are responsible for detection of cancer biomarkers in biosensors (and immunosensors).
Adtpp is an open-source tool that adds support for algebraic data types (ADTs) to the C programming language.
 ADTs allow more precise description of program types and more robust handling of data structures than are directly supported by C.
 ADT definitions and other declarations are put in a file that is preprocessed by adtpp to produce a C header (.
h') file that can be included in C source files.
 The generated header file contains C type definitions.
Advanced biomedical instruments and data acquisition techniques generate large amount of physiological data.
 For accurate diagnosis of related pathology.
Advanced medical devices exploit the advantages of embedded software whose development is subject to compliance with stringent requirements of standardization and certification regimes due to the critical nature of such systems.
 This paper presents initial results and lessons learned from an ongoing project focusing on the development of a formal model of a subsystem of a software-controlled safety-critical active medical device (AMD) responsible for renal replacement therapy.
 The use of formal approaches for the development of AMDs is highly recommended by standards and regulations.
Advanced modern processors support single instruction.
Advancement in information technology is playing an increasing role in the use of information systems comprising relational databases.
 These databases are used effectively in collaborative environments for information extraction; consequently.
Advances in artificial intelligence and computer graphics digital technologies have contributed to a relative increase in realism in virtual characters.
 Preserving virtual characters' communicative realism.
Advances in brain-computer interface (BCI) technology have facilitated the detection of Motor Imagery (MI) from electroencephalography (EEG).
Advances in computing technology and computer graphics engulfed with huge collections of data have introduced new visualization techniques.
 This gives users many choices of visualization techniques to gain an insight about the dataset at hand.
Advances in database technologies are moving the attention of data managers from well known structured relational databases (SQL-Like DBs) towards NoSQL approaches.
Advances in geographic information.
Advances in high-throughput proteomics have led to a rapid increase in the number.
Advances in laboratory and information technologies are transforming public health microbiology.
 High-throughput genome sequencing and bioinformatics are enhancing our ability to investigate and control outbreaks.
Advances in optical microscopy.
Advances in vehicular technology have resulted in more controls being incorporated into cabin designs.
 We present a system to determine which vehicle occupant is interacting with a control on the center console when it is activated.
AES algorithm or Rijndael algorithm is a network security algorithm which is most commonly used in all types of wired and wireless digital communication networks for secure transmission of data between two end users.
AES represents the algorithm for advanced encryption standard consistof different operations required in the steps of encryption and decryption.
 The proposed architecture is based on optimizing area in terms of reducing no of slices required for design of AES algorithm in VHDL.
 This paper presents AES-128 bit algorithm design consist of 128 bit symmetric key.
 The AES implementation.
Affine equivalence classification of Boolean functions has significant applications in logic synthesis and cryptography.
 Previous studies for classification have been limited by the large set of Boolean functions and the complex operations on the affine group.
 Although there are many research on affine equivalence classification for parts of Boolean functions in recent years.
After analyzing the current situation of Computer Programming Course.
After coming to rest on the night side of comet 67P/Churyumov-Gerasimenko.
After decades of independent morphological and functional brain research.
After more than a decade of research.
Age estimation from face images is an important yet difficult task in computer vision.
 Its main difficulty lies in how to design aging features that remain discriminative in spite of large facial appearance variations.
Agent-based modelling and simulation is a promising methodology that can be applied in the study of population dynamics.
 The main advantage of this technique is that it allows representing the particularities of the individuals that are modeled along with the interactions that take place among them and their environment.
Agent-based models (ABMs) are increasingly being used to study population dynamics in complex systems.
Agent-based simulations are increasingly popular in exploring and understanding cellular systems.
Aggregate makes up a high mass fraction in asphalt pavements.
 The morphological characteristics of aggregate (i.
Agility is a concept and practice with significant importance in managing projects and organizations.
Aging is associated with a progressive loss of the CD28 costimulatory molecule in CD4(+) lymphocytes (CD28(null) T cells).
Aim Less than 6% of the worlds described plant species have been assessed on the IUCN Red List.
Aim of this paper was to assess the clinical effectiveness of a novel ultrasound (US) approach for the estimation of bone fragility.
 A total of 85 female patients (40-80 years) were recruited and underwent conventional DXA investigations of both lumbar spine and proximal femur.
Aim of this paper was to assess the discrimination power of a novel ultrasound (US) parameter.
Aim: A plasmid was isolated from Aeromonas dhakensis strain F2S2-1 to understand their function in the host.
 The main objective of the study was to isolate and sequence the plasmid.
Aim: Alzheimer's disease patients are increasing rapidly every year.
 Scholars tend to use computer vision methods to develop automatic diagnosis system.
 (Background) In 2015.
Aim: This study reports structural modeling.
Aiming at a poor optimization effect.
Aiming at efficiently aware the network security situation.
Aiming at improving the poor real-time performance of existing nonlinear filtering algorithms applied to spacecraft autonomous navigation based on Global Positioning System (GPS) measurements and simplifying the algorithm design of navigation algorithms.
Aiming at machine learning applications in which fast online learning is required.
Aiming at solving the problems on dynamic occlusion of complete denture in the field of dental restoration.
Aiming at the problem that the fixed radio frequency identification (RFID) system with lightweight cryptography may be easily illegally controlled.
Aiming to an automatic sound recognizer for radio broadcasting events.
Aiming to reduce the losses of shipwreck.
Aims: A normal tissue complication probability (NTCP) model of severe acute mucositis would be highly useful to guide clinical decision making and inform radiotherapy planning.
 We aimed to improve upon our previous model by using a novel oral mucosal surface organ at risk (OAR) in place of an oral cavity OAR.
 Materials and methods: Predictive models of severe acute mucositis were generated using radiotherapy dose to the oral cavity OAR or mucosal surface OAR and clinical data.
 Penalised logistic regression and random forest classification (RFC) models were generated for both OARs and compared.
 Internal validation was carried out with 100-iteration stratified shuffle split cross-validation.
Aims: Protocol security which is important concern in the network security.
AimsExplorations of freshwater Cyanobacteria as antimicrobial (bacteria.
Air pollution has been a major environment-related health threat.
 Most of the studies on PM2.
5 toxicity have verified on the cardiovascular system and endothelial cells.
Algebraic local cohomology classes associated with parametric semi-quasihomogeneous hypersurface isolated singularities are considered in the context of symbolic computation.
 The motivations for this paper are computer calculations of complete lists of Tjurina numbers of semi-quasihomogeneous polynomials with isolated singularity.
 A new algorithm.
Algorithm design and analysis is a core course of the computer and related professional.
Algorithm Design and Analysis is one of the core courses for undergraduates majoring in Computer Science and Technology.
 Based on practical teaching experiences.
Algorithmic and computer programming in the bachelor's degree is a course that demands a large involvement of students in performing non-standard exercises.
 This practical aspect is incompatible with classical ex cathedra course.
 It is the reason why we implement a blended learning approach much more responsive to students in a bachelor class of Bio Engineering at the Gembloux Agro Bio Tech Faculty (University of Liege.
Algorithmic music composition is concerned with generation of music through algorithms.
 Using natural language text as the basis for generation opens up interesting ideas for research.
 We have identified three strategies for mapping text to music: using all the letters.
Algorithmic thinking development is a difficulty that students have to confront when they learn programming the right use of selection and control structures is a big challenge.
 In this research were used generative learning objects for algorithmic thinking development in the programming foundations course that is offered to new students of computer systems career.
 Research methodological approach.
Algorithms are hard to understand for novice computer science students because they dynamically modify values of elements of abstract data structures.
 Animations can help to understand algorithms.
Algorithms for extracting hydrologic features and properties from digital elevation models (DEMs) are challenged by large datasets.
Alice is an incredibly fun 3D programming environment that allows users to manipulate objects in a 3D world in order to create program animated movies.
 This paper discusses the impact of adopting Alice on female students' attitude and performance in an introductory computer programming course in Java language.
 The target population of this research is first year computing students at Arab Open University - Jordan branch.
 Quasi-experiment was conducted in this research.
Alignment-free methods are one of the mainstays of biological sequence comparison.
All known approaches to parallel data processing in relational client-server database management systems are based only on inter-query parallelism.
 Nevertheless.
Allocating memory dynamically for virtual machines (VMs) according to their demands provides significant benefits as well as great challenges.
 Efficient memory resource management requires knowledge of the memory demands of applications or systems at runtime.
 A widely proposed approach is to construct a miss ratio curve (MRC) for a VM.
Allocating tasks to processors is a well-known NP-Hard problem in distributed computing systems.
 Due to the lack of practicable exact solutions.
All-to-all comparison problems represent a class of big data processing problems widely found in many application domains.
 To achieve high performance for distributed computing of such problems.
All-to-all personalized exchange (ATAPE) is an inspired process to speedup the parallel and distributed computing.
Along with the development of human being.
Along with the progress in computer hardware architecture and computational power.
Along with the rapid advancement of digital image processing technology.
Alternative splicing provides a major mechanism to generate protein diversity.
 Increasing evidence suggests a link of dysregulation of splicing associated with cancer.
 Genome-wide alternative splicing profiling in lung cancer remains largely unstudied.
 We generated alternative splicing profiles in 491 lung adenocarcinoma (WAD) and 471 lung squamous cell carcinoma (LUSC) patients in TCGA using RNA-seq data.
Although many changes have been discovered during heart maturation.
Although missing data methods have advanced in recent years.
Although most business application data is stored in relational databases.
Although motion estimation (ME) approaches for fluid flows have been widely studied in computer vision domain.
Although multi-label learning can deal with many problems with label ambiguity.
Although quite recent as a forensic research domain.
Although recent laboratory tests are showing promising progresses in the materials and production technologies of photovoltaic (PV) devices.
Although the determination of university students' attitudes towards computer programming is a significant issue.
Although the importance of cellular forces to a wide range of embryogenesis and disease processes is widely recognized.
Although the introduction of commercial RGB-D sensors has enabled significant progress in the visual navigation methods for mobile robots.
Although the problem of k-area coverage has been intensively investigated for dense wireless sensor networks (WSNs).
although the tendency towards a Semantic Web is increasing.
Although there is a strong focus on NoSQL databases for cloud computing environments.
Although vision-based methods for displacement and vibration monitoring have been used in civil engineering for more than a decade.
Aluminum/graphite (Al/Gr) composites have been used as self-lubricating materials due to the superior lubricating effect of graphite during sliding.
 This paper summarizes various tribological aspects of self-lubricating aluminum composites.
 The influence of various factors such as (a) material factors.
Alzheimer's disease is a complex progressive neurodegenerative brain disorder.
Ambient noise variability is a critical challenge encountered by multiple stakeholders.
American Sign Language (ASL) fingerspelling is the act of spelling a word letter-by-letter when a specific sign does not exist to represent it.
 Synthesizing intelligible ASL.
Amid growing calls for greater collaboration between journalism and computer programming.
Among many factors that influence the success of a software project.
Among the computer science courses intended for high school non-majors.
Among the emerging nonvolatile memory (NVM) technologies.
Among the methods to knowledge discovery.
Among the population of known Galactic black hole X-ray binaries.
Among the preeclampsia-related long non-cording RNAs (lncRNAs) screened with a gene chip in our preliminary study.
Among these response signals in the external auditory canal evoked by stimulating sounds.
Among uncertain graph queries.
Ampoule injection is a routinely used treatment in hospitals due to its rapid effect after intravenous injection.
 During manufacturing.
Amr H Sawalha is Professor of Internal Medicine and Marvin and Betty Danto Research Professor of Connective Tissue Research at the University of Michigan.
An accurate algorithm for three-dimensional (3-D) pose recognition of a rigid object is presented.
 The algorithm is based on adaptive template matched filtering and local search optimization.
 When a scene image is captured.
An ad hoc wireless network is a set of nodes connected by wireless links in which nodes cooperate to forward packets from a source to a destination.
 Geographic routing (or position-based routing) has become an attractive solution for such networks since it reduces routing control overhead flooded in the network to construct routes (routes discovery).
 Many geographic routing protocols have been designed to guarantee packet delivery in such networks.
An advanced particle distribution controlling approach is proposed for laser melt injection process.
An air exposed single-chamber microbial fuel cell (SCMFC) using microalgal biocathodes was designed.
 The reactors were tested for the simultaneous biodegradation of real dye textile wastewater (RTW) and the generation of bioelectricity.
 The results of digital image processing revealed a maximum coverage area on the biocathodes by microalgal cells of 42%.
 The atmospheric and diffused CO2 could enable good algal growth and its immobilized operation on the cathode electrode.
 The biocathode-SCMFCs outperformed an open circuit voltage (OCV).
An algorithm and code for spectrum calculation for periodic nanostructures in homogeneous magnetic field are developed.
 The approach is based on the zero-range potentials model.
 The mathematical background of the model is based on the theory of self-adjoint extensions of symmetric operators.
An algorithm for automatic digitization of pluviograph strip charts is presented.
 The rainfall signal is incrementally extracted from the scanned image of a strip chart by combining the moving average method and the curve edge following method.
 The mechanical properties of float-based rain gauge were used as constraints in the algorithm design.
 The algorithm was tested on 58 strip chart images.
 The comparison between the data derived from the algorithm and the data from the Slovenian Environment Agency shows that the algorithm produces an accurate rainfall time series except for the charts that contain ink smudges.
An aluminum foam can be characterized by its architecture and by the solid phase' microstructure.
 Our aim is to link the foam's morphological and microstructural features with its mechanical properties thanks to X-ray tomography and finite element (FE).
 An approach combining X-ray tomography at different resolutions.
An ambulatory pre-screening Point-of-Care (POC) device compatible with commercially available diapers has been developed to rapidly screen urine samples for incontinent or functionally impaired elderly.
 This POC device consists of a set of colorimetric reaction pads with accompanying reference colors.
 A smartphone with camera is a convenient tool for analysis of colorimetric assays; and a software application has been developed for smartphones to photograph the colorimetric assay and classify colorimetric reactions according to the reference colors.
 To facilitate detection of multiple biomarkers.
An application of the (G (')/G)-expansion method to search for exact solutions of nonlinear partial differential equations is analysed.
 This method is used for Burgers.
An approach to performing linguistic summaries of graph datasets.
An attack graph depicts multiple-step attack and provides a description of system security vulnerabilities.
 It illustrates critical information necessary to identify potential weaknesses and areas for enhanced defense.
 Attack graphs include multiple attack paths.
An automated sensing and control system (hardware and software) was developed for real-time spot-application of granular fertilizer in mowed wild blueberry fields.
 The custom hardware system was incorporated into a commercial pneumatic granular fertilizer spreader.
 Custom software for the sensing and control system was developed by combining color co-occurrence matrix based texture analysis and g-ratio algorithms in C++ to acquire and process images in real-time to differentiate mowed wild blueberry plants from bare spots and weeds.
 The performance accuracy of the spot-applicable fertilizer spreader was evaluated both in laboratory simulation and real-time field tests.
 Simulation results reported that the accuracy of the developed system was 94.
 Real-time field tests reported that the system produced acceptable results at ground speeds of 1.
2 km h(-1) for the spot-application of fertilizer at target areas (in plant areas only) within the field.
 Results also indicated that the ground speed of 4.
8 km h(-1) was unacceptable.
An automatized procedure for the parameterization of fundamental equations of state (EOS) that are explicit in terms of the Helmholtz energy and are based on molecular simulation data is presented.
 The simulation runs are carried out via a cloud-based framework that combines multiple.
An ear prosthesis was designed in 3D computer graphics software and fabricated using a 3D printing process of polyvinylidene fluoride (PVDF) for use as a hearing aid.
 In addition.
An effective lane detection algorithm employing the Hough transform and inverse perspective mapping to estimate distances in real space is utilized to send steering control commands to a self-driving vehicle.
 The vehicle is capable of autonomously traversing long stretches of straight road in a wide variety of conditions with the same set of algorithm design parameters.
 Better performance is hampered by slowly updating inputs to the steering control system.
 The 5 frames per second (FPS) using a Raspberry Pi 2 for image capture and processing can be improved to 23 FPS with an Odroid XU3.
 Even at 5 FPS.
An effective method based on measuring the fiber orientation of yarn floats with two-dimensional Fourier transform (2-D FFT) is proposed to recognize the weave pattern of yarn-dyed fabric in the high-resolution image.
 The recognition process consists of four main steps: 1.
 High-resolution image reduction.
An effective way to learn computer programming is to sit side-by-side in front of the same computer with a tutor or peer.
An efficient exploitation of Distributed Computing Infrastructures (DCIs) is needed to deal with the data deluge that the scientific community is facing.
An efficient parallel algorithm for Caputo fractional reaction-diffusion equation with implicit finite-difference method is proposed in this paper.
 The parallel algorithm consists of a parallel solver for linear tridiagonal equations and parallel vector arithmetic operations.
 For the parallel solver.
An efficient procedure for calculating the electromagnetic fields in multilayered cylindrical structures is reported in this paper.
 Using symbolic computation.
An efficient routing with Quality of Service (QoS) guarantees for any safety traffic communication plays a vital role for the success of vehicle ad hoc networks (VANETs).
 Due to the connection less nature of such network.
An enhanced asymmetric cryptosystem for color image is proposed by using equal modulus decomposition (EMD) in the gyrator transform domains.
 In this scheme.
An enhanced version of a segmentation algorithm applied in X-ray images using a prior shape and a straightened boundary image (SBI) is proposed.
 In the SBI method.
An enormous growth in internet usage has resulted into great amounts of digital data to handle.
 Data sharing has become significant and unavoidable.
 Data owners want the data to be secured and perennially available.
 Data protection and any violations thereby become crucial.
 This work proposes a traitor identification system which securely embeds the fingerprint to provide protection for numeric relational databases.
 Digital data of numeric nature calls for preservation of usability.
 It needs to be done so by achieving minimum distortion.
 The proposed insertion technique with reduced time complexity ensures that the fingerprint inserted in the form of an optimized error leads to minimum distortion.
 Collusion attack is an integral part of fingerprinting and a provision to mitigate by avoiding the same is suggested.
 Robustness of the system against several attacks like tuple insertion.
An experimental technique based on time-averaged circular geometric moire for optical measurement of angular oscillations is presented in this paper.
 The pitch of the circular moire is preselected in such a way that angular oscillations of different amplitudes yield time-averaged moire fringes at different locations of the cover image.
 This optical effect enables to construct an optical scale for direct measurement of angular oscillations.
 The proposed technique is similar to visual cryptography.
An image intensifier installed in the optical path of a compact spectrometer may act not only as a fast gating unit.
An image processing technique using the proper orthogonal decomposition (POD) of infrared thermal data was developed to improve the speed of assessment of 2D heat source fields accompanying mechanical transformation.
 This method involved the generation of a reduced orthonormal basis to approximate thermal fields prior to heat source estimation.
 The robustness of the method was first assessed using a penalising benchmark test.
 This test involved artificially setting several tricky situations that arise in practice (high diffusivity.
An important factor underlying the entire EUD enterprise is how to incorporate basic computer programming in school curricula.
 Rapidly increasing initiatives towards this goal have typically explored two kinds of abilities associated with learning how to program: logical problem solving and digital (multimedia) storytelling.
 In this paper we report on an exploratory qualitative study with a group of middle school children from a one-semester computational thinking acquisition class.
 We combined three technologies with which participants: (i) created a game; (ii) explored the representation of implicit and explicit meanings in their game; and (iii) created a scripted asynchronous Web-based conversation with their teacher about their game.
 We concluded that this combination can not only introduce new forms of 1st-person expression through software in basic education.
An important number of academic tasks should be solved collaboratively by groups of learners.
 The Computer-Supported Collaborative Learning (CSCL) systems support this collaboration by means of shared workspaces and tools that enable communication and coordination between learners.
 Successful collaboration and interaction can depend on the criteria followed when forming the groups of learners.
 This paper proposes a method that analyses the collaboration and interaction between learners using a set of indicators or variables about how they solve academic tasks.
An important problem in the field of bioinformatics is to identify interactive effects among profiled variables for outcome prediction.
 In this paper.
An increasing number of wireless Internet users and deployed wireless access points over the past several years and have raised the importance of wireless security issues.
 The absolute majority of wireless users are not IT professionals.
An Omega-3 chicken egg is a chicken egg produced through food engineering technology.
 It is produced by hen fed with high omega-3 fatty acids.
An online visual analytical system based on Java Web and WebGIS for air quality data for Shanghai Municipality was designed and implemented to quantitatively analyze and qualitatively visualize air quality data.
 By analyzing the architecture of WebGIS and Java Web.
Analysis of data stored in a graph enables the discovery of certain information that could be hard to see if the data were stored using some other model (e.
 relational).
Analysis of DNA sequences is a data and computational intensive problem.
Analysts prototyping trading strategies often reuse previously computed values: both full problems and subproblems.
 Avoiding recomputing these would increase productivity.
 We built a memoization library that caches function computations to files to avoid recomputation.
 This should minimize the need for users to think about whether caching is appropriate while giving them control over speed.
Analytic two-dark soliton solutions for a variable-coefficient nonlinear Schrodinger equation are obtained via modified Hirota method.
 Parallel solitons are observed and soliton control such as the soliton compression is realized with different group velocity dispersion profiles.
Analytical solutions of partial differential equation (PDE) models describing reactive transport phenomena in saturated porous media are often used as screening tools to provide insight into contaminant fate and transport processes.
 While many practical modelling scenarios involve spatially variable coefficients.
Analytical solutions of the van der Waals normal form for fluidized granular media have been done to study the phase separation phenomenon by using two different exact methods.
 The Painleve analysis is discussed to illustrate the integrability of the model equation.
 An auto-Backlund transformation is presented via the truncated expansion and symbolic computation.
 The results show that the exact solutions of the model introduce solitary waves of different types.
 The solutions of the hydrodynamic model and the van der Waals equation exhibit a behavior similar to the one observed in molecular dynamic simulations such that two pairs of shock and rarefaction waves appear and move away.
Android malware has grown in exponential proportions in recent times.
 Smartphone operating systems such as Android are being used to interface with and manage various IoT systems.
Ankle bracelets (anklets) imposed by law to track convicted individuals are being used in many countries as an alternative to overloaded prisons.
 There are many different systems for monitoring individuals wearing such devices.
anomaly detection is playing an increasingly important role in network security.
Anonymous systems on the Internet aim to protect users from revealing to an external unauthorized entity their identities and their network activities.
 Despite using layered encryption.
Ant colony-based optimization approach is based on stigmergy behavior of natural insects.
 Ant Colony Optimization shows promising behavior on dynamic problems like Travelling Sales Person (TSP) problems and other TSP like problems.
 The paper discusses variants of ACO algorithms as well as measures their performance using TspAntSim simulation tool (Aybars U.
Antimicrobial peptides (AMPs) are indivisible part of the innate immune system in invertebrates.
 AMPs have been proven to have crucial role with a wide range of biological activities.
Antimicrobial peptides (AMPs) from cuticular extracts of worker ants of Trichomyrmex criniceps (Mayr.
Any information system built on a database can not function without data access control component.
 Role-Based Access Control (RBAC) models are the most acceptable for large database operating.
 There are several approaches to implementation of modified RBAC models that support hierarchical and temporal access control.
 This paper considers an approach of the implementation at the logical level of a database with a set of user role permissions tables and a set of data access control views.
 The tables and views are created and used according to defined rules of temporal access permissions distribution.
Any machine exposed to the Internet today is at the risk of being attacked and compromised.
 Detecting attack attempts.
Apache Hadoop is a widely used distributed computing framework and its file system is Hadoop Distributed File System (HDFS).
Apart from being able to support the bulk of student activity in suitable disciplines such as computer programming.
Apple leaf disease is one of the main factors to constrain the apple production and quality.
 It takes a long time to detect the diseases by using the traditional diagnostic approach.
Application integration (AI) is the process of creating impartially designed application systems that operate collectively.
 On the whole.
Application of fuzzy rule interpolation (FRI) has been escalating for making intelligent systems viable in many areas.
Application of microscopy to evaluate the morphology and size of filamentous proteins and amyloids requires new and creative approaches to simplify and automate the image processing.
 The estimation of mean values of fibrils diameter.
Application of smart grid produces massive data.
 Unfortunately.
Applications of process assessment have been established in various domains that require also safety considerations regarding processes.
Applications of remote sensing using unmanned aerial vehicle (UAV) in agriculture has proved to be an effective and efficient way of obtaining field information.
 In this study.
Applications that manipulate sparse data structures contain memory reference patterns that are unknown at compile time due to indirect accesses such as A [B [1]].
 To exploit parallelism and improve locality in such applications.
Applied computer laboratory lessons could be unproductive because of many students in there.
 Correcting students' mistakes one by one is wasting lesson time.
 Especially for beginners.
Apply machine learning principles to predict hip fractures and estimate predictor importance in Dual-energy X-ray absorptiometry (DXA)-scanned men and women.
 Dual-energy X-ray absorptiometry data from two Danish regions between 1996 and 2006 were combined with national Danish patient data to comprise 4722 women and 717 men with 5 years of follow-up time (original cohort n = 6606 men and women).
 Twenty-four statistical models were built on 75% of data points through k-5.
Appropriate structural analysis and optimization methods are of great significance for the conceptual design of automotive body-in-white (BIW) structure.
 This paper simplifies BIW structure as a spatial semi-rigid framed structure to provide early-stage predictions.
 Then a novel exact transfer stiffness matrix method (TSMM) is proposed for both static and dynamic analyses of three-dimensional semi-rigid framed structures.
 The matrix storage and equation solution techniques for large-scale engineering structures are also considered.
 Additionally.
Appropriately merging visual words are an effective dimension reduction method for the bag-of-visual-words model in image classification.
 The approach of hierarchically merging visual words has been extensively employed.
Approximate adders are widely being advocated as a means to achieve performance gain in error resilient applications.
 In this paper.
Arabinogalactan proteins (AGPs) belong to a class of Pro/Hyp-rich glycoproteins and are some of the most complex types of macromolecules found in plants.
 In the economically important plant species.
Architects around the world search for new forms of buildings.
 Many interesting shapes are derived from geometry and theory of surfaces.
 Among them the minimal surfaces discovered by Heinrich Scherk deserve special attention due to their esthetical values.
 One kind of them turns out to be especially suitable for shaping facades of tall buildings.
 The present paper makes use of Weierstrass-Enneper parameterization to construct these surfaces by explicit formulae.
 Selected surfaces are implemented in the symbolic computation software.
 Upon exporting the models to the CAD system one obtains possibility of designing sketches ready to use in the engineering practice.
 (C) 2015 The Authors.
 Published by Elsevier Ltd.
Architectural heterogeneity has proven to be an effective design paradigm to cope with an ever-increasing demand for computational power within tight energy budgets.
ArchiveDB is the data archive for all scientific and technical data collected at the Wendelstein 7-X project.
 It is a distributed system allowing continuous data archival.
 ArchiveDB has demanding requirements regarding performance efficiency (storage performance of 30 GB/s during experiments.
Areas within an agricultural field in the same season often differ in crop productivity despite having the same cropping history.
Argument-based machine learning (ABML) knowledge refinement loop offers a powerful knowledge elicitation tool.
ARINC 653 provides a strong isolation mechanism for safety computing fields.
ARM servers are becoming increasingly common.
Aroma of rice greatly affects palatability as well as consumer acceptability and is one of the main factors of rice quality.
 In this work.
Articular bone erosion in rheumatoid arthritis (RA) is mediated by the interaction between inflammation and pathways regulating bone metabolism.
 Inflammation promotes osteoclastogenesis and also inhibits osteoblast function.
Artists and animators have observed that children's movements are quite different from adults performing the same action.
 Previous computer graphics research on human motion has primarily focused on adult motion.
 There are open questions as to how different child motion actually is.
Arts & Bots combines intrinsically creative craft materials.
As 3D technology.
As a basic method for monitoring the activities of Internet applications.
As a commercial distributed computing mode.
As a fundamental optimization problem.
As a global optimization algorithm.
As a means to share knowledge.
As a mobile operating system framework.
As a next generation networking protocol.
As a novel learning algorithm for a single hidden-layer feedforward neural network.
As a powerful nonparametric Bayesian model.
As a preprocessing step for computer vision tasks.
As a result of rapid advances in technology and computer programming.
As a result of the application of a technique of multistep processes stochastic models construction the range of models.
As a result of the development of internet and ICT (information-centric technology) advances including mobile.
As a result of the rapid growth in popularity of portable digital devices in recent years.
As a result of the relief image surface pattern shapes are often very complicated.
As a signing capability delegation technique.
As a technique for image segmentation.
As a well-known clustering algorithm.
As an important pre-filtering procedure for object detection.
As an important pre-processing stage in many machine learning and pattern recognition domains.
As an interesting and emerging topic.
As an overlay cognitive radio.
As Automatic Test Systems continue to adopt architectures based on synthetic instrumentation and modular I/O platforms.
As Chronic Kidney Disease progresses slowly.
As computer networks are emerging in everyday life.
As computer programming and computational thinking (CT) become more integrated into K-12 instruction.
As computer systems become more complicated.
As distributed generators in distribution networks have brought much influence to the fault current.
As educators and researchers.
As GPUs become general purpose.
As is well known.
As location-aware applications and location-based services continue to increase in popularity.
As mobile social networks (MSNs) are booming and gaining tremendous popularity.
As motion sensors are getting light-weighted and low-priced.
As muscle activity during growth is considerably important for mandible quality and morphology.
As networking has become major innovation driver for the Internet of Things as well as Networks on Chips.
As networks applications are growing fast.
As non-volatile memory (NVM) technologies are expected to replace DRAM in the near future.
As one branch of stereo matching.
As one of the most classic fields in computer vision.
As one of the most developed intelligent operating systems on mobile devices.
As one of the most important tasks in computer vision.
As one of the most reliable technologies.
As one of the variations in frequent pattern mining.
As part of an effort to expand broadcasting services based on Japanese Sign Language (JSL).
As part of curriculum changes in South Africa.
As part of transmission network expansion planning (TNEP).
As recent heterogeneous system designs integrate general purpose processors.
As scientific and technological innovation has become a key component of economic growth.
As scientific applications become more data intensive.
As senescence develops.
As simulations are becoming popular in the analysis of the complex behavior of large-scale systems with immense inputs and outputs.
As synchrophasor data start to play a significant role in power system operation and dynamic study.
As the area of Software Engineering (SE) matures the role of human factors in software development is commonly recognized as important.
 Increasingly we see empirical studies that investigate the connection between.
As the basic part of the mechanical transmission.
As the conventional power systems turn towards smart grids (SGs) on a fast pace.
As the course of computer operating system requires students to understand the interactions between computer systems.
as the deceleration of processor scaling due to Moore's law accelerates research in new types of computing structures.
As the detection methods of covert channels can provide a better way to detect the existence of advanced persistent threat.
As the development level of economy and society improves and technology changes quickly.
As the development of sensor and machine learning technologies has progressed.
As the first component of SPARC (Simulation Package for Ab-initio Real-space Calculations).
As the need for faster power system dynamic simulations increases.
As the network size grows more and more.
As the popularity of software-defined networks (SDN) and OpenFlow increases.
As the prevalence of embedded systems in various fields has spread.
As the prevalence of social media on the Internet.
As the processing control and sensory evaluation of green tea are highly subjective and the tea industry is highly professionalized.
As the stages of emergency response shift.
As the variety of new social media applications are developed at an ever-increasing rate.
As the volumes of spatiotemporal trajectory data continue to grow at a rapid pace; a new generation of data management techniques is needed in order to be able to utilize these data to provide a range of data-driven services.
As video games are continuously growing in popularity and number of users.
As VLSI technology advances towards nanoscale devices.
As with other nature-inspired algorithms.
Asian soybean rust (ASR).
Assembling and simultaneously using different types of distributed computing infrastructures (DCI) like Grids and Clouds is an increasingly common situation.
 Because infrastructures are characterized by different attributes such as price.
Assessing the quality of aperture synthesis maps is relevant for benchmarking image reconstruction algorithms.
Assessment of attack surface is a formidable challenge for the present-day dynamic networks.
 Essentially.
Assessment of the physical properties of agricultural and food products has always been of major importance.
 The purpose of this study was to investigate the possibility of estimating physicochemical properties of raw cane sugars using digital image processing and colorimetric measurements.
 Various characteristics of raw cane sugars such as ash.
Assignment of 16S rRNA gene sequences to operational taxonomic units (OTUs) is a computational bottleneck in the process of analyzing microbial communities.
 Although this has been an active area of research.
Associated with the prime number p = 3.
Associated with the prime number p = 3.
Associative memories are data structures that allow retrieval of previously stored messages given part of their content.
Astronomical data does not always use Cartesian coordinates.
 Both all-sky observational data and simulations of rotationally symmetric systems.
Astronomy has a rich tradition of using color photography and imaging.
Astronomy is in an era where all-sky surveys are mapping the Galaxy.
 The plethora of photometric.
Asynchronous tasks in programming are those tasks executed free of context of the main task.
 Therefore asynchronous tasks are methods implemented in a non-blocking style.
At early phases of a product development lifecycle of large scale Cyber-Physical Systems (CPSs).
At nano/micro-scales.
At present MapReduce computing model-based Hadoop framework has gradually become the most famous distributed computing framework because of its remarkable features such as scalability.
At present very large volumes of information are being regularly produced in the world.
 Most of this information is unstructured.
At the beginning of the 21st century.
At the heart of databases is a data model referred to as a schema.
 Relational databases store information in tables.
At the present time virtual simulation has become a very popular subject in computer graphics.
 The main goal of the simulation in the context of computer graphics is reproducing the natural phenomena surrounding us in a more true-to-life possible way.
 When representing natural phenomena.
ATLAS software and computing is in a period of intensive evolution.
 The current long shutdown presents an opportunity to assimilate lessons from the very successful Run 1 (2009-2013) and to prepare for the substantially increased computing requirements for Run 2 (from spring 2015).
 Run 2 will bring a near doubling of the energy and the data rate.
Attack graphs model possible paths that a potential attacker can use to intrude into a target network.
 They can be used in determining both proactive and reactive security measures.
 Attack graph generation is a process that includes vulnerability information processing.
Attack graphs show possible paths that an attacker can use to intrude into a target network and gain privileges through series of vulnerability exploits.
 The computation of attack graphs suffers from the state explosion problem occurring most notably when the number of vulnerabilities in the target network grows large.
 Parallel computation of attack graphs can be utilized to attenuate this problem.
 When employed in online network security evaluation.
Attack intention recognition is to reason and judge the goal of attackers according to attack behavior and network environment.
 In order to deal with the dynamical character of offense-defense confrontation.
Attacks against web-based applications is one of the most serious network security threats.
 At present.
Attacks in network have caused a variety of serious problems.
Attempting to educate practitioners of computer security can be difficult if for no other reason than the breadth of knowledge required today.
 The security profession includes widely diverse subfields including cryptography.
Attention is a very important cognitive and behavioral process.
Attention-Deficit Hyperactive Disorder (ADHD) is one of the most common mental health disorders amongst school-aged children with an estimated prevalence of 5% in the global population (American Psychiatric Association.
Attic spaces are one of the most dynamic components in the building envelope.
Attribute reduction is one of the most fundamental and important topics in rough set theory.
 Uncertainty measures play an important role in attribute reduction.
 In the classical rough set model.
Attribute-based signature (ABS) schemes play a vital role to accomplish authentication and signer privacy simultaneously.
 In recent years.
Audio description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers.
 Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics.
 In this work we propose a novel dataset which contains transcribed ADs.
Augmented Reality & Mobile Augmented Reality has very widely used in nowadays Smartphone's and there is not any Software Architecture available.
 So when one application will develop for Mobile Augmented Reality will targeted only on one mobile handsets or device.
 When the device will change for some reason the application will require to Re-Architect because of its operating system and handle the network and GPS.
 Also the application will required to Install in device and require some kind of storage and some extra libraries to developed Mobile Augmented Reality System.
 In this paper propose algorithm for web based service oriented architecture for mobile augmented reality system.
 Using this prototype algorithm user can design any mobile augmented reality system application; also extend the services using current web services.
 A Framework is proposed in order to achieve any Application design using this Software Architecture will run on any mobile device platforms without the need for Installation or no Installation.
 Architecture will give the Web Based Interface so the User's have some kind of Network support and GPS Support will easily get the Application on any mobile Device.
 Also the framework will base on SOA so web services will design according to handle the Mobile data e.
Augmented reality allows to add virtual object in real scene.
 It has an increasing interest last years since mobile device becomes performant and cheap.
 The augmented reality is used in different domains.
Augmented TV is an augmented reality system for making TV video to appear to come out of the screen.
 With Augmented TV.
Authentication of user in online banking is a major issue in recent days where transactions are carried out using insecure Internet channel.
 The modern communication medium is very much exposed to various threats.
 One time password (OTP) is used to prove one's identity over the wireless channel.
 The OTP sent to user's registered mobile number as SMS is most commonly used technique for user authentication.
 OTP SMS sent normally as plain text is vulnerable to various attacks along the communication channel.
 To solve this problem.
Authentication schemes have been widely deployed access control and mobility management in various communication networks.
 Especially.
Authors often convey meaning by referring to or imitating prior works of literature.
Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder mainly showed atypical social interaction.
Autofocusing and feature detection are two essential processes for performing automated biological cell manipulation tasks.
 In this paper.
Automat layout detection of color yarns is necessary for weaving and producing processes of yarn-dyed fabrics.
 This study presents a novel approach to inspect the layout of color yarns of double-system-melange color fabrics automatically.
Automated analysis and assessment of students' programs.
Automated assessment systems are gaining popularity within computer programming courses.
 In this paper we perform an empirical evaluation of Mooshak.
Automated computer vision-based fire detection has gained popularity in recent years.
Automated systems based on programmable logic controllers (PLC) are still applied in discrete event systems (DES) for controlling and monitoring of industrial processes signals.
 PLC-based control systems are characterized for having physical input and output signals coming from and going to sensors and actuators.
Automatic computation of surface correspondence via harmonic map is an active research field in computer vision.
Automatic detection and recognition of traffic sign has been a topic of great interest in advanced driver assistance system.
 It enhances vehicle and driver safety by providing the condition and state of the road to the drivers.
Automatic detection of brain tumors in single-spectral magnetic resonance images is a challenging task.
 Existing techniques suffer from inadequate performance.
Automatic detection of salient objects in images has gained its popularity in computer vision field for its usage in numerous vision tasks in recent years.
 Depth information plays an important role in the human vision system while it is underutilized in most existing two-dimensional (2-D) saliency detection methods.
 In this letter.
Automatic identification of abnormal cervical cells.
Automatic image annotation has been an active topic of research in the field of computer vision and pattern recognition for decades.
 In this paper.
Automatic segmentation of subtitles is a novel research field which has not been studied extensively to date.
Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing.
 In this paper.
Automating fabric defect detection has a significant role in fabric industries.
Automotive systems are widely used in industry and our daily life.
 As the reliability of automotive systems is becoming a greater challenge in our community.
Autonomous adaptation in self-adapting embedded real-time systems introduces novel risks as it may lead to unforeseen system behavior.
 An anomaly detection framework integrated in a real-time operating system can ease the identification of such suspicious novel behavior and.
Autonomous landing has become a core technology of unmanned aerial vehicle (UAV) guidance.
Autonomous unmanned aerial vehicles (UAVs) often carry video cameras as part of their payload.
 Outdoor video captured by such cameras can be used to estimate the attitude and altitude of the UAV by detecting the location of the horizon in the video frames.
 This paper presents a video frame processing algorithm for estimating the pitch and roll of a UAV.
Average consensus is a widely used algorithm for distributed computing and control.
Avian scavengers are declining throughout the world.
Back pain and upper extremities injuries due to overexertion account for over twenty percent of leave days from work in the US.
 This explains why a vast amount of initiatives have been.
Back pain is one of the major musculoskeletal pain problems that can affect many people and is considered as one of the main causes of disability all over the world.
 Lower back pain.
Background and objective: According to the World Health Organization (WHO) epilepsy affects approximately 45-50 million people.
 Electroencephalogram (EEG) records the neurological activity in the brain and it is used to identify epilepsy.
 Visual inspection of EEG signals is a time-consuming process and it may lead to human error.
 Feature extraction and classification are two main steps that are required to build an automated epilepsy detection framework.
 Feature extraction reduces the dimensions of the input signal by retaining informative features and the classifier assigns a proper class label to the extracted feature vector.
 Our aim is to present effective feature extraction techniques for automated epileptic EEG signal classification.
 Methods: In this study.
Background and objective: Distinguishing cancer subtypes is critical for selecting the appropriate treatment strategy.
 Bioinformatics approaches have gradually taken the place of clinical observations and pathological experiments.
Background and Objective: Medical image processing can contribute to the detection and diagnosis of human body anomalies.
Background and Objective: Sarcomas are rare but highly aggressive tumors.
Background and objective: The isolated cardiomyocyte preparation is amenable to several experimental approaches not suitable to the myocardial tissue.
Background and objective: The manual transformation of DNA fingerprints of dominant markers into the input of tools for population genetics analysis is a time-consuming and error-prone task; especially when the researcher deals with a large number of samples.
 In addition.
Background and objective: The preoperative planning of bone fractures using information from CT scans increases the probability of obtaining satisfactory results.
Background and objective: The volume of healthcare data including different and variable text types.
Background and objective: To safely select the proper therapy for Ventricullar Fibrillation (VF) is essential to distinct it correctly from Ventricular Tachycardia (VT) and other rhythms.
 Provided that the required therapy would not be the same.
Background and Objective: Various digital pathology tools have been developed to aid in analyzing tissues and improving cancer pathology.
 The multi-resolution nature of cancer pathology.
Background and ObjectiveBreast cancer is one of the most common cancers.
Background and ObjectiveColorectal cancer (CRC) remains the second deadliest cancer in the United States.
 Several screening methods exist; however.
Background and ObjectivePeriodontal diseases are a major public health concern leading to tooth loss and have also been shown to be associated with several chronic systemic diseases.
 Smoking is a major risk factor for the development of numerous systemic diseases.
Background and objectives: Early-phase virtual screening of candidate drug molecules plays a key role in pharmaceutical industry from data mining and machine learning to prevent adverse effects of the drugs.
 Computational classification methods can distinguish approved drugs from withdrawn ones.
 We focused on 6 data sets including maximum 110 approved and 110 withdrawn drugs for all and nervous system diseases to distinguish approved drugs from withdrawn ones.
 Methods: In this study.
Background and objectives: Highly accurate classification of biomedical images is an essential task in the clinical diagnosis of numerous medical diseases identified from those images.
 Traditional image classification methods combined with hand-crafted image feature descriptors and various classifiers are not able to effectively improve the accuracy rate and meet the high requirements of classification of biomedical images.
 The same also holds true for artificial neural network models directly trained with limited biomedical images used as training data or directly used as a black box to extract the deep features based on another distant dataset.
 In this study.
Background Codes of ethics (CoE) are widely adopted in several professional areas.
BACKGROUND Dengue is considered one of the world's most important mosquito-borne diseases.
 MicroRNAs (miRNAs) are small non-coding single-stranded RNAs that play an important role in the regulation of gene expression in eukaryotes.
 Although miRNAs possess antiviral activity against many mammalian-infecting viruses.
BACKGROUND The electrocardiographically measured QT interval (QT) is heritable and its prolongation is an established risk factor for several cardiovascular diseases.
Background The transition of whole-exome and whole-genome sequencing (WES/WGS) from the research setting to routine clinical practice remains challenging.
 Objectives With almost no previous research specifically assessing interface designs and functionalities of WES and WGS software tools.
 A previous small study suggested that Brain Network Activation (BNA).
 Cystic fibrosis (CF) is a chronic catabolic disease often requiring hospitalization for acute episodes of worsening pulmonary exacerbations.
 Limited data suggest that vitamin D may have beneficial clinical affects.
 Energy efficiency is an increasingly important property of software.
 A large number of empirical studies have been conducted on the topic.
 Liver hepatocellular carcinoma accounts for the overwhelming majority of primary liver cancers and its belated diagnosis and poor prognosis call for novel biomarkers to be discovered.
 Multi-Scale Ranked Organizing Map coupled with Implicit Function as Squashing Time algorithm(MS-ROM/I-FAST) is a new.
 Rheumatoid arthritis (RA) is a chronic auto-inflammatory disorder of joints.
 The present study aimed to identify the key genes in RA for better understanding the underlying mechanisms of RA.
 The integrated analysis of expression profiling was conducted to identify differentially expressed genes (DEGs) in RA.
 Sensory-processing deficits appear crucial to the clinical expression of symptoms of schizophrenia.
 The visual cortex displays both dysconnectivity and aberrant spontaneous activity in patients with persistent symptoms and cognitive deficits.
 In this paper.
Background: A basic task in bioinformatics is the counting of k-mers in genome sequences.
 Existing k-mer counting tools are most often optimized for small k= 32.
 Our software is the result of an intensive process of algorithm engineering.
 It implements a two-step approach.
 In the first step.
Background: A common problem of some information technology courses is the difficulty of providing practical exercises.
 Although different approaches have been followed to solve this problem.
Background: A computational evolution system (CES) is a knowledge discovery engine that can identify subtle.
Background: A current focus of biofilm research is the chemical interaction between microorganisms within the biofilms.
 Prerequisites for this research are bioassay systems which integrate reliable tools for the planning of experiments with robot-assisted measurements and with rapid data processing.
Background: A gene regulatory network (GRN) represents interactions of genes inside a cell or tissue.
Background: A key challenge in human nutrition is the assessment of usual food intake.
 This is of particular interest given recent proposals of eHealth personalized interventions.
 The adoption of mobile phones has created an opportunity for assessing and improving nutrient intake as they can be used for digitalizing dietary assessments and providing feedback.
 In the last few years.
Background: A major challenge of bioinformatics in the era of precision medicine is to identify the molecular biomarkers for complex diseases.
 It is a general expectation that these biomarkers or signatures have not only strong discrimination ability.
Background: A number of dysregulated miRNAs have been identified and are proposed to have significant roles in the pathogenesis of type 2 diabetes mellitus or renal pathology.
 Alpinia oxyphylla has shown significant anti-inflammatory properties and play an anti-diabetes role.
 The objective of this study was to detect the alteration of miRNAs underlying the anti-diabetes effects of A.
 oxyphylla extract (AOE) in a type II diabetic animal model ( C57BIKsj db-/db-).
 Results: Treatment with AOE for 8 weeks led to lower concentrations of blood glucose.
Background: A standard task in pharmacogenomics research is identifying genes that may be involved in drug response variability.
Background: Aberrant activation of fibroblast growth factor receptor 3 (FGFR3) is frequently observed in bladder cancer.
Background: Advances in cloning and sequencing technology are yielding a massive number of viral genomes.
 The classification and annotation of these genomes constitute important assets in the discovery of genomic variability.
Background: Aligning multiple sequences arises in many tasks in Bioinformatics.
Background: Although meningioma is a common disease.
Background: Animal's efficiency in converting feed into lean gain is a critical issue for the profitability of meat industries.
 This study aimed to describe shared and specific molecular responses in different tissues of pigs divergently selected over eight generations for residual feed intake (RFI).
 Results: Pigs from the low RFI line had an improved gain-to-feed ratio during the test period and displayed higher leanness but similar adiposity when compared with pigs from the high RFI line at 132 days of age.
 Transcriptomics data were generated from longissimus muscle.
Background: Ascending thoracic aortic aneurysm (ATAA) is a major cause of morbidity and mortality worldwide.
 The pathogenesis of medial degeneration of the aorta remains undefined.
 High-throughput secretome analysis by mass spectrometry may be useful to elucidate the molecular mechanisms involved in aneurysm formation as well as to identify biomarkers for early diagnosis or targets of therapy.
 The purpose of the present study was to analyze the secreted/released proteins from ATAA specimens of both tricuspid aortic valve (TAV) and bicuspid aortic valve (BAV) patients.
 Methods: Aortic specimens were collected from patients undergoing elective surgery and requiring graft replacement of the ascending aorta.
 Each sample of the ascending aortic aneurysm.
Background: Astrocyte elevated gene-1 (AEG-1) is related to the tumorigenesis and deterioration of different cancers.
Background: Attachment theory has been proven essential for mental health.
Background: Autophagy is a conserved molecular pathway involved in the degradation and recycling of cellular components.
 It is active either as response to starvation or molecular damage.
 Evidence is emerging that autophagy plays a key role in the degradation of damaged cellular components and thereby affects aging and lifespan control.
 In earlier studies.
Background: Avian pathogenic E.
 coli (APEC) can lead to a loss in millions of dollars in poultry annually because of mortality and produce contamination.
 Studies have verified that many immune-related genes undergo changes in alternative splicing (AS).
Background: Bacteria present in cave often survive by modifying their metabolic pathway or other mechanism.
 Understanding these adopted bacteria and their survival strategy inside the cave is an important aspect of microbial ecology.
 Present study focuses on the bacterial community and geochemistry in five caves of Mizoram.
Background: Bacterial sRNA-mediated regulatory networks has been introduced as a powerful way to analyze the fast rewiring capabilities of a bacteria in response to changing environmental conditions.
 The identification of mRNA targets of bacterial sRNAs is essential to investigate their functional activities.
BACKGROUND: Because the graphical presentation and analysis of motif distribution can provide insights for experimental hypothesis.
BACKGROUND: Bipolar disorder (BD) is characterized by a dysregulation of affect and impaired integration of emotion with cognition.
 These traits are also expressed in probands at high genetic risk of BD.
 The inferior frontal gyrus (IFG) is a key cortical hub in the circuits of emotion and cognitive control.
Background: Bisulfite treatment of DNA followed by sequencing (BS-seq) has become a standard technique in epigenetic studies.
Background: Brain networks in fMRI are typically identified using spatial independent component analysis (ICA).
Background: Characterization of mature protein N-termini by large scale proteomics is challenging.
 This is especially true for proteins undergoing cleavage of transit peptides when they are targeted to specific organelles.
Background: Computational analysis of protein-protein interaction provided the crucial information to increase the binding affinity without a change in basic conformation.
 Several docking programs were used to predict the near-native poses of the protein-protein complex in 10 top-rankings.
 The universal criteria for discriminating the near-native pose are not available since there are several classes of recognition protein.
Background: Coronary computed tomography angiography (CTA) allows the evaluation of coronary plaque volume and low attenuation (lipid-rich) component.
Background: Current development of sequencing technologies is towards generating longer and noisier reads.
Background: Data capture for clinical registries or pilot studies is often performed in spreadsheet-based applications like Microsoft Excel or IBM SPSS.
Background: Data from biological samples and medical evaluations plays an essential part in clinical decision making.
 This data is equally important in clinical studies and it is critical to have an infrastructure that ensures that its quality is preserved throughout its entire lifetime.
 We are running a 5-year longitudinal clinical study.
Background: Depression is currently underdiagnosed among older adults.
 As part of the Novel Assessment of Nutrition and Aging (NANA) validation study.
Background: Discovery of clinical workflows to target for redesign using methods such as Lean and Six Sigma is difficult.
 VoIP telephone call pattern analysis may complement direct observation and EMR-based tools in understanding clinical workflows at the enterprise level by allowing visualization of institutional telecommunications activity.
 Objective: To build an analytic framework mapping repetitive and high-volume telephone call patterns in a large medical center to their associated clinical units using an enterprise unified communications server log file and to support visualization of specific call patterns using graphical networks.
 Methods: Consecutive call detail records from the medical center's unified communications server were parsed to cross-correlate telephone call patterns and map associated phone numbers to a cost center dictionary.
 Hashed data structures were built to allow construction of edge and node files representing high volume call patterns for display with an open source graph network tool.
 Results: Summary statistics for an analysis of exactly one week's call detail records at a large academic medical center showed that 912.
Background: During the last fifteen years.
Background: Dystonia is clinically and genetically heterogeneous.
 Despite being a first-line testing tool for heterogeneous inherited disorders.
Background: Feature selection methods are commonly used to identify subsets of relevant features to facilitate the construction of models for classification.
Background: Female moths synthesize species-specific sex pheromone components and release them to attract male moths.
Background: Flux analyses.
Background: Gene expression profiles of intestinal mucosa of chickens and pigs fed over long-term periods (days/weeks) with a diet rich in rye and a diet supplemented with zinc.
Background: Genomic interaction studies use next-generation sequencing (NGS) to examine the interactions between two loci on the genome.
BACKGROUND: Grapevine flower number per inflorescence provides valuable information that can be used for assessing yield.
 Considerable research has been conducted at developing a technological tool.
Background: Graph-based hierarchical clustering algorithms become prohibitively costly in both execution time and storage space.
Background: Graphical models have long been used to describe biological networks for a variety of important tasks such as the determination of key biological parameters.
Background: Hands-on training in point-of-care ultrasound (POC-US) should ideally comprise bedside teaching.
Background: Healthcare providers generate a huge amount of biomedical data stored in either legacy system (paper-based) format or electronic medical records (EMR) around the world.
Background: Hepatocellular carcinoma (HCC) is the most common liver malignancy worldwide.
Background: High throughput sequencing technologies have become fast and cheap in the past years.
 As a result.
Background: Hypertension or high blood pressure is on the rise.
 Not only does it affect the elderly but is also increasingly spreading to younger sectors of the population.
 Treating this condition involves exhaustive monitoring of patients.
 The current mobile health services can be improved to perform this task more effectively.
 Objective: To develop a useful.
Background: In a computed protein multiple sequence alignment.
Background: In acupuncture practice.
Background: In Quantum Chemistry.
Background: Infliximab shows good efficacy in treating refractory rheumatoid arthritis (RA).
Background: Japanese encephalitis virus (JEV) is a mosquito-borne flavivirus that causes Japanese Encephalitis (JE) and Acute Encephalitis Syndrome (AES) in humans.
 Genotype-I (as co-circulating cases with Genotype-III) was isolated in 2010 (JEV28.
Background: Knee dislocations often require multiple concurrent ligament reconstructions.
Background: LAT1 (SLC7A5) is the transport competent unit of the heterodimer formed with the glycoprotein CD98 (SLC3A2).
 It catalyzes antiport of His and some neutral amino acids such as Ile.
Background: Long non-coding RNAs (lncRNAs) have emerged as key players in a remarkably variety of biological processes and pathologic conditions.
Background: Lung cancer is the leading cause of cancer-related morbidity and mortality worldwide.
 Patients with chronic respiratory diseases.
Background: M2M (Machine-to-Machine) communications represent one of the main pillars of the new paradigm of the Internet of Things (IoT).
Background: Many plant pathogen secretory proteins are known to be elicitors or pathogenic factors.
Background: Mapping disease rates over a region provides a visual illustration of underlying geographical variation of the disease and can be useful to generate new hypotheses on the disease aetiology.
Background: Methods for in silico screening of large databases of molecules increasingly complement and replace experimental techniques to discover novel compounds to combat diseases.
 As these techniques become more complex and computationally costly we are faced with an increasing problem to provide the research community of life sciences with a convenient tool for high-throughput virtual screening on distributed computing resources.
 Results: To this end.
Background: Microbial genomes at the National Center for Biotechnology Information (NCBI) represent a large collection of more than 35.
Background: MicroRNAs (miRNAs) are a class of small non-coding RNAs that are strongly involved in various types of carcinogenesis.
Background: MicroRNAs (miRNAs) was reported to be involved in cancer radio-resistance.
Background: MiR-101-3p can promote apoptosis and inhibit proliferation.
Background: miR-181a is a small non-coding RNA known to be dysregulated in osteoarthritis (OA).
Background: Mobile health (mHealth) has huge potential to deliver preventative health services.
Background: Monoamine oxidase inhibitors (MAOIs) are being developed for major depressive disorder.
Background: Myocardial microvascular loss after myocardial infarction (MI) remains a therapeutic challenge.
 Autologous stem cell therapy was considered as an alternative; however.
Background: Next generation sequencing (NGS) technologies enable studies and analyses of the diversity of both T and B cell receptors (TCR and BCR) in human and animal systems to elucidate immune functions in health and disease.
 Over the last few years.
Background: Next Generation Sequencing (NGS) technologies provide exciting possibilities for whole genome sequencing of a plethora of organisms including bacterial strains and phages.
Background: Next-generation sequencing (NGS) allows ultra-deep sequencing of nucleic acids.
 The use of sequence-independent amplification of viral nucleic acids without utilization of target-specific primers provides advantages over traditional sequencing methods and allows detection of unsuspected variants and co-infecting agents.
Background: Next-generation sequencing (NGS) has revolutionized how research is carried out in many areas of biology and medicine.
Background: Next-generation sequencing is making it critical to robustly and rapidly handle genomic ranges within standard pipelines.
 Standard use-cases include annotating sequence ranges with gene or other genomic annotation.
Background: Obesity remains a major public health concern.
 Mobile apps for weight loss/management are found to be effective for improving health outcomes in adults and adolescents.
Background: One of the primary obstacles to the widespread adoption of openEHR methodology is the lack of practical persistence solutions for future-proof electronic health record (EHR) systems as described by the openEHR specifications.
 This paper presents an archetype relational mapping (ARM) persistence solution for the archetype-based EHR systems to support healthcare delivery in the clinical environment.
 Methods: First.
Background: Online social networks share similar topological characteristics as real-world social networks.
 Many studies have been conducted to analyze the online social networks.
Background: Organisms typically face infection by diverse pathogens.
Background: Ovarian cancer (OC) is a gynecological oncology that has a poor prognosis and high mortality.
 This study is conducted to identify the key genes implicated in the prognosis of OC by bioinformatic analysis.
 Methods: Gene expression data (including 568 primary OC tissues.
Background: Phenotypic data are routinely used to elucidate gene function in organisms amenable to genetic manipulation.
Background: Phenotypic studies in Triticeae have shown that low temperature-induced protective mechanisms are developmentally regulated and involve dynamic acclimation processes.
 Understanding these mechanisms is important for breeding cold-resistant wheat cultivars.
 In this study.
Background: Phenotyping is a critical component of plant research.
 Accurate and precise trait collection.
Background: Population structure inference using the software STRUCTURE has become an integral part of population genetic studies covering a broad spectrum of taxa including humans.
 The ever- expanding size of genetic data sets poses computational challenges for this analysis.
 Although at least one tool currently implements parallel computing to reduce computational overload of this analysis.
Background: Porphyromonas gingivalis is one of the etiological agents in the initiation of combined periodontal-endodontic (perio-endo) lesions.
 Successful treatment of perio-endo lesions with photo activated disinfection (PAD) as a novel therapeutic approach depends on the selection of an appropriate target site.
Background: Pre-mRNA splicing involves the stepwise assembly of a pre-catalytic spliceosome.
Background: Previously.
Background: Primary mediastinal large B-cell lymphoma (PMBL) shares pathological features with diffuse large B-cell lymphoma (DLBCL).
Background: Quantifying gene expression at single cell level is fundamental for the complete characterization of synthetic gene circuits.
Background: Radiogenetic therapy is a novel approach in the treatment of cancer.
Background: Reactome aims to provide bioinformatics tools for visualisation.
Background: Recently.
Background: Reduction in the cost of genomic assays has generated large amounts of biomedical-related data.
 As a result.
Background: Retrotransposons comprise a ubiquitous and abundant class of eukaryotic transposable elements.
 All members of this class rely on reverse transcriptase activity to produce a DNA copy of the element from the RNA template.
Background: Since the turn of the 21 st century.
Background: Smoking is a leading cause of preventable death.
 Early studies based on samples of twins have linked the lifetime smoking practices to genetic predisposition.
 The flavin-containing monooxygenase (FMO) protein family consists of a group of enzymes that metabolize drugs and xenobiotics.
 Both FMO1 and FMO3 were potentially susceptible genes for nicotine metabolism process.
 Methods: In this study.
Background: Software product line (SPL) scoping is an important phase when planning for product line adoption.
 An SPL scope specifies: (1) the extent of the domain supported by the product line.
Background: Statistical analysis and data visualization are two crucial aspects in molecular biology and biology.
 For analyses that compare one dependent variable between standard (e.
Background: Suffix arrays and their variants are used widely for representing genomes in search applications.
 Enhanced suffix arrays (ESAs) provide fast search speed.
Background: Symbiotic relationships between insects and bacteria are found across almost all insect orders.
Background: The 22q11.
2 deletion syndrome is the most common microdeletion syndrome in livebirths.
Background: The ability to analyze the genomics of malignancies has opened up new possibilities for off-label targeted therapy in cancers that are refractory to standard therapy.
 At Mayo Clinic these efforts are organized through the Center for Individualized Medicine (CIM).
 Results: Prior to GTB.
Background: The ability to reliably identify the state (activated.
Background: The Biological Magnetic Resonance Data Bank (BMRB) is a public repository of Nuclear Magnetic Resonance (NMR) spectroscopic data of biological macromolecules.
 It is an important resource for many researchers using NMR to study structural.
Background: The Biomembrane Force Probe is an approachable experimental technique commonly used for single-molecule force spectroscopy and experiments on biological interfaces.
 The technique operates in the range of forces from 0.
1 pN to 1000 pN.
 Experiments are typically repeated many times.
BACKGROUND: The detection of mutated epidermal growth factor receptor (EGFR) in non-small cell lung cancer (NSCLC) with residual cell pellets derived from liquid-based cytology (LBC) samples (eg.
Background: The determination and regulation of cell morphology are critical components of cell-cycle control.
Background: The etiology of strabismus has a genetic component.
 Our study aimed to localize the candidate causative gene mutant in a Chinese family with strabismus and to describe its underlying etiology.
 Material/ Methods: Genomic DNA was extracted from the affected individual and his parents in a Chinese pedigree with strabismus.
 The resulting exomes were sequenced by whole-exome sequencing.
 After variant calling and filtering.
Background: The fourth round of the Critical Assessment of Small Molecule Identification (CASMI) Contest (www.
 casmi-contest.
 org) was held in 2016.
Background: The house dust mite species Dermatophagoides farinae releases allergens that cause allergies and asthma worldwide.
 This study sought to clone and express the full-length cDNA encoding the group 9 allergen of D.
 farinae (Der f 9).
 Methods: The published sequence of Der f 9 was used to design primers for RT-PCR and RACE to obtain the full-length cDNA encoding Der f 9.
 After removal of signal peptide sequence.
Background: The human brain is the most complex system in the known universe.
Background: The human chromosome 19 miRNA cluster (C19MC) of 43 genes is a primate-specific miRNA cluster that may have biological significance in the genetic complexity of the primate.
 Despite previous reports on individual C19MC miRNA expression in cancer and stem cells.
Background: The increasing use of computers in science allows for the scientific analyses of large datasets at an increasing pace.
 We provided examples and interactive demonstrations at Dundee Science Centre as part of the 2015 Women in Science festival.
Background: The natural history of chronic hepatitisB(CHB) infection is divided into different phases including immune tolerance (IT).
Background: The parenteral nutrient (PN) mixtures may pose great risks of physical.
Background: The purpose of gene set enrichment analysis (GSEA) is to find general trends in the huge lists of genes or proteins generated by many functional genomics techniques and bioinformatics analyses.
 Results: Here we present SetRank.
Background: The statistical evaluation of pathway enrichment.
Background: Time-lapse microscopy is an essential tool for capturing and correlating bacterial morphology and gene expression dynamics at single-cell resolution.
 However state-of-the-art computational methods are limited in terms of the complexity of cell movies that they can analyze and lack of automation.
 The proposed Bacterial image analysis driven Single Cell Analytics (BaSCA) computational pipeline addresses these limitations thus enabling high throughput systems microbiology.
 Results: BaSCA can segment and track multiple bacterial colonies and single-cells.
Background: To extract more information.
Background: To our knowledge.
Background: Today.
Background: Traditional cine imaging for cardiac functional assessment requires breath-holding.
Background: Traditional Sanger sequencing has been used as a gold standard method for genetic testing in clinic to perform single gene test.
Background: Translational researchers need robust IT solutions to access a range of data types.
Background: Tuberculosis remains a major global threat.
 Two billion of the world's population is latently infected with Mycobacterium tuberculosis and is at the risk of progression to active disease.
 Bacillus Calmette-Guerin (BCG).
Background: Two-dimensional (2D) barcoding has the potential to enhance documentation of vaccine encounters at the point of care.
Background: Type 1 narcolepsy (NT1) is characterized by symptoms believed to represent Rapid Eye Movement (REM) sleep stage dissociations.
Background: Variants of unknown significance (VUSs) have been identified in BRCA1 and BRCA2 and account for the majority of all identified sequence alterations.
Background: Various medical fields rely on detailed anatomical knowledge of the distal radius.
 Current studies are limited to two-dimensional analysis and biased by varying measurement locations.
 The aims were to 1) generate 3D shape models of the distal radius and investigate variations in the 3D shape.
Background: Vasohibin 2 (VASH2) has previously been identified as an agiogenenic factor and a cancer related protein.
 Here we investigated the association of VASH2 expression and chemoresistance in pancreatic cancer.
 Methods: Immunohistochemical staining for VASH2 was performed on 102 human pancreatic cancer samples.
 Pancreatic cancer cell line models exhibiting overexpression or knockdown of VASH2 were generated.
 Gene expression analyses were carried out to determine genes differentially regulated by VASH2.
 Putative transcription factors that are downstream mediators of gene expression regulated by VASH2 were queried bioinformatically.
 Dual-luciferase reporter assays and ChIP assays were performed to confirm transactivation of target genes following VASH2 overexpression or knockdown.
 Results: VASH2 protein expression was higher in human pancreatic cancer than in paired adjacent tissues and elevated VASH2 levels were associated with gemcitabine chemoresistance.
 In cell line models of pancreatic cancer.
Background: Viral hepatitis C is an important global health problem that affects about 2.
2% of humans.
 Strategies on the control of this hepatotropic virus focused on chemotherapy and surveillance of emerging HCV drug resistant mutants.
Background: We investigated identifying patients with mild cognitive impairment (MCI) who progress to Alzheimer's disease (AD).
Background: When fitting statistical models for complex health outcome data; zero inflation.
Background: While next generation sequencing has enhanced our understanding of the biological basis of malignancy.
Background: Whole genome amplification techniques have enabled the analysis of unexplored genomic information by sequencing of single-amplified genomes (SAGs).
 Whole genome amplification of single bacteria is currently challenging because contamination often occurs in experimental processes.
Background: With the advancement of high-throughput technologies and enrichment of popular public databases.
Background: With the deepening of research.
BackgroundAutomated peripheral blood (PB) image analyzers usually underestimate the total number of blast cells.
BackgroundFollicular Unit Extraction (FUE) is considered to be a minimally invasive procedure.
BackgroundOrofacial clefts are congenital malformations of the orofacial region.
BackgroundThe liver is the major site for alcohol metabolism in the body and therefore the primary target organ for ethanol (EtOH)-induced toxicity.
 In this study.
Bacterial nanocellulose (BNC) is an emerging nanomaterial with a morphologic structure of a 3-D network and unique properties produced by several species of bacteria.
 The objective of the present work was to evaluate whether the addition of BNC improved the baking quality of wheat flours.
Base bleed is an important extended-range technology wherein low-speed mass flux is injected with high temperature by using an ammonium perchlorate (AP)-based burning composite propellant.
 The range of a projectile can be increased effectively by controlling the burning rate of the propellant.
 In this study.
Based on a bench-scale opposed multi-burner (OMB) gasifier and advanced visualization techniques.
Based on a Lie symmetry analysis.
Based on a ternary quantum logic circuit.
Based on comprehensive analysis of the structure and the potential safety problem of oil and gas SCADA(Supervisor control and data acquisition) network.
Based on generalized bilinear forms.
Based on logical GHZ states and logical Bell states.
Based on the extended homoclinic test technique and the Hirota's bilinear method.
Based on the extended three-wave approach and the Hirota's bilinear method.
Based on the in-depth analysis of the basic principle of non-stationary parallel hot wire method and existing measuring standards.
Based on the invariant subspace method.
Based on the Lax representation.
Basic computer programming is one of the fundamental subjects that students in the departments of computer engineering.
Basic graph structures such as maximal independent sets (MIS's) have spurred much theoretical research in randomized and distributed algorithms.
Basidiomycetous fungi.
Batch processes are of great importance in process industry.
Battery recovery effect is a phenomenon that the available capacity of a battery could increase if the battery can sleep for a certain period of time since its last discharging.
 Accordingly.
Bayesian networks (BN) have been used for decision making in software engineering for many years.
 In other fields such as bioinformatics.
Bayesianism is fast becoming the dominant paradigm in archaeological chronology construction.
 This paradigm shift has been brought about in large part by widespread access to tailored computer software which provides users with powerful tools for complex statistical inference with little need to learn about statistical modelling or computer programming.
 As a result.
B-cell novel protein-1 (BCNP1) or Family member of 129C (FAM129C) was identified as a B-cell-specific plasma-membrane protein.
 Bioinformatics analysis predicted that BCNP1 might be heavily phosphorylated.
 The BCNP1 protein contains a pleckstrin homology (PH) domain.
Because data dissemination is crucial to wireless sensor networks (WSNs).
Because of the open wireless channel.
Because the nodes in a wireless sensor network (WSN) are mobile and the network is highly dynamic.
Because there are many differences in the different operating systems of intelligent equipment.
Bed-load transport rate in gravel-bed rivers shows large non-Gaussian fluctuations.
Behaviors involving the interaction of multiple individuals are complex and frequently crucial for an animal's survival.
 These interactions.
Being a powerful instrument for modelling.
Being often deployed in remote or hostile environments.
Benefits of collaborative learning are established and gamification methods have been used to motivate students towards achieving course goals in educational settings.
Bent functions are actively investigated for their various applications in cryptography.
Bent functions are maximally nonlinear Boolean functions with an even number of variables.
 They have attracted a lot of research for four decades because of their own sake as interesting combinatorial objects.
Berta et al's uncertainty principle in the presence of quantum memory (Berta et al 2010 Nat.
 6 659) reveals uncertainties with quantum side information between the observables.
 In the recent important work of Coles and Piani (2014 Phys.
 A 89 022112).
Bertossi et al.
 proposed a data-cleaning technique based on matching dependences and matching functions.
Between 2008 and 2015.
Betweenness is a measure of the centrality of a node in a network.
Bidirectional reflectance distribution function (BRDF) estimation is one of the fundamental problems in computer graphics.
Big Data analytics is recognized as one of the major issues in our current information society.
Big Data consists of huge volume of complex growing data sets from several independent sources.
 With the rapid development of data collection and storage capacity.
Big Data deals with enormous volumes of complex and exponentially growing data sets from multiple sources.
 With rapid growth in technology.
Big Data has recently gained popularity and has strongly questioned relational databases as universal storage systems.
Big data has shifted the computing paradigm of data analysis.
 While some of the data can be treated as simple texts or independent data records.
Big data is a potential research area receiving considerable attention from academia and IT communities.
 In the digital world.
Big Data is a technology developed for 3-V management of data by which large volumes and different varieties of data would be processed in optimal velocity.
 The data to be dealt with may be structured or unstructured.
 Relational databases (spreadsheets) are typical examples of structured data and the methods.
Big Data is a term that has jumped overnight from its roots.
 It can be described as an innovative technique and technology to save.
Big Data is growing remarkably with technological development.
 In the field of business.
Big data is one of the most referred key words in recent information and communications technology industry.
 As the new-generation distributed computing platform.
Big data is popular in the areas of computer science.
Big data jobs are usually executed on large-scale distributed computing platforms that automatically divide a job into multiple computation phases.
Big data solution is an emerging trend in today's world as it provides the enterprises with certain advantages.
Bilingual education became popular in Chinese universities considered as an important aspect of connotative development.
 In this paper.
Binary Space Partitioning (BSP) is one of the most successful method of acceleration of ray tracing in computer graphics.
 BSP is applied in the ray tracing coupled with Geometrical Optics (GO) in the modeling of wireless propagation.
 The performance of acceleration by BSP is investigated.
 Then the result of GO accelerated by BSP is compared with a full-wave electromagnetic simulating method.
Binary tree traversal algorithm can be applied in many aspects.
Bioinformatics has grown very quickly for the last 20 years.
Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology.
Biological and environmental changes are creating a growing demand for historical and global data sets.
 Comparing up-to-date ecological and biological findings with historical statements has become a major part of scientific work in the field of ecology.
 This evaluation and comparison procedure is very time-consuming while the availability of raw data is very low.
 Comparisons between original findings if available require a lot of work from print publication to digitalization or transformation to appropriate data formats.
 The effective use of working capacity is a general issue and has become important.
Biological data science is an emerging field facing multiple challenges for hosting.
Biologics (disease modifying antirheumatic drugs.
Biometrics deals with authenticating a person's identity based on the physiological or behavioral characteristics.
 Visual cryptography (VC) is a promising information security technique that allows the secret sharing of images without any cryptographic computations.
 Various existing schemes were introduced for securing the raw biometric data and template in the database using the VC technique.
 The complexity of encryption plays a vital role in security improvement.
 In order to overwhelm the above limitations.
Biosensors for rapid environmental pollution detection can be designed with biomodule based on the bacterial bioluminescent system.
 Usually this method returns total value of toxicity and does not allow to distinguish pollutants types.
 Herein we demonstrate the classification of pollutants by the kinetic analysis utilizing artificial neural networks with multilayer perceptron architecture.
 The kinetics of light emission of NAD(P)H:FMN-oxidoreductase-luciferase bioluminescent reaction was measured for clean water and in the presence of three environment pollutants (1.
Bipedal locomotion has been an active area of research for many decades.
Blasting is important and an essential prerequisite in any opencast mine for fragmenting hard deposits.
 Blasting always produces unwanted effects like ground vibrations.
Blockchains represent a novel application of cryptography and information technology to age-old problems of financial record-keeping.
Bloom filters (BFs) are used in many applications for approximate check of set membership.
 Counting Bloom filters (CBFs) are an extension of BFs that enable the deletion of entries at the cost of additional storage requirements.
 Several alternatives to CBFs can be used to reduce the storage overhead.
 For example schemes based on d-left hashing or Cuckoo hashing have been proposed.
Bloom syndrome is a rare and recessive disorder characterized by loss-of-function mutations of the BLM gene.
Both network security and quality of service (QoS) consume computational resource of IT system and thus may evidently affect the application services.
 In the case of limited computational resource.
Both subspace learning methods and feature selection methods are often used for removing irrelative features from high-dimensional data.
 Studies have shown that feature selection methods have interpretation ability and subspace learning methods output stable performance.
 This paper proposes a new unsupervised feature selection by integrating a subspace learning method (i.
Botnet has become a popular technique for deploying Internet crimes.
 The command of botnet has evolved into a major way for attackers to launch Distributed Denial of Service attacks on network servers.
 Modelized analysis methods need to be studied for botnet attacks implements.
Botnets are one of the leading threats to network security nowadays and are used to conduct a wide variety of malicious activities.
Bound-state vector soliton solutions for the coupled variable-coefficient higher-order nonlinear Schrodinger equations.
Braille-character recognition is one of the foundational skills required for teachers of braille.
 Prior research has evaluated computer programming for teaching braille-to-print letter relations (e.
Brain data processing has been embracing the big data era driven by the rapid advances of neuroscience as well as the experimental techniques for recording neuronal activities.
 Processing of massive brain data has become a constant in neuroscience research and practice.
Brain network alterations in patients with Alzheimer's disease (AD) has been the subject of much investigation.
Branch-and-bound (B&B) is a popular approach to accelerate the solution of the optimization problems.
Brand communities and corporate social responsibility have been touted for their ability to both generate significant equity for their brands and strong bonds among community members.
Bread wheat is one of the major staple foods of worldwide population and iron plays a significant role in growth and development of the plant.
 In this report.
Breakthrough performances have been achieved in computer vision by utilizing deep neural networks.
 In this paper we propose to use random forest to classify image representations obtained by concatenating multiple layers of learned features of deep convolutional neural networks for scene classification.
 Specifically.
Breast cancer is a common cancer among women.
 With the development of modern medical science and information technology.
Breast cancer is the most common invasive cancer among women and its incidence is increasing.
 Risk assessment is valuable and recent methods are incorporating novel biomarkers such as mammographic density.
 Artificial neural networks (ANN) are adaptive algorithms capable of performing pattern-to-pattern learning and are well suited for medical applications.
 They are potentially useful for calibrating full-field digital mammography (FFDM) for quantitative analysis.
 This study uses ANN modeling to estimate volumetric breast density (VBD) from FFDM on Japanese women with and without breast cancer.
 ANN calibration of VBD was performed using phantom data for one FFDM system.
 Mammograms of 46 Japanese women diagnosed with invasive carcinoma and 53 with negative findings were analyzed using ANN models learned.
 ANN-estimated VBD was validated against phantom data.
Bringing quantum science and technology to the space frontier offers exciting prospects for both fundamental physics and applications such as long-range secure communication and space-borne quantum probes for inertial sensing with enhanced accuracy and sensitivity.
 But despite important terrestrial pathfinding precursors on common microgravity platforms and promising proposals to exploit the significant advantages of space quantum missions.
Broadcasting a message from one to many processors in a network corresponds to concurrent reading on a random access shared memory parallel machine.
 Computing the trees of a forest.
Broadly speaking.
Bug deduplication.
Bug reports are one of the most crucial information sources for software engineering offering answers to many questions.
Building detection from two-dimensional highresolution satellite images is a computer vision.
Building fractional mathematical models for specific phenomena and developing numerical or analytical solutions for these fractional mathematical models are crucial issues in mathematics.
Building oscillator-based computing systems with emerging nano-device technologies has become a promising solution for unconventional computing tasks like computer vision and pattern recognition.
Buildings tend to not operate as intended.
Built-in data structures are a key contributor to the performance of dynamic languages.
 Record data structures.
Butanol as an advanced biofuel has gained great attention due to its environmental benefits and superior properties compared to ethanol.
By block compressed sensing (BCS).
By combining the photorefractive spatial soliton waveguide of a Ce:SBN crystal with a coherent 4-f system we are able to manipulate the spatial frequencies of an input optical image to perform edge enhancement and direct component enhancement operations.
 Theoretical analysis of this optical image processor is presented to interpret the experimental observations.
 This work provides an approach for optical image processing by using photorefractive spatial solitons.
 (C) 2017 Elsevier B.
 All rights reserved.
By considering the loaded and empty moving tie of robots moving among loading station.
By employing the generalized ( G'/ G)- expansion method and symbolic computation.
By engaging in construction-based robotics activities.
By integrating next-generation sequencing (NGS).
By means of Miedema formation enthalpy model with Toop model.
By means of symbolic computation and Darboux transformation.
By representing a test sample with a linear combination of training samples.
By the reason of the variability of light and pedestrians' appearance.
By using GHZ-like states and entanglement swapping.
By using the Bell polynomials method and symbolic computation.
Byte addressable non-volatile memory (NVRAM) is likely to supplement.
C is a basic computer language widely used in both industry and education.
 Many computer programming beginners choose C as the first computer language to learn.
 But it is difficult for the beginners.
C is the most widely used programming language for developing embedded software.
Ca2+ plays a critical role in several processes involved in skeletal and cardiac muscle contraction.
 One key step in cardiac excitation-contraction (E-C) coupling is the activation of the cardiac ryanodine receptor (RYR2) by cytosolic Ca2+ elevations.
 Although this process is not critical for skeletal E-C coupling.
Cable is the most important component in cable-supported bridges and roof structures.
 Existing vibration methods for cable tension estimates are mainly based on measured acceleration responses.
 Such practice is relatively expensive and time-consuming due to the required installation of contact-type sensors and data acquisition systems.
 In this study.
Caching popular contents at mobile devices can potentially improve the quality of service for mobile users and relieve traffic burden of base station in cellular networks.
 In this paper.
CAD modelers enable designers to construct complex 3D shapes with high-level B-Rep operators.
 This avoids the burden of low level geometric manipulations.
 However a gap still exists between the shape that the designers have in mind and the way they have to decompose it into a sequence of modeling steps.
 To bridge this gap.
Calibration and validation of satellite observations are essential and on-going tasks to ensure compliance with mission accuracy requirements.
 An automated above water hyperspectral radiometer significantly augmented Australia's ability to contribute to global and regional ocean color validation and algorithm design activities.
 The hyperspectral data can be re-sampled for comparison with current and future sensor wavebands.
 The continuous spectral acquisition along the ship track enables spatial resampling to match satellite footprint.
 This study reports spectral comparisons of the radiometer data with Visible Infrared Imaging Radiometer Suite (VIIRS) and Moderate Resolution Imaging Spectroradiometer (MODIS)-Aqua for contrasting water types in tropical waters off northern Australia based on the standard NIR atmospheric correction implemented in SeaDAS.
 Consistent match-ups are shown for transects of up to 50 km over a range of reflectance values.
 The MODIS and VIIRS satellite reflectance data consistently underestimated the in situ spectra in the blue with a bias relative to the dynamic above water radiance and irradiance collector (DALEC) at 443 nm ranging from 9.
8 x 10(-4) to 3.
1 x 10(-3) sr(-1).
 Automated acquisition has produced good quality data under standard operating and maintenance procedures.
 A sensitivity analysis explored the effects of some assumptions in the data reduction methods.
Calpains are a family of intracellular cysteine proteases involved in various biological processes.
 Previously.
Camera motion estimation from observed scene features is an important task in image processing to increase the accuracy of many methods.
Campylobacter concisus is a bacterium that is associated with inflammatory bowel disease (IBD).
 Immunosuppressive drugs including azathioprine (AZA) and mercaptopurine (MP).
Cancer transcriptome analysis is one of the leading areas of Big Data science.
Card-based protocols enable us to easily perform cryptographic tasks such as secure multiparty computation using a deck of physical cards.
 Since the first card-based protocol appeared in 1989.
Cardinal trees (or tries of degree ) are a fundamental data structure.
Cardiovascular disease is one of the most rampant causes of death around the world and was deemed as a major illness in Middle and Old ages.
 Coronary artery disease.
Casein has been recognized as a good source of bioactive peptides that can be used for the production of functional food.
 In this study.
Celestial spectrum contains a great deal of astrophysical information.
 Through the analysis of spectra.
Cell tracking plays crucial role in biomedical and computer vision areas.
 As cells generally have frequent deformation activities and small sizes in microscope image.
Cellular automata (CA) and artificial neural networks (ANNs) have been used by researchers over the last three decades to simulate land-use change (LUC).
 While conventional CA and ANN models assign a cell to only one land-use class.
Cellular learning automata (CLA) is a distributed computational model which was introduced in the last decade.
 This model combines the computational power of the cellular automata with the learning power of the learning automata.
 Cellular learning automata is composed from a lattice of cells working together to accomplish their computational task; in which each cell is equipped with some learning automata.
 Wide range of applications utilizes CLA such as image processing.
Central to behavior and cognition is the way that sensory stimuli are represented in neural systems.
 The distributions over such stimuli enjoy rich structure; however.
Certain drugs are nitroaromatic compounds.
Cervical cancer is one of the most common types of cancer among women worldwide.
 In order to identify the microRNAs (miRNAs/miRs) and mRNAs associated with the carcinogenesis of cervical cancer.
Challenging optimisation problems are abundant in all areas of science and industry.
 Since the 1950s.
Change Impact Analysis is the process of determining the consequences of a modification to software.
Change points are abrupt variations in time series data.
 Such abrupt changes may represent transitions that occur between states.
 Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring.
Changes in the electric utility will necessitate new needs and opportunities for monitoring and controlling electric power consumption and generation.
 Technical solutions exploiting these opportunities and answering these needs would ideally preserve best practices like reliability.
Changes in the human pigmentary system can lead to imbalances in the distribution of melanin in the skin resulting in artefacts known as pigmented lesions.
 Our work takes as departing point biological data regarding human skin.
Changing trends and globalization has given rise to various challenges to the software industry.
 Today the repute of any software engineering is related to its quality and timely delivery of product.
 The repute also depends on how the industry keeps in pace with the new expertise and changing market situation.
 The factors like transparency.
Character animation based on motion capture provides intrinsically plausible results.
Characterizing humans' ability to discriminate changes in illumination provides information about the visual system's representation of the distal stimulus.
 We have previously shown that humans are able to discriminate illumination changes and that sensitivity to such changes depends on their chromatic direction.
 Probing illumination discrimination further would be facilitated by the use of computer-graphics simulations.
Chaste is an open-source C++ library for computational biology that has well-developed cardiac electrophysiology tissue simulation support.
 In this paper.
Chinese Herbal Medicine (CHM) plays a significant role in breast cancer treatment.
 We conduct the study to ascertain the relative molecular targets of effective Chinese herbs in treating stage IV breast cancer.
 Survival benefit of CHM was verified by Kaplan-Meier method and Cox regression analysis.
 A bivariate correlation analysis was used to find and establish the effect of herbs in complex CHM formulas.
 A network pharmacological approach was adopted to explore the potential mechanisms of CHM.
 Patients in the CHM group had a median survival time of 55 months.
Chinook salmon (Oncorhynchus tshawytscha) are external fertilizers that display sneak-guard alternative reproductive tactics.
 The larger hooknose males dominate mating positions.
Chitinases are varied sized proteins which have the ability to degrade chitin and are present in a huge range of organisms like fungi.
Chlamydophila pneumoniae.
City governments and energy utilities are increasingly focusing on the development of energy efficiency strategies for buildings as a key component in emission reduction plans and energy supply strategies.
 To support these diverse needs.
City models by 3D CG (Computer Graphics) are important in promoting public participation for smart city.
Class retrieval in gene expression microarray data analysis is highly challenging task.
 Because of high class imbalance.
Classification and numeric estimation are the two most common types of data mining.
 The goal of classification is to predict the discrete type of output values whereas estimation is aimed at finding the continuous type of output values.
 Predictive data mining is generally achieved by using only one specific statistical or machine learning technique to construct a prediction model.
 Related studies have shown that prediction performance by this kind of single flat model can be improved by the utilization of some hierarchical structures.
 Hierarchical estimation approaches.
Classification based on image sets has recently attracted increasing interests in computer vision and pattern recognition community.
 It finds numerous applications in real-life scenarios.
Classification plays a critical role in False Positive Reduction (FPR) in lung nodule Computer Aided Detection (CAD).
 To achieve effective recognition of nodule.
Classifying 3D measurement data has become a core problem in photogrammetry and 3D computer vision.
Clifford algebra or geometric algebra (GA) is a simple and intuitive way to model geometric objects and their transformations.
 Operating in high-dimensional vector spaces with significant computational costs.
Clinical and preclinical studies of cardiovascular calcification often require interpretation of images from histopathology.
Clinical assessment of pupil appearance and pupillary light reflex (PLR) may inform us the integrity of the autonomic nervous system (ANS).
 Current clinical pupil assessment is limited to qualitative examination.
Clinical data sharing between healthcare institutions.
Clinical decisions are sometimes based on a variety of patient's information such as: age.
Cloud computing has become a buzzword in the area of high performance distributed computing as it provides on demand access to shared resources over the Internet in a self-service.
Cloud computing is a distributed computing model that has lot of drawbacks and faces difficulties.
 Many new innovative and emerging techniques take advantage of its features.
 In this paper.
Cloud computing is a distributed computing paradigm that provides opportunities to solve large scale problems.
 Resource provisioning and pay per use are the main advantage of moving towards cloud.
 Even though there are many workflow scheduling algorithms available.
Cloud Computing is a new distributed computing paradigm that consists in provisioning of infrastructure.
Cloud computing is a paradigm in which information is permanently stored in servers on the Internet and cached temporarily on clients.
 Virtual private network (VPN) is the most widely used technology for secure cloud access.
 Unfortunately.
Cloud computing is a paradigm that provides scalable IT resources as a service over the Internet.
 Vulnerabilities in the cloud infrastructure have been readily exploited by the adversary class.
Cloud computing is a promising utility-based distributed computing environment in which resources (hardware/software) are offered as a service over the Internet on a pay per use basis.
 It involves elastic resource provisioning capabilities and hence charge only for those Cloud resources that are actually needed.
Cloud computing is a robust technology.
Cloud computing is a set of Information Technology services that are provided to a customer over a network on a leased basis and with the ability to scale up or down their service requirements.
 Cloud computing is a new general purpose internet-based technology through which information is stored in servers and provided as a service and on-demand to the clients.
 It builds on decades of research in virtualization.
Cloud Computing is a specialized form of Distributed computing in which the resources such as storage.
Cloud computing is an evolutionary model for distributed computing.
Cloud Computing is an Internet based Computing where virtual shared servers provide software.
Cloud Computing is one of the increasing technology that is connected with Grid Computing.
Cloud computing is one of the Information Technology services which are provided from IT infrastructures to application services.
 It is the combination of Distributed computing and virtualization technology using virtual machines.
Cloud computing is proven service delivery model over the internet.
 Network play's an important role during this service provisioning but Cloud network have major security issue during service delivery.
 Network security and reliability achieve together is much more difficult task.
 Now a day cloud traditional network is replaced by the programmable and unified software defined network which have separate control plane and data plane for managing network traffic.
 SDN have capability to reduce cost of networking device using network virtualization which have facilitate to hardware and software virtualization using NFV(Network Function Virtualization).
 SDN and NFV integration in cloud computing give power of virtualization and improve network security and service.
 So in this paper we can describe SDN and NFV and how both are integrate in Openstack cloud to minimize network attack surface.
Cloud Computing is the long envisaged vision of computing as a utility.
 Innovative advances in hardware.
Cloud computing offers the possibility to store and process massive amounts of remotely sensed hyperspectral data in a distributed way.
 Dimensionality reduction is an important task in hyperspectral imaging.
Cloud computing represents the most recent enterprise trend in information technology and refers to the virtualization of computing resources that are available on demand.
 Cloud computing saves cost and time for businesses.
Cloud computing which uses outsourcing and remote processing of applications first appeared about ten years ago.
 Cloud Computing built on research in virtualization.
Cloud computing.
Cloud enterprise resource planning (ERP) is a buzz in the information technology domain.
 Small and medium enterprises (SMEs) do not have the financial budget to invest in on-premise ERP solution.
 The use of cloud-based services for SMEs has led to widespread diffusion of technology.
 The two big stakeholders in the cloud ERP are cloud user and cloud vendor.
 This paper brings out the factors that are under the influence of these two stakeholders.
 Critical success factors that are influenced by the people in the organization are considered for the study.
 Compliance.
Cloud environments have become a standard method for enterprises to offer their applications by means of web services.
Cloud gaming has become a new trend for gamers to access high-end video games.
 By rendering games in the remote cloud and streaming video scenes to the users.
Cloud security has become one of the emergent issues because of the immense growth of cloud services.
 A major concern in cloud security is the insider threat because of the harm that it poses.
Cloud storage system provides facilitative file storage and sharing services for distributed clients.
 To address integrity.
Cloud-based mobile networks are foreseen to be a technological enabler for the next generation of mobile networks.
 Their design requires substantial research as they pose unique challenges.
Cloud-based Software-as-a-Service (SaaS) providers want to grow into the space of business process outsourcing (BPO).
 BPO refers to the systematic and controlled delegation of many steps of a company's business process.
 BPO is a new and important extension to SaaS.
Cluster analysis aims at classifying objects into categories on the basis of their similarity and has been widely used in many areas such as pattern recognition and image processing.
 In this paper.
Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).
 It is a main task of exploratory data mining.
Clustering data continues to be a highly active area of data analysis.
Clustering is an unsupervised learning algorithm.
 k-Means algorithm is one of the well-known and promising clustering algorithms that can converge to a local optimum in few iterative.
 In our work.
Clustering is one of the basic tasks in data mining and machine learning which aims at discovering hidden structure in the data.
 For many real-world applications.
Clustering is one of the important data mining issues.
Clustering is regarded as one of the significant task in data mining and has been widely used in very large data sets.
 Soft clustering is unlike the traditional hard clustering which allows one data belong to two or more clusters.
 Soft clustering such as fuzzy c-means and rough k-means have been proposed and successfully applied to deal with uncertainty and vagueness.
Clustering plays an important role in data mining.
Clusters with the CPU-MIC heterogeneous architecture are becoming more popular in recent years.
Coarse-grained reconfigurable architecture (CGRA) is a promising parallel computing platform that provides high performance.
Code cloning is one of the active research areas in the software engineering community.
 Specifically.
Coding has an image problem.
 Teenage girls just don't do it.
 In New South Wales Australia.
Coding is considered by educators to be a fundamental skill for everybody.
 The awareness campaigns launched worldwide in the last two years have attracted the attention of media and initiated tens of millions of people of all ages to computer programming.
 The availability of online visual programming platforms (like Scratch and Blockly) and freely accessible playful educational tools (like the ones offered by Code.
 org) have significantly contributed to lower the access barrier to coding.
Cognitive impairments.
Cognitive radio (CR) is an emerging technique to improve the efficiency of spectrum resource utilization.
 In CR networks.
Cognitive robots have started to find their way into manufacturing halls.
Cointegration analysis is particularly sensitive to outlying observations.
 Traditional robust approaches rely on parameter estimates based on weighting schemes to penalize aberrant units.
Collaborative Anomaly Detection (CAD) is an emerging field of network security in both academia and industry.
 It has attracted a lot of attention.
Collaborative applications with energy and low-delay constraints are emerging in various networked embedded systems like wireless sensor networks and multimedia terminals.
 Conventional energy-aware task allocation schemes developed for collaborative applications only concentrated on energy saving when making allocation decisions.
 Consequently.
Collaborative systems are well established solutions for sharing work among people.
 In computer graphics these workflows are still not well established.
Collision detection.
Colonoscopy exam images are useful to identify diseases.
Color image segmentation is a crucial step in many computer vision and pattern recognition applications.
 This paper introduces an adaptive and unsupervised approach based on Vorondi regions to solve the color image segmentation problem.
 The proposed method uses a hybrid of spatial and feature space Dirichlet tessellation followed by inter-Vorondi region proximal cluster merging to automatically find the number of clusters and cluster centroids in an image.
Colour detection plays an important role for many computer vision-based applications.
Combination of reverse transcription (RT) and deep sequencing has emerged as a powerful instrument for the detection of RNA modifications.
Combined Cycle Power Plant (CCPP) is one of the most efficient systems of energy conversion with different topping and bottoming cycles.
 One of the acceptable schemes.
Combined cycle power plants (CCPPs) are in operation with diverse thermodynamic cycle configurations.
 Assortment of thermodynamic cycle for scrupulous locality is dependent on the type of fuel available and different utilities obtained from the plant.
 In the present paper.
Combining high-resolution level set surface tracking with lower resolution physics is an inexpensive method for achieving highly detailed liquid animations.
 Unfortunately.
Commercial anti-virus software traditionally memorizes specific byte sequences (known as signatures) in the file contents of previously encountered malware.
Commodity operating system kernels are vulnerable to a wide range of attacks due to the large code base and broad attack surface.
 Mitigation mechanisms such as code signing.
Communication in global software development is hindered by language differences in countries with a lack of English speaking professionals.
 Machine translation is a technology that uses software to translate from one natural language to another.
 The progress of machine translation systems has been steady in the last decade.
 As for now.
Communication over the ether is by nature erratic.
Community detection is a common problem in various types of complex networks.
 With the emerging of large scale real networks like social networks.
Community editing is an effective tool for improving contributions in peer production communities like Wikipedia and question-answer (Q&A) communities.
Community-driven Question Answering (Q&A) platforms are gaining popularity now-a-days and the number of posts on such platforms are increasing tremendously.
Compared to the hydrostatic hydrodynamic model.
Compared with Computer Aided Design (CAD) to use computer graphics technologies to describe geometric information for the product design.
Compared with prime lenses.
Compared with vertical photogrammtry.
Comparing strings and assessing their similarity is a basic operation in many application domains of machine learning.
Comparing time surfaces at different integration time points.
Complex analytic invariants of hypersurface isolated singularities are considered in the context of symbolic computation.
 The motivations for this paper are computer calculations of mu*-sequences that introduced by B.
 Teissier to study the Whitney equisingularity of deformations of complex hypersurfaces.
 A new algorithm that utilizes parametric local cohomology systems is proposed to compute mu*-sequences.
 Lists of mu*-sequences of some typical cases are also given.
Complex information-processing systems.
Complex network is the abstract topology of a large number of nodes and edges in reality.
 How to reveal the influences of internal network topology on network connectivity and vulnerability characteristics is a hotspot of current research.
 In this paper.
Complex networks are heterogeneous relational data sets with nontrivial substructures and statistical properties.
 They are typically represented as graphs consisting of vertices and edges.
 The analysis of their intricate structure is relevant to many areas of science and commerce.
Complex software development projects rely on the contribution of teams of developers.
Complex traffic networks include a number of controlled intersections.
Complexity of Internet of Things (IoT) applications and difficulty to predict their behaviour in wireless communications make modelling of IoT units an important research topic.
 The IoT unit is considered here as the two-node IoT model that supports bi-directional wireless communications.
 There are many approaches to model security and energy awareness; however.
Complications of embedded applications are increasing.
 Within due time delivery to the market is also a pressure.
Component deployment is a combinatorial optimisation problem in software engineering that aims at finding the best allocation of software components to hardware resources in order to optimise quality attributes.
Component-based approaches are prevalent in software development for robotic applications due to their reusability and productivity.
 In this article.
Composite data sets measured on different objects are usually affected by random errors.
Composite penalties have been widely used for inducing structured properties in the empirical risk minimization (ERM) framework in machine learning.
 Such composite regularizers.
Compound files information hiding technology has become a hot research topic in the field of information security.
 This paper proposes an information hiding method for compound files based on structured storage.
 In the method.
Computation is classically studied in terms of automata.
Computational complexity of public key cryptography over sensor nodes is not anymore a blocking concern in modern devices which natively (and efficiently) support elliptic curve cryptography.
 The problem has rather shifted toward the significant airtime consumption required to exchange multiple messages and certificates so as to perform authentication and key agreement.
 This letter addresses such problem by exploiting implicit certificates (elliptic curve Qu-Vanstone).
 We specifically propose a novel key management protocol (KMP) which suitably integrates implicit certificates with a standard elliptic curve Diffie-Hellman exchange.
Computational fluid dynamic (CFD) calculations on geometrically complex domains such as porous media require high geometric discretisation for accurately capturing the tested physical phenomena.
Computational grids are established with the intention of providing shared access to hardware and software based resources with special reference to increased computational capabilities.
 Fault tolerance is one of the most important issues faced by the computational grids.
 The main contribution of this survey is the creation of an extended classification of problems that incur in the computational grid environments.
 The proposed classification will help researchers.
Computational thinking (CT).
Computational thinking ability is important in computer science education.
 It emphasizes abstraction and automation.
 For automation.
Computational thinking has been gaining new impetus in the academic community and in K-12 level education.
 Scratch is a visual programming environment that can be utilized to teach and learn introductory computing concepts.
 There are some studies investigating the effectiveness of Scratch for K-12 level education.
Computational thinking is an increasingly important focus in computer science or informatics curricula around the world.
Computational thinking.
Computational topology is a vibrant contemporary subfield and this article integrates knot theory and mathematical visualization.
 Previous work on computer graphics developed a sequence of smooth knots that were shown to converge point wise to a piecewise linear (PL) approximant.
 This is extended to isotopic convergence.
Compute-intensive applications have gradually changed focus from massively parallel supercomputers to capacity as a resource obtained on-demand.
 This is particularly true for the large-scale adoption of cloud computing and MapReduce in industry.
Computer aided x-ray microtomography is an increasingly popular method to investigate the structure of materials.
 Continuing improvements in the technique are resulting in increasingly larger data sets.
 The analysis of these data sets generally involves executing a static workflow for multiple samples and is generally performed manually by researchers.
 Executing these processes requires a significant time investment.
 A workflow which is able to automate the activities of the user would be useful.
 In this work.
Computer generated holography plays a main role in the contents generation for holographic displays and digital archiving of three-dimensional objects.
 The fully analytic mesh based computer generated holography finds exact complex optical field for each triangular mesh of the three-dimensional objects for given sampling interval in the hologram plane without any approximation.
Computer generated images are most easily generated as pinhole images whereas images obtained with optical lenses exhibit a Depth-of-Field (DOF) effect.
 This is due to the fact that optical lenses gather light across finite apertures whereas the simulation of a pinhole lens means that the light is gathered through an infinitesimal small aperture.
Computer graphics and image processing technologies for editing photographs taken by a user.
Computer Graphics has been the advent of our latest technology over the last few decades.
 In this paper WebGL framework has been used to create a 3D Virtual Tour of the University of Mauritius for the web.
 A Virtual Tour is designed to provide viewers with a more life-like 3D view of an existing location using advanced simulation techniques.
Computer graphics in combination with mobile devices finds many applications in the fields of entertainment.
Computer graphics remains one of the most exciting and rapidly growing computer fields.
 It includes geometry processing as a major part of it.
 Every element in Graphics can be processed using different algorithms for acquisition.
Computer has received more and more applications in various fields.
 In this paper.
Computer laboratories are some of the limited resources that South African universities are battling to improve students' access to.
Computer network is a dynamic entity whose state changes with the introduction of new services.
Computer network vulnerability analysis is a method of analysis and evaluation of network security beforehand.
 The attacks method has occurred in the network.
Computer networks consist of several assets such as hardware.
Computer numerical control (CNC) technology is a key technology in machine tools and is also the base of industrial unit computerization.
 CNC machines are operated by controllers.
Computer numerical control machine tool is a typical complex product related with multidisciplinary fields.
Computer programmers with dyslexia can be found in a range of academic and professional scenarios.
 Dyslexia may have the effect on a computer programmer of degrading the expected results during collaborative software development.
 These people may perform better using visual programming languages.
Computer Programming and related courses account for a large proportion in current information courses.
 Such courses typically include C/C++ language programming.
Computer Programming as a process that embodies the creation of an executable computer program for a given computational problem by analyzing the task and developing an algorithm that computes the desired result.
 Due to its complex and diverse nature.
Computer Programming competence is a good research field in which students of Computer Science can be assisted by an Intelligent Tutoring System (ITS).
 An ITS can guide the students in their learning process proposing the corresponding learning activities for each particular student.
 In this article.
Computer programming courses (C.
Computer programming courses aim to develop students' programing skills and problem solving abilities.
 But it cannot easily be achieved through traditional teaching method which focuses on boring grammar more than approach and thinking.
 Students should learn deeper.
 In this article.
Computer programming courses are very important specialized courses of computer science.
 And the teaching effects of these courses are closely related to the students' learning subjective initiative.
 This article took C programming course for example and proposed a set of teaching strategies which can promote the students' learning initiative.
 A bank of programming exercises which hides programming knowledge is designed and is integrated in an online judge system; Theory teaching is organized around the students' programming practice just like the idea of flipped classroom; Four special time nodes during the course of study are pointed out and corresponding measures are taken to continuously stimulate students' programming initiative.
 Practice has proved that such teaching strategies can well mobilize the enthusiasm of students.
Computer programming courses is a compulsory course in many universities.
 Learning this course is helpful to cultivate students' ability of analytical thinking and dealing with issues.
Computer programming education has practice-oriented as well as theory-oriented learning goals.
Computer programming has a technological part and a creative part; it involves specific technical aspects of programming languages and creative aspects to find the best solutions for different problem domains.
 The programming learning process encompasses a group of different teacher-student techniques that are put into practice.
 These techniques have the object of learning a programming language to solve real problems; in this learning process we must include good software development practices of analysis and design so the novice programmer disciplines himself into developing quality software.
 To improve the learning programming process we use techniques and methods of software development adapting them to the context of courses in programs of the curricula.
 There are different methods to help us develop quality software; this article is a case study of using PSP (Personal Software Process) method and XP (eXtreme Programming) techniques on curricula's first programming course for engineering students.
Computer programming is a challenge for students and a major reason why people avoid Computer Science courses.
 Investigating alternative teaching methods is essential to encourage students to learn and understand the concepts of programming.
 The use of games in learning and training is advocated and supported by many researchers due to its motivational and attractive features.
 This study focuses on an approach that supports the use of learning methodologies based on constructionist activities.
Computer programming is a challenging skill that students in computer science and related disciplines are expected to learn.
 Computer science educators and students are concerned about the failures in programming competency.
 Programming errors reflect various details of student conceptual understanding and programming skills developed.
 This paper attempts to predict the failures in programming comprehension and debugging skills based on programming errors generated by the learner.
 We conduct a mixed method approach with prepost test experimental design to evaluate the Java programming competency of the learner.
 We also compute the error metrics and supplement the course material to improve the competency through self-learning spoken tutorial workshops.
 The characterization of student programming patterns helps to identify at-risk students and determine specific interventions.
 We analyze the compilation errors.
Computer Programming is a core subject of almost all degrees of Engineering that is perceived as a very complex matter for the students.
Computer programming is a highly cognitive skill.
 It requires mastery of many domains.
 But in reality many learners are not able to cope with the mental demands required in learning programming.
 Thus it leads to rote learning and memorization.
 There are many reasons for this situation.
 However one of the main reasons is the nature of the novice learners who experience high cognitive load while learning programming.
 Given the fact that the novice learners lack well defined schema and the limitation of the working memory.
Computer programming is an important competence for engineering and computer science students.
 Teaching and learning programming concepts and skills have been recognized as being a big challenge to both teachers and students.
 Accordingly.
Computer programming is an important skill for engineering and computer science students.
Computer programming is complex and all personality factors might influence it.
 Personality factors are comprehensive but broad and.
Computer programming is essential in engineering education.
 We are developing a programming education support tool pgtracer in order to facilitate learning process of computer programming.
 Pgtracer utilizes fill-in-the-blank question composed of a pair of a source program and a trace table.
 We propose a set of feedback functions for the students in this paper.
 Pgtracer automatically collects learning log of the students when they fill a blank.
 The feedback functions provide various analysis result of the collected log so that students can easily understand their achievement level and weak points.
 Many students highly appreciate the feedback functions through an experimental evaluation of the functions.
Computer programming is not commonly taught to geographers as a part of geographic information system (GIS) courses.
Computer programming is notoriously difficult to learn.
 To this end.
Computer programming is one of the main skills that students gain when they graduate from computer science programs.
Computer programming is regarded as a difficult skill to learn both by researchers and often by learners themselves.
 Metacognition has been identified as an important factor to be a successful learner in learning computer programming.
 Metacognitive in educational psychology is generally described as monitoring and controlling activities of one's cognition.
 The researchers have examined the Metacognitive Awareness Inventory OM to identify how it relates to student academic achievement at school and universities.
 In this research work.
Computer programming learning requires declarative and procedural knowledge.
 Novice learners acquire programming declarative knowledge via lectures.
Computer programming offered in universities is intended to prepare and provide undergraduate students not only with technical knowledge.
Computer Programming remains a difficult discipline to teach.
 E-learning can help improve student engagement and outcomes but offerings designed to teach programming in a University context are rudimentary when compared to publicly available sites such as Code Academy.
 This paper describes NoobLab.
Computer programming skills in younger ages seem to be a promising and challenging aspect.
 Many visual programming tools have been developed in order to assist young students and to improve the current teaching practices and pedagogies.
 In this paper.
Computer programming subject is a core ingredient for most of the engineering disciplines.
 However for the first year engineering.
Computer programs.
Computer science (CS) activities for young students are widely used.
Computer Science students are usually enthusiastic about learning Artificial Intelligence (AI) due to the possibility of developing computer games that incorporate AI behaviors.
 Under this scenario.
Computer Supported Collaborative Learning (CSCL) aims to improve education by combining collaborative learning with modern information and communication technology.
 The opportunity exists to develop successful CSCL applications due to the increase in popularity of social networking and online gaming among students.
 In this chapter.
Computer vision-based human activity recognition (HAR) has become very famous these days due to its applications in various fields such as smart home healthcare for elderly people.
 A video-based activity recognition system basically has many goals such as to react based on people's behavior that allows the systems to proactively assist them with their tasks.
 A novel approach is proposed in this work for depth video based human activity recognition using joint-based motion features of depth body shapes and Deep Belief Network (DBN).
 From depth video.
Computer-Aided Diagnosis (CAD) of Alzheimer's disease (AD) has drawn the attention of computer vision research community over the last few years.
 Several attempts have been made to adapt pattern recognition approaches to specific neuroimaging data such as Structural MRI (sMRI) for early AD diagnosis.
 One strategy is to boost the discrimination power of such approaches by integrating complementary imaging modalities in a single learning framework.
 Diffusion Tensor Imaging (DTI) is a new and promising modality giving complementary information to the anatomical MRI.
Computer-assisted orthopedic surgery allows clinicians to have better results and decreases the number of early prosthetic replacements.
 Nevertheless.
Computer-based control systems.
Computerized obstructive sleep apnea detection is necessary to speed-up sleep apnea diagnosis and research and for assisting medical professionals.
Computers are the leading technology of the 21st century.
 Programming.
Computing centroidal Voronoi tessellations (CVT) has many applications in computer graphics.
 The existing methods.
Computing connected dominating sets (CDSs) have been widely used to construct virtual backbones in wireless sensor networks.
Computing deterministic performance guarantees is a defining issue for systems with hard real-time constraints.
Computing discrete geodesic distance over triangle meshes is one of the fundamental problems in computational geometry and computer graphics.
 In this problem.
Computing education researchers have become increasingly interested in leveraging log data automatically collected within computer programming environments in order to understand students' learning processes and tailor instruction to student needs.
 While data on students' programming activities has been positively correlated with their learning outcomes.
Computing has recently been introduced as a core subject in British schools.
Computing performance is one of the key problems in embedded systems for high-resolution face detection applications.
 To improve the computing performance of embedded high-resolution face detection systems.
Computing the fractal dimension (FD) can be a very time-consuming process.
Concept map model is a method that creates domain model by identifying the relationship between concepts in course contents.
 This study presents an adaptive intelligent web based learning system called OPCOMITS (Object Oriented Programming Tutor using Concept Map Model).
 OPCOMITS has a free domain model which can be regulated by an expert for any course.
 It uses concept map model to regulate the topic hierarchy.
Concerning the problems that assessment scope of present network security situation is limited.
Concerns about human-machine interaction are becoming one of the most important thrusts for the development of innovative and successful products.
 Interaction design methods and tools are already described in literature and available for designers.
Concrete cracks are the most important representation for evaluating the bridge health condition and conducting to take appropriate actions to optimize expenditure on maintenance and rehabilitation.
 In this paper.
Concurrent data structures that have fast and predictable performance are of critical importance for harnessing the power of multicore processors.
Concurrent hash tables are one of the most important concurrent data structures with numerous applications.
 Since hash table accesses can dominate the execution time of the overall application.
Concurrent with global economic development in the last 50 years.
Conditional differential cryptanalysis on NFSR-based cryptosystems was first proposed by Knellwolf et al.
 in Asiacrypt 2010 and has been successfully used to attack reduced variants of Grain v1.
 In this paper.
Configuration management practices allied to the quality management processes prove to be an indispensable solution in software development.
 This article presents a case study of a company that develops software for public management and implemented a process of configuration management.
Configuration options are widely used for customizing the behavior and initial settings of software applications.
Confirmation of pregnancy viability (presence of fetal cardiac activity) and diagnosis of fetal presentation (head or buttock in the maternal pelvis) are the first essential components of ultrasound assessment in obstetrics.
 The former is useful in assessing the presence of an on-going pregnancy and the latter is essential for labour management.
 We propose an automated framework for detection of fetal presentation and heartbeat from a predefined free-hand ultrasound sweep of the maternal abdomen.
 Our method exploits the presence of key anatomical sonographic image patterns in carefully designed scanning protocols to develop.
Conformal approach to anomaly detection was recently developed as a reliable framework of classifying examples into normal and abnormal groups based on a training data set containing only normal examples.
 Its validity property is that a normal example.
Conformal immersions are harmonic and numerically stable surfaces whose tangents scale isometrically.
Conformal maps between planar domains are an important tool in geometry processing.
Consider a complete communication network on n nodes.
 In synchronous 2-counting.
Consider a tree T on n nodes.
Considered the cooperation of the container truck and quayside container crane in the container terminal.
Considering that it's difficult to dealing with symbolic expressions which are stored in the form of strings.
Considering the actual needs of safety measures of explosion accidents or terrorist attacks in large complex buildings.
Considering the ultrashort optical soliton propagation in the non-Kerr media.
Considering transmission capabilities and renewable integration.
Constructing smooth surface representations from point clouds is a fundamental problem in geometric modeling and computer graphics.
Consumers want to ensure that their enterprise data is stored securely and obliviously on the cloud.
Consumption of bread and the demands concerning its quality features.
Consumption of energy in the large computing system is an important issue not only because energy sources are depleting fast but also due to the deteriorating environmental conditions.
 A computational grid is a large heterogeneous distributed computing platform which consumes enormous energy in the task execution.
 Energy-aware job scheduling.
Contemporary journalism explores new formulas at the international level to prepare information by means of transmedia story-telling and mobile devices.
Contemporary RDBMS-based systems for visualization of high-volume numerical data have difficulty to cope with the hard latency requirements and high ingestion rates of interactive visualizations.
 Existing solutions for lowering the volume of large data sets disregard the spatial properties of visualizations.
Content creation for realtime interactive systems is a difficult problem.
 In game development.
Content management systems (CMSs) are able to let people.
Contests are one of the best ways to teach.
 It serves as a gamification of the learning process.
 In the cyber security field there are two additional unique obstacles: the first is that we don't want to teach criminal activities and the second is that we actually don't really know what the future cyber world will actually need.
 Both this problems are solved by asking to solve hard out-of-the-box computer programming tasks that are correlated to the current cyber security techniques.
Context A number of systematic literature reviews and mapping studies (SLRs) covering numerous primary research studies on various aspects of agile software development (ASD) exist.
 Objective: The aim of this paper is to provide an overview of the SLRs on ASD research topics for software engineering researchers and practitioners.
 Method: We followed the tertiary study guidelines by Kitchenham et al.
 to find SLRs published between late 1990s to December 2015.
 Results: We found 28 SLRs focusing on ten different ASD research areas: adoption.
Context A software system's structure often degrades due to repetitive maintenance.
 To make a sustainable evolution of such systems.
Context Data miners have been widely used in software engineering to.
Context Exception handling has become popular in most major programming languages.
Context switch is an essential feature of modern operating systems.
 The purpose of context switch is to provide concurrency processing of multiple programs.
Context With the increasing popularity of the Systematic Literature Review (SLR) process.
 Information about shapes and spin states of individual asteroids is important for the study of the whole asteroid population.
 For asteroids from the main belt.
Context: According to the search reported in this paper.
Context: Agile approaches are an alternative for organizations developing software.
Context: An enormous number of papers (more than 70.
Context: Any newcomer or industrial practitioner is likely to experience difficulties in digesting large volumes of knowledge in software testing.
 In an ideal world.
Context: Business Process Model and Notation (BPMN) is the de facto standard for business process modeling.
 It was developed by the Object Management Group with support of the major organizations in the fields of software engineering and information systems.
 Despite its wide use.
Context: Combinatorial testing strategies have lately received a lot of attention as a result of their diverse applications.
 In its simple form.
Context: Component-based software engineering is aimed at managing the complexity of large-scale software development by composing systems from reusable parts.
 To understand or validate the behavior of such a system.
Context: Component-based software systems require decisions on component origins for acquiring components.
 A component origin is an alternative of where to get a component from.
 Objective: To identify factors that could influence the decision to choose among different component origins and solutions for decision-making (For example.
Context: Constructing bespoke software development methodologies for specific project situations has become a crucial need.
Context: Coordinating a software project across distances is challenging.
 Even without geographical and time zone distances.
Context: Currently.
Context: Global Software Development (GSD) presents significant challenges to share and understand knowledge required for developing software.
 Organizations are expected to implement appropriate practices to address knowledge-sharing challenges in GSD.
 With the growing literature on GSD and its widespread adoption.
Context: In requirements engineering phase of the software development life cycle.
Context: In this study we report on a Systematic Mapping Study (SMS) for Domain-Specific Languages (DSLs).
Context: It is an enigma that agile projects can succeed 'without requirements' when weak requirements engineering is a known cause for project failures.
 While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases.
 Objective: We have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities.
Context: Many researchers have argued that providing interoperability support only considering the format and meaning (i.
 syntax and semantic) of data exchange is not enough to achieve complete.
Context: Methods and processes.
Context: Modern software systems often are distributed.
Context: Over the past 50 years numerous studies have investigated the possible effect that software engineers' personalities may have upon their individual tasks and teamwork.
 These have led to an improved understanding of that relationship; however.
Context: Processes are central to the operation of many systems or organizations.
 Process-centric systems.
Context: Recent years have seen an increasing interest in general theories of software engineering.
 As in other academic fields.
Context: Research has shown the majority of real faults are complex and cannot be simulated with traditional First Order Mutants (FOMs).
 Higher Order Mutants (HOMs).
Context: Reuse can improve productivity and maintainability in software development.
 Research has proposed a wide range of methods and techniques.
 Are these successfully adopted in practice? Objective: We propose a preliminary answer by integrating two in-depth empirical studies on software reuse at two large software-producing companies.
 Method: We compare and interpret the study results with a focus on reuse practices.
Context: Several research efforts have been targeted to support architecture centric development and evolution of software for robotic systems for the last two decades.
 Objective: We aimed to systematically identify and classify the existing solutions.
Context: Software defect prediction (SDP) is an important task in software engineering.
 Along with estimating the number of defects remaining in software systems and discovering defect associations.
Context: Software engineering (SE) has a multidisciplinary and dynamic nature that makes it challenging to design its educational material.
 Guide to the software engineering body of knowledge (SWEBOK) which has evolved to become ISO/IEC 19759 standard has identified various knowledge areas to be part of any SE curricula.
 Although there is a number of studies that address the gap between SE curricula and software industry.
Context: Software Engineering (SE) is an evolving discipline with new subareas being continuously developed and added.
 To structure and better understand the SE body of knowledge.
Context: Software engineering for ubiquitous systems has experienced an important and rapid growth.
Context: Software engineering has experienced increased calls for attention to theory.
Context: Software library reuse has significantly increased the productivity of software developers.
Context: Software process improvement (SPI) is one type of innovation often formulated to address problems such as uncontrollable costs.
Context: The global software industry and the software engineering (SE) academia are two large communities.
Context: The increasing dependence of our society on software driven systems has led Software Reliability to become a key factor as well as making it a highly active research area with hundreds of works being published every year.
Context: The technical debt metaphor describes the effect of immature artifacts on software maintenance that bring a short-term benefit to the project in terms of increased productivity and lower cost.
Context: The trustworthiness of research results is a growing concern in many empirical disciplines.
 Aim: The goals of this paper are to assess how much the trustworthiness of results reported in software engineering experiments is affected by researcher and publication bias.
Context: There continues to be concern that research is not addressing the challenges that practice faces.
 For the benefit of academia and industry.
Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques.
Context: Traceability in Software Product Lines (SPL) is the ability to interrelate software engineering artifacts through required links to answer specific questions related to the families of products and underlying development processes.
 Despite the existence of studies to map out available evidence on traceability for single systems development.
Context-aware data tailoring studies the means for the system to furnish the users.
Context-awareness is a key feature in ubiquitous middleware.
Context-awareness is an essential component of systems developed in areas like Intelligent Environments.
Continuity and interpolation have been crucial topics for computer graphics since its very beginnings.
 Every time we want to interpolate values across some area.
Continuous monitoring of actual evapotranspiration (ET) is critical for water resources management at both regional and local scales.
 Although the MODIS ET product (MOD16A2) provides viable sources for ET monitoring at 8-day intervals.
Contracts feel misunderstood.
Contrary to the known benefits fromamoderate dietary reduction during adulthood on life span and health.
Control applications are implemented using real-time operating systems.
 Digital control theory is based on sampling intervals that have to be strictly met in order to get predictable behaviors.
Control systems for heating.
Controlling the geometric parameters of the weld pool and the weld bead is the main method for getting better welding quality in industrial robotic welding.
 This paper presents the technology on a real-time welding process sensor.
Controlling the moisture content of self-compacting concrete (SCC) mixtures during batching is important for assuring workability and meeting design standards.
Conventional chloride ingress prediction models rely on simplified assumptions.
Conventional multilevel modeling works well with purely hierarchical data; however.
Conventional online education platforms implement a combination of static and PTZ (Pan-Tilt-Zoom) cameras which are not only costly but also require operators and high computational power.
 In this paper.
Conventional reflow method in a convection oven is the principal step during the package assembly process in order to guarantee high productivity and make full use of the self-alignment.
 The package behavior.
Conventional taught learning practices often experience difficulties in keeping students motivated and engaged.
 Video games.
Converting geographic features (e.
Convolutional neural network (CNN).
Cooperative spectrum sensing.
Coordinated intrusion.
Countercurrent Flow Limitation (CCFL) was experimentally investigated in a 1/3.
9 downscaled COLLIDER facility with a 190 mm pipe's diameter using air/water at 1 atmospheric pressure.
 Previous investigations provided knowledge over the onset of CCFL mechanisms.
 In current article.
Counting the solution number of combinational optimization problems is an important topic in the study of computational complexity.
Coverage and connectivity are two important problems in wireless sensor network.
 This paper focuses on the wireless sensor network communication radius in the high density of sensor nodes deployed randomly and two times smaller than the sensing radius; put forward a distributed k coverage multi connected node deployment algorithm based on grid.
 Simulation results show that the algorithm in this paper while guaranteeing the wireless sensor network coverage and connectivity can reduce the number of the active state nodes effectively.
Covering generalized rough set theory is an important extension of classical rough set theory.
 To characterize a fuzzy set in a given covering approximation space.
Covering is a common form of data representation.
Covering problems are well studied in the Operations Research literature under the assumption that both the set of users and the set of potential facilities are finite.
 In this paper.
Covert channel in network protocols has been an area absorbing great interests for many years in secret transmission.
Covert channel is a major threat to the information system security and commonly found in operating systems.
Covert channels are communication channels to transmit information utilizing existing system resources without being detected by network security elements.
Covert channels are widely considered as a major risk of information leakage in various operating systems.
Covert channels provide means to conceal information transfer between hosts and bypass security barriers in communication networks.
 Hidden communication is of paramount concern for governments and companies.
Cracks are formed during the drying process of spaghetti production.
Craig interpolation has been a valuable tool in program analysis and verification.
 Modern SMT solvers implement interpolation procedures for the theories that are most commonly used in these applications.
Cricket system is a typical multivariable.
Critical node discovery plays a vital role in assessing the vulnerability of a computer network to malicious attacks and failures and provides a useful tool with which one can greatly improve network security and reliability.
 In this paper.
Critical onsets for draw resonance instability occurring in two-dimensional (2-D) film casting processes with Newtonian and viscoelastic (Upper-Convected Maxwell and Phan-Thien and Tanner) fluids have been newly determined using a transient frequency response method.
 Under a constant tension condition.
Crohn's disease (CD) is a chronic.
Crop simulation models are valuable tools for quantifying crop yield response to water.
Crowd simulation is studied extensively in computer graphics.
Crowds arise in a variety of situations.
Crowdsensing has attracted more and more attention in recent years.
Crowdsourcing is a new emerging distributed computing and business model on the backdrop of Internet blossoming.
 With the development of crowdsourcing systems.
Cryptanalysis refers to finding the plaintext from the given cipher text.
 The problem reduces to finding the correct key from a set of possible keys.
Cryptography has an important role in data security against known attacks and decreases or limits the risks of hacking information.
Cryptography via public key cryptosystems (PKC) has been widely used for providing services such as confidentiality.
Cubic splines in Euclidean space minimize the mean squared acceleration among all curves interpolating a given set of data points.
 We extend this observation to the Riemannian manifold of discrete shells in which the associated metric measures both bending and membrane distortion.
 Our generalization replaces the acceleration with the covariant derivative of the velocity.
 We introduce an effective time-discretization for this novel paradigm for navigating shell space.
 Further transferring this concept to the space of triangular surface descriptorsedge lengths.
Cultural heritage (CH) documentation tasks usually involve professionals from different knowledge areas.
Curcumin is a potent antitumor agent.
 The objective of this study was to explore the interaction between curcumin and PGK1.
Current accurate stereo matching algorithms employ some key techniques that are not suitable for parallel GPU architecture.
 It will be tricky and cumbersome to directly take these techniques into GPU applications.
 Trying to tackle this difficulty.
Current and future space missions demand highly reliable on-board computing systems.
Current applications.
Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization.
Current computer programs for intracellular recordings often lack advanced data management.
Current day networks operate on multiple hardware devices assisted by numerous numbers of operating systems.
 Systems may have vulnerabilities.
 These are explored and then exploited.
 Among the overall malicious activity countries.
Current geophysical techniques for visualizing seismic activity employ image reconstruction methods that rely on a centralized approach for processing the raw data captured by seismic sensors.
 The data is either gathered manually.
Current image processing techniques capture large vessels reliably but often fail to preserve connectivity in bifurcations and small vessels.
 Imaging artifacts and noise can create gaps and discontinuity of intensity that hinders segmentation of vascular trees.
Current programming practical classes are individual based.
Current smart devices (phones.
Current studies have presented multiple understandings or meanings to the concept of agile project management.
Current topics in e-Learning Management Systems encourage innovative technologies to use digital libraries to exploit Learning Objects metadata in order to facilitate dissemination and re-process of their content.
 Digital repositories are mostly implemented as distributed computing systems architectures dealing with major technological and modeling issues that hinder interoperability between heterogeneous databases impeding data accessibility and reusability.
 Learning Object metadata are lack of interactivity and address operational deficiencies.
Currently there are limited supportive and reliable commercial software packages for GNSS/INS data processing.
 The shortage of compatible GNSS/INS data processing software has become a bottleneck in the development of position and orientation systems (POS) for survey and mapping applications.
 Thus this paper introduces a GNSS/INS data processing software called Cinertial.
Customizing a desired naturalistic fluid simulation result from video to obtain similar artistic effect is significant in practice.
 But art-directed customizing is challengeable due to the chaotic nature of the physics contained in it.
Cutaneous squamous cell carcinoma (cSCC) is a malignancy of epidermal keratinocytes that is responsible for similar to 20% of annual skin cancer-associated mortalities.
 Accumulating evidence demonstrates that the dysregulation of micro (mi) RNAs serves a significant role in the tumorigenesis and progression of human cSCC.
 MicroRNA-31 (miR-31) is upregulated in cSCC and is involved in cSCC development.
Cutting plane algorithm (CPA) is a generalization of iterative first-order gradient method.
Cyber physical systems (CPS) are used to control chemical processes.
Cyber Physical Systems (CPSs) consist of hardware and software components.
 To verify that the whole (i.
Cyber-Physical Embedded Systems (CPESs) are distributed embedded systems integrated with various actuators and sensors.
 When it comes to the issue of CPES security.
Cyber-physical system (CPS) is an emerging area.
Cylindrical algebraic decomposition (CAD) is a standard tool in symbolic computation.
 In this paper we use it to compute a bound for the convergence rate for a numerical method that usually is merely resolved by numerical interpolation.
 Applying CAD allows us to determine an exact bound.
Cytochrome P450 (CYP) enzymes are heme-containing monooxygenases that catalyze metabolisms of various endogenous and exogenous compounds.
 They constitute a superfamily of enzymes present in various organisms including mammals.
Dagger is a modeling and visualization framework that addresses the challenge of representing knowledge and information for decision-makers.
Dastarcus helophoroides.
Data access delay has become the prominent performance bottleneck of high-end computing systems.
 The key to reducing data access delay in system design is to diminish data stall time.
 Memory locality and concurrency are the two essential factors influencing the performance of modern memory systems.
Data aggregation is recognized as a key method for reducing the amount of network traffic and the energy consumption on wireless sensor network nodes.
 Mobile agent (MA) technology represents a distributed computing paradigm which has been proposed as a means for increasing the energy efficiency of data aggregation tasks and addressing the scalability problems of centralized methods.
 Nevertheless.
Data centers hosting distributed computing systems consume huge amounts of electrical energy.
Data cleaning is a critical part of the data transformation stage in data warehousing where the extracted data from relational databases are usually unclean.
 This may affect critical tasks in different organizations such as data analysis and decision making.
 Current techniques of data cleaning generally deal with one or two quality aspects.
 The techniques assume the availability of master data.
Data clustering is a popular analysis tool for data statistics in many fields such as pattern recognition.
Data clustering is an important step in data mining and machine learning.
 It is especially crucial to analyze the data structures for further procedures.
 Recently a new clustering algorithm known as 'neutrosophic c-means' (NCM) was proposed in order to alleviate the limitations of the popular fuzzy c-means (FCM) clustering algorithm by introducing a new objective function which contains two types of rejection.
 The ambiguity rejection which concerned patterns lying near the cluster boundaries.
Data Communication and network have changed the way business and other daily affair works.
Data conversion has become an emerging topic in BigData era.
 To face the challenge of rapid data growth.
Data dependencies are a useful tool to design relational databases.
 In particular.
Data entry can result in errors that cause analytic problems and delays in disseminating research.
 Invalid responses can lead to incorrect statistics and statistical conclusions.
 The purpose of this article is to provide researchers some basic strategies for avoiding out-of-range data entry errors and streamlining data collection.
 This article identifies some basic strategies using Microsoft (R) Excel.
Data has become more and more important to individuals.
Data have become a very important asset to many organizations.
Data hiding is a wide field that is helpful to secure network communications.
 It is common that many data hiding researchers consider improving and increasing many aspects such as capacity.
Data Integration (DI) is the problem of combining a set of heterogeneous.
Data mining for intrusion detection is one of the most cutting-edge researches which focus on network security.
Data models are a central piece in information systems.
Data owners with large volumes of data can outsource spatial databases by taking advantage of the cost-effective cloud computing model with attractive on-demand features such as scalability and high computing power.
 Data confidentiality in outsourced databases is a key requirement and therefore.
Data processing complexity.
Data produced by wearable sensors is key in contexts such as performance enhancement and training help for sports and fitness.
Data Provenance is the history associated with that data.
 It constitutes the origin.
Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients.
 In this paper.
Data storage and information retrieval are some of the most important aspects when it comes to the development of a language corpus.
 Currently most corpora use either relational databases or indexed file systems.
 When selecting a data storage system.
Data Structure is an important and compulsory course in the computer science and engineering.
 The topics of the course require detailed view for various algorithms such as queues.
Data Structures and Algorithms are a central part of Computer Science.
 Due to their abstract and dynamic nature.
Data structures and algorithms are important foundation topics in computer science education.
Data structures and relations are becoming increasingly complex and difficult to assess and manage.
 Although automated rules and algorithms can be used for many data-mining tasks.
Data structures have been a core discipline in computer engineering studies.
 Several difficulties related to teaching and learning of these contents have been detected by the academic community.
 With the aim of obtaining a better knowledge of these situations.
Data structures have been explored for several domains of computer applications in order to ensure efficiency in the data store and retrieval.
Data Structures is an integral topic for any Computer Science or Software Engineering degree.
Data systems are typically categorized into source-of-truth systems that serve as primary stores for the user-generated writes.
Data Warehouse evolution is a critical problem in present scenario due to perpetual transactions and change in their structure arising out of continual evolving users' requirements.
 Handling properly all type of changes is a crucial process as it forms the core component of the modern DSS.
 Therefore DW has to be updated periodically according to different type of evolution of information sources.
 The problem of evolving an appropriate set of views is subjected to as the materialized view evolution problem.
 Many different materialized view evolution methods have been proposed in the literature to address this issue.
 This paper provides a survey of materialized view evolution methods.
 The paper aims at studying the materialized view evolution in relational databases and data warehouses as well as in a distributed setting.
 It defines an evolutionary approach for highlighting the materialized view evolution problem by identifying the three main dimensions that are the basis in the classification of materialized view evolution methods namely; (i) Framework.
Data warehouses are designed with a multidimensional structure based on fact and dimension tables.
Data warehousing is a traditional domain of relational databases.
Database auditing is one of the biggest issues in data security.
 Absence of information auditing drives the business applications to the lost trail of business procedures.
 To cope with auditing and in order to track operations and the actors of those operations in time.
Database models for road inventories are based on classical schemes for relational databases: many related tables.
Database systems play a very important role in computer science.
 Most software and mobile applications store and populate data on a server site.
 Relational databases dominate the professional market which requires students to be familiar with Structured Query Language (SQL).
 The client-server is the standard architecture in the database industry.
Database table having lots of records.
Database technology affects many disciplines beyond computer science and business.
 This paper describes two animations developed with images and color that visually and dynamically introduce fundamental relational database concepts and querying to students of many majors.
 The goal is for educators in diverse academic disciplines to incorporate the animations in their existing courses in order to meet their pedagogical needs.
 The introduction of the animations was assessed and evaluated within several contexts.
Databases are imperative for research in bioinformatics and computational biology.
 Current challenges in database design include data heterogeneity and contextdependent interconnections between data entities.
 These challenges drove the development of unified data interfaces and specialized databases.
 The curation of specialized databases is an ever- growing challenge due to the introduction of new data sources and the emergence of new relational connections between established datasets.
Data-dependent operations (DDOs) that were introduced by Moldovyan in 2003 (Moldovyan in MMM-ACNS 2003.
Data-driven models can be used as an efficient proxy to model complex concepts in engineering.
 It is common engineering practice to optimize some controllable input parameters in a model to increase efficiency of operations.
 Machine Learning can be used to predict the rate of penetration (ROP) during drilling to a great accuracy as shown by Hegde.
Data-intensive systems in e-governance collect and process data to ensure conformance to a set of business rules.
 Testers meticulously verify data in test databases.
Data-related businesses is an emerging trend in the recent decade.
DateView is a freeware desktop database system for the structured storage and retrieval of geochronological information.
 It provides a user-friendly interface for constructing queries based on information in the database so as to extract information on specific units.
Day by day network and internet applications is becoming very popular.
 Sensitive information requires security and safety measures.
 Security is the most challenging aspect in the internet and network applications.
 Encryption algorithm provides the necessary protection against the data intruders' attacks by converting information from its normal form into an unreadable form.
 The majority of current web authentication is built on username/password.
 And the password replacement offers more security.
DBMask is a system that implements encrypted query processing with support for complex queries and fine grained access control with create.
Decentralized monitoring and alarming systems can be an attractive alternative to centralized architectures.
 Distributed sensor nodes (e.
Decidualization of endometrial stromal cells is an important feature of implantation and pregnancy.
 The molecular mechanism underlying decidualization remains unclear.
Decomposing a divisor over a suitable factor basis in the Jacobian of a hyperelliptic curve is a crucial step in an index calculus algorithm for the discrete log problem in the Jacobian.
 For small genus curves.
Deep convolutional neural networks (CNNs) have been widely used to obtain high-level representation in various computer vision tasks.
Deep convolutional neural networks (DCNNs) have recently emerged as a dominant paradigm for machine learning in a variety of domains.
Deep convolutional neural networks have demonstrated breakthrough accuracies for image classification.
 A series of feature extractors learned from CNN have been used in other computer vision tasks.
Deep learning (DL) is a powerful state-of-the-art technique for image processing including remote sensing (RS) images.
 This letter describes a multilevel DL architecture that targets land cover and crop type classification from multitemporal multisource satellite imagery.
 The pillars of the architecture are unsupervised neural network (NN) that is used for optical imagery segmentation and missing data restoration due to clouds and shadows.
Deep learning has been the most popular feature learning method used for a variety of computer vision applications in the past 3 years.
 Not surprisingly.
Deep learning is a thing of tomorrow which is causing a complete drift from shallow architecture to deep architecture and an estimate shows that by 2017 about 10 % of computers will be learning rather than processing.
 Deep learning has fast growing effects in the area of pattern recognition.
Deep learning techniques for Sentiment Analysis have become very popular.
 They provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques (i.
Deep models have recently shown improved performance on numerous benchmark tasks in computer vision and machine learning.
 The availability of huge amount of digital data.
Deep Neural Networks for image/video classification have obtained much success in various computer vision applications.
 Existing deep learning algorithms are widely used on RGB images or video data.
Defect detection and classification of ceramic tile surface defects occurred in firing units are usually performed by human observations in most factories.
 In this paper.
Defect detection is becoming an increasingly important task during the manufacturing process.
 The early detection of faults or defects and the removal of the elements that may produce them are essential to improve product quality and reduce the economic impact caused by discarding defective products.
 This point is especially important in the case of products that are very expensive to produce.
 In this paper.
Defect management is a central task in software maintenance.
 When a defect is reported.
Defending against return-oriented programing (ROP) attacks is extremely challenging for modem operating systems.
 As the most popular mobile OS running on ARM.
Defining Unified Modelling Language (UML) profiles allows adaptation of the UML metamodel for specific domain.
Deliberate practice is important in many areas of learning.
Demand management in residential buildings is a key component toward sustainability and efficiency in urban environments.
 The recent advancements in sensor based technologies hold the promise of novel energy consumption models that can better characterize the underlying patterns.
 In this paper.
Dengue virus serotype 3 (DENV-3).
Denial of service (DoS) attack is purely malicious and commonly used to overwhelm a network system making network resources unavailable to legitimate users.
 One such DoS attack is to target the firewall system of the enterprise.
Dense motion field estimation is a key computer vision problem.
 Many solutions have been proposed to compute small or large displacements.
Dense stereo correspondence is a challenging research problem in computer vision field.
 To address the poor accuracy behavior of stereo matching.
Dependable complex systems often operate under variable and non-stationary conditions.
Derivation of input sequences for distinguishing states of a finite state machine (FSM) specification is well studied in the context of FSM-based functional testing.
 We present three heuristics for the derivation of distinguishing sequences for nondeterministic FSM specifications.
 The first is based on a cost function that guides the derivation process and the second is a genetic algorithm that evolves a population of individuals of possible solutions (or input sequences) using a fitness function and a crossover operator specifically tailored for the considered problem.
 The third heuristic is a mutation based algorithm that considers a randomly generated input sequence as a candidate solution.
Deriving products from a Feature Model (FM) for testing Software Product Lines (SPLs) is a hard task.
 It is important to select a minimum number of products but.
Deriving the spatial distribution of specific catchment area (SCA) from a gridded digital elevation model (DEM) is one of the most important issues in digital terrain analysis.
 Conventional methods usually estimate SCA for each cell using a flow direction algorithm.
Describing the dispersion decreasing fiber.
Design defects are symptoms of design decay.
Design of an effective and efficient fractional order PID (FOND) controller.
Design pattern is widely used in the software engineering field.
Design Patterns (DPs) are acknowledged as powerful conceptual tools to improve design quality and to reduce time and cost of the development process by effect of the reuse of good design solutions.
 In many fields (e.
Designated confirmer signature (DCS).
Designed with the goal of mimicking key features of real HPC workloads.
Designing effective menu systems is a key ingredient to usable graphical user interfaces.
 This task generally relies only on human ability in building hierarchical structures.
Designing efficient and effective keypoint descriptors for an image plays a vital role in many computer vision tasks.
 The traditional binary descriptors such as local binary pattern and its variants directly perform a binarization operation on the intensity differences of the local affine covariant regions.
Designing fast and scalable algorithm for mining frequent itemsets is always being a most eminent and promising problem of data mining.
 Apriori is one of the most broadly used and popular algorithm of frequent itemset mining.
 Designing efficient algorithms on MapReduce framework to process and analyze big datasets is contemporary research nowadays.
 In this paper.
Designing lightweight security protocols for cloud-based Internet-of-Things (IoT) applications for battery-limited mobile devices.
Designing local feature descriptors for 3D objects is a fundamental yet challenging task in 3D computer vision.
 Both geometry and spatial information descriptions are critical for a 3D local descriptor.
Designing programming environments for physical simulation is challenging because simulations rely on diverse algorithms and geometric domains.
 These challenges are compounded when we try to run efficiently on heterogeneous parallel architectures.
 We present Ebb.
Desktop grid systems are distributed computing paradigms which use the idle and underutilized processing cycles and memory of the desktop machines (hosts) to support large scale computations.
 These systems have inherent uncertainties because the hosts do not work under one administrative domain and can become unavailable at any given point in time.
 Desktop grid frameworks are based on client server model and employ various scheduling policies at both ends to handle the hostile desktop grid environment.
 At server end.
Desktop ToonTalk was first released twenty years ago and was successfully used by children as young as 3 to construct computer programs.
 Uniquely these programs are constructed by demonstration using game elements such as robots.
Despite extensive research having been conducted on the subject.
Despite great efforts in recent years to accelerate global illumination computation.
Despite marked progress over the past several decades.
Despite progress in perceptual tasks such as image classification.
Despite recent advances in distributed RDF data management.
Despite the existence and popularity of many new and classical computer languages.
Despite the fact that numerous online 3D virtual worlds (3DVWs) are implemented as elearning and e-training platforms.
Despite the growing demand by Brazilian companies for well-qualified professionals in Information Technology (IT).
Despite the huge amount of work devoted to the treatment of time within the relational context.
Despite the popularity of GPUs in high-performance and scientific computing.
Despite the popularity of JavaScript for client-side web applications.
Despite the recent advent of various radiographic imaging techniques.
Despite the unconditionally secure theory of the Quantum Key Distribution (QKD).
Detailed information on the distribution of airway diameters during bronchoconstriction in situ is required to understand the regional response of the lungs.
 Imaging studies using computed tomography (CT) have previously measured airway diameters and changes in response to bronchoconstricting agents.
Detecting cancer at an early stage is useful in better patient prognosis and treatment planning.
 Even though there are several preliminary tests and non-invasive procedures that are conducted for the detection of cancer of various organs.
Detecting correlated network flows aids with exposing the attackers who hide behind anonymous networks or stepping stones.
 More recent flow watermarking schemes can link flows.
Detecting different categories of objects in an image and video content is one of the fundamental tasks in computer vision research.
 Pedestrian detection is a hot research topic.
Detecting early enough the anomalous behavior of technical systems facilitates cost savings thanks to avoiding system downtimes.
Detecting fake accounts in online social networks (OSNs) protects both OSN operators and their users from various malicious activities.
 Most detection mechanisms attempt to classify user accounts as real (i.
Detecting latency-related problems in production environments is usually carried out at the application level with custom instrumentation.
 This is enough to detect high latencies in instrumented applications but does not provide all the information required to understand the source of the latency and is dependent on manually deployed instrumentation.
 The abnormal latencies usually start in the operating system kernel because of contention on physical resources or locks.
Detecting moving objects in a scene is a fundamental and critical step for many high-level computer vision tasks.
Detecting sentiment of sentences in online reviews is still a challenging task.
 Traditional machine learning methods often use bag-of-words representations which cannot properly capture complex linguistic phenomena in sentiment analysis.
Detecting the shape of the non-rigid molten metal during welding.
Detecting vehicles in aerial imagery plays an important role in a wide range of applications.
 The current vehicle detection methods are mostly based on sliding-window search and handcrafted or shallow-learning-based features.
Detection of data structures in spectral clustering approaches becomes a difficult task when dealing with complex distributions.
Detection of DDoS (Distributed Denial of Service) traffic is of great importance for the availability protection of services and other information and communication resources.
 The research presented in this paper shows the application of artificial neural networks in the development of detection and classification model for three types of DDoS attacks and legitimate network traffic.
 Simulation results of developed model showed accuracy of 95.
6% in classification of pre-defined classes of traffic.
Detection of moving objects in a video captured by a freely moving camera is a challenging problem in computer vision.
 Most existing methods often assume that the background (BG) can be approximated by dominant single plane/multiple planes or impose significant geometric constraints on BG.
Determination of conserved regions that plays vital roles on regulation of transcription and translation processes is one of the most challenging problems in bioinformatics.
Developers work together during software development and maintenance to resolve issues and implement features in large software projects.
 The structure of their development collaboration activity may have impact on the quality of the final product in terms of higher number of defects.
 In this paper.
Developing a bead shape to process parameter model is challenging due to the multi-parameter.
Developing a complex intelligent system by abstracting their behaviors.
Developing a video game is a costly activity.
Developing an automatic weeding system requires robust detection of the exact location of the crop to be protected from damage.
 Computer vision techniques can be an effective means of determining plant location.
 In this paper.
Developing distributed applications.
Developing legally compliant systems is a challenging software engineering problem.
Developing Natural Language Query Interface to Relational Databases has gained much interest in research community since forty years.
 This can be termed as structured free query interface as it allows the users to retrieve the data from the database without knowing the underlying schema.
 Structured free query interface should address majorly two problems.
 Querying the system with Natural Language Interfaces (NLIs) is comfortable for the naive users but it is difficult for the machine to understand.
 The other problem is that the users can query the system with different expressions to retrieve the same information.
 The different words used in the query can have same meaning and also the same word can have multiple meanings.
 Hence it is the responsibility of the NLI to understand the exact meaning of the word in the particular context.
 In this paper.
Developments in technology.
Diabetes disrupts the operation of the eye and leads to vision loss.
Diagnosis of a specific learning disability such as dysgraphia impacts children's academic progress and well-being.
 Dysgraphia is diagnosed by clinicians based on children's written product and educational staff's impressions.
 This process is time consuming and subjective.
 Consequently.
Differences in noise and density values in MDCT images obtained using ultra-low doses with FBP.
Different studies in the field of agricultural engineering have successfully related irrigation needs of plants with the percentage of green cover in crop images.
Different types of serious games have been used in elucidating computer science areas such as computer games.
Differential cryptanalysis is an effective tool in modern cryptanalysis.
 The differential chain of a Markov cipher forms a Markov chain.
Differential evolution (DE) research for multi-objective optimization can be divided into proposals that either consider DE as a stand-alone algorithm.
Diffuse intrinsic pontine glioma (DIPG).
DigiMathArt is an interdisciplinary method of teaching and learning that uses computer graphics and programming in order to make the mathematics concepts easier to understand.
 By combining arts.
Digital audio signal provides a large capacity for embedding hidden messages using digital steganography techniques.
 How to prevent hazardous steganography embedding on the Internet becomes an important task in the field of network security.
 For the Internet environment.
Digital content makes improvement in e-learning for transferring knowledge to learners.
 By e-learning that delivers digital contents.
Digital down converter (DDC) is a time-intensive and data-intensive computing task and considered as the key technology in software defined radio.
 This paper proposes a high-performance implementation of DDC on a graphics processing unit (GPU) using CUDA.
Digital Earth is a global reference model for integrating.
Digital image correlation (DIC) is one of the most widely used non-invasive methods for measuring full-field surface strains in a wide variety of applications.
 The DIC method has been used by numerous researchers for measuring strains during the plastic range of deformation where the strains are relatively large.
 The estimation of the amount of background strain error in the measurements is of prime importance for determining the applicability of this method for measuring small strains (such as the elastic strains in metals.
Digital Image Processing can be mentioned as computer image processing.
 It refers to a process within which image signals are transformed into digital ones and computers will be applied to the process of transformation so as to generate images that should be received by human vision or receiving devices of other types.
 Mathematics serves as the base of digital image processing and its primary missions include algorithm design and realization of the algorithm.
 By utilization of computers.
Digital image processing methods were applied on Landsat-8 data to differentiate different natural land covers and urban areas in Jizan.
Digital images may contain undesired blurred regions.
 Automatic detection of such regions and estimation of the amount of blurriness in a given image are important issues in many computer vision applications.
 This paper presents a simple and effective method to automatically detect blurred regions.
 The proposed method consists of two main parts.
Digital restoration of film content that has historical value is crucial for the preservation of cultural heritage.
Digital rights management problem has become a critical issue due to the increasing number of database applications.
 Proposed watermarking methods to deal with copyright management problem of multimedia cannot be applied to the databases directly.
 Some methods that watermark the databases use the date fields in the relation.
 In this work.
Digital signal processing algorithms are implemented using fixed point arithmetic due to expected area and power savings.
Digital tomosynthesis is a three-dimensional imaging technique with a lower radiation dose than computed tomography (CT).
 Due to the missing data in tomosynthesis systems.
Digital watermarking protocols are the one.
Dimension reduction is a crucial technique in machine learning and data mining.
Dimensionality reduction (DR) aims to reveal salient properties of high-dimensional (HD) data in a low dimensional (LD) representation space.
 Two elements stipulate success of a DR approach: definition of a notion of pairwise relations in the HD and LD spaces.
Dimensionality reduction is a challenging task for high-dimensional data processing in machine learning and data mining.
 It can help to reduce computation time.
Dipeptidyl peptidase IV (DPP-IV) is a promising Type 2 diabetes mellitus (T2DM) drug target.
 DPP-IV inhibitors prolong the action of glucagon-like peptide-1 (GLP-1) and gastric inhibitory peptide (GIP).
Discovering kinship relations from face images in the wild has become an interesting and important problem in multimedia and computer vision.
 Despite the rapid advances in face analysis in unconstrained environment.
Discovering of new and effective antibiotics is a major issue facing scientists today.
Discrete conformal mappings of planar triangle meshes.
Discrete cosine transform (DCT) based JPEG standard significantly improves the coding efficiency of image compression.
Discrete multiple signal classification (MUSIC) with its low computational cost and mild condition requirement becomes a significant noniterative algorithm for joint sparse recovery (JSR).
Discretization by rasterization is introduced into the method of images (MI) in the context of 3D deterministic radio propagation modeling as a way to exploit spatial coherence of electromagnetic propagation for fine-grained parallelism.
 Traditional algebraic treatment of bounding regions and surfaces is replaced by computer graphics rendering of 3D reflections and double refractions while building the image tree.
 The visibility of reception points and surfaces is also resolved by shader programs.
 The proposed rasterization is shown to be of comparable run time to that of the fundamentally parallel shooting and bouncing rays.
 The rasterization does not affect the signal evaluation backtracking step.
Disheveled-Axin domain containing 1 (DIXDC1) is involved in the development and progression of multiple cancers.
Distance measures are part and parcel of many computer vision algorithms.
 The underlying assumption in all existing distance measures is that feature elements are independent and identically distributed.
Distributed computing gives a backing to buyers to diminish their inner foundation.
Distributed computing guarantees to on a very basic level change the way we utilize PCs and get to and store our own specific and business data.
 With these new registering and correspondences models develop new data security challenges.
 Existing information security structures.
Distributed Computing has achieved tremendous development since cloud computing was proposed in 2006.
Distributed computing is a good alternative to expensive supercomputers.
 There are plenty of frameworks that enable programmers to harvest remote computing power.
Distributed computing strategy over a network of wireless sensors has emerged as a promising application in order to enable real-time structural health monitoring.
 Most previous studies of distributed computing strategy consider relatively simple operations like modal parameters identification (e.
 frequencies and modal shapes) that cannot directly assess structural health condition demanded by many practical engineering applications.
 This article focuses on developing distributed computing strategy techniques in wireless sensor networks used for direct stiffness estimation of structural damage locations and extent.
 An improved frequency-domain regression method is proposed to detect and quantify damage in frame structures.
 In the proposed method.
Distributed Denial of Service (DDoS) attacks always remain problematic for the security of Data centers.
Distributed Denial of Service (DDoS) is an austere menace to network security.
 The in-time detection of DDoS attacks poses a stiff challenge to network security professionals.
 In this paper.
Distributed Denial of Service attacks produce large volumes of spoofed network data.
 Manual analysis of gigabytes of network logs to determine source of the attacks.
Distributed denial-of-service (DDoS) attacks have become a weapon of choice for hackers.
Distributed query processing is of paramount importance in next-generation distribution services.
Distributed real-time systems present a particular challenge.
Distributed systems for big data management very often face the problem of load imbalance among nodes.
 To address this issue.
Distributed version control systems (D-VCSs - such as git and mercurial) and their hosting services (such as Github and Bitbucket) have revolutionalized the way in which developers collaborate by allowing them to freely exchange and integrate code changes in a peer-to-peer fashion.
Distributed video coding (DVC) provides a different compression paradigm that consists of lightweight encoder and complex decoder.
 This compression paradigm shed a light for nowadays mobile devices which equipped with limited battery and computing resources while the corresponding multimedia service is backed up by cloud computing infrastructures.
 After years of developing.
Distribution network analyses have been traditionally carried out by sequentially processing computational tasks.
Diverse molecules mediate cross-kingdomcommunication between bacteria and their eukaryotic partners and determine pathogenic or symbiotic relationships.
 N-acyl-L-homoserine lactone-dependent quorum-sensing signaling represses the biosynthesis of bacterial cyclodipeptides (CDPs) that act as auxin signal mimics in the host plant Arabidopsis thaliana.
 In this work.
Divisible e-cash systems allow users to withdraw a unique coin of value 2(n) units from a bank.
Division is one of the most important basic arithmetic operations.
DLV is a powerful system for Knowledge Representation and Reasoning which supports Answer Set Programming (ASP) - a logic-based programming paradigm for solving problems in a fully declarative way.
 DLV is currently widely used in academy.
DLV is a powerful system for knowledge representation and reasoning which supports Answer Set Programming (ASP) - a logicbased programming paradigm for solving problems in a fully declarative way.
 DLV is widely used in academy.
DNA palindromes are known to have many beneficial and detrimental functions in cell biology.
 Most of these functions are shared between perfect and imperfect palindromic sequences.
DNS (Domain name System) is one of the most prevalent protocols on modern networks and is essential for the correct operation of many network activities including the malicious operation.
 Monitoring the DNS traffic is an effective method to detect malicious activities.
 In this paper.
Document embedding is a technology that captures informative representations from high-dimensional observations by some structure-preserving maps over corpus and has been intensively explored in machine learning.
Document image segmentation into text lines is one of the stages in unconstrained handwritten document recognition.
 This paper presents a new algorithm for text line separation in handwriting.
 The developed algorithm is based on a method using the projection profile.
 It employs thresholding.
Domain Name System (DNS) traffic has become a rich source of information from a security perspective.
Double absorption heat transformer (DAHT) is a promising device in reducing the use of fossil fuels since it can utilize renewable sources or waste heat to provide high temperature energy.
 The absorber evaporator is an important component in the DAHT system.
Double-digested RADseq (ddRADseq) is a NGS methodology that generates reads from thousands of loci targeted by restriction enzyme cut sites.
Double-rotator-structure ternary optical processor (DRSTOP) has two characteristics.
Downy mildew pathogens affect several economically important crops worldwide but.
Dramatic technology progress in data manipulation induced several attempts of baleful and illegal processing.
 In this regard.
Driven by the dominance of the relational model and the requirements of modern applications.
Drogue detection is a fundamental issue during the close docking phase of autonomous aerial refueling (AAR).
 To cope with this issue.
Drosophila have served as a model for research on innate immunity for decades.
Drug resistance in tuberculosis predominantly.
Dry eye is an increasingly common disease in modern society which affects a wide range of population and has a negative impact on their daily activities.
Drying uniformity is one of the most important indicators in evaluating a drying technique as well as the final quality of dried products.
 In the current study.
DTE Energy Electric Company began a pilot project in 2009 to use power quality monitors to locate faults on numerous trunk lines and tie lines of its 24-kV and 41.
57-kV systems.
 Fault measurements captured by the meters are downloaded automatically.
Dual assignment clustering (DAC) has been recently proposed in computer vision.
Dual energy computed tomography (DECT) has significant impacts on material characterization.
Dual-energy X-ray imaging has a vast range of application in security.
 Luggage inspection is an essential process for an airplane or court house security as well as securing mass events.
 An image of a content of some package may help to figure out if there is any dangerous object inside and avoid possibly threatening situation.
 As the raw X-ray images are not always easy to analyze and interpret.
Due to an increase in the number of internet users.
Due to formal protection and commercial development.
Due to increase of unstructured data most of the companies now shifting towards NoSQL.
Due to its ability to eliminate the visual ambiguities in single-shot algorithms.
Due to its historical nature.
Due to its impact on society.
Due to lack of the Source-network-Ioad global perception and multiple time-scale situation awareness.
Due to network security situation affected by the threat degree of network attacks.
Due to rapid data growth.
Due to rapid developments in the research areas of medical imaging.
Due to technical bottlenecks and errors caused by artificial operation.
Due to that conventional teaching method mainly focus on the impartment and the memorization of knowledge points.
Due to the appearance of some new characteristics of the electric power industry such as widely interconnection.
Due to the broadcast nature of radio propagation.
Due to the complex in building experiment environment.
Due to the contention for the shared disk bandwidth.
Due to the diversity of geographical objects.
Due to the edge-preserving ability.
Due to the ever-increasing efficiency of computer systems.
Due to the extensive use of network services and emerging security threats.
Due to the fact that existing database systems are increasingly more difficult to use.
Due to the high availability of data.
Due to the high computing demand of whole-trip train dynamics simulations and the iterative nature of optimizations.
Due to the increasing complexity of software development activities.
Due to the increasing complexity of the surgical working environment.
Due to the increasing vulnerabilities in cyberspace.
Due to the massive parallel computing capability and outstanding image and signal processing performance.
Due to the network alarm data in cloud environment has the characteristics of massive.
Due to the proliferation of Web 2.
0 technology.
Due to the quick growth of data created and analyzed by industry and business requirements become more complex.
Due to their high peak performance and energy efficiency.
Duo is a general.
During an actual drilling process.
During an earthquake.
During image-guided cancer radiation treatment.
During recent years.
During the design of complex systems.
During the evolution procedure of GA.
During the folding process substrates are exposed to high-localized stresses.
During the last decades photogrammetric computer vision systems have been well established in scientific and commercial applications.
 Recent developments in image-based 3D reconstruction systems have resulted in an easy way of creating realistic.
During the last decades.
During the last few years.
During the last few years.
During the last years.
During the last years.
During the machining of thick.
During the past few decades there have been many examples where computer algebra methods have been applied successfully in the analysis and construction of numerical schemes.
Dynamic behavior of the propagation of incoherent optical spatial solitons in a nonlocal nonlinear medium is investigated.
 By means of the Hirota method.
Dynamic data structures in software applications have been shown to have a large impact on system performance.
 In this paper.
Dynamic environmental modelling of spatio-temporal systems often requires the representation of both fields and agents.
 Fields are continuous with values in the whole spatio-temporal domain of a model.
Dynamic modeling and simulation of the mooring system are the key technologies in anchor handling simulator (AHS).
 Built up the mooring line's dynamics model based on lumped-mass method (LMM).
Dynamic nature of mobile ad hoc networks combined with their lack of centralized infrastructure make security problem the most challenging issue in such networks.
 The main contribution of this paper is to propose a secure and efficient key management algorithm.
Dynamic network visualization has been a challenging research topic due to the visual and computational complexity introduced by the extra time dimension.
 Existing solutions are usually good for overview and presentation tasks.
Dynamic optimization problems (DOPs) have attracted increasing attention in recent years.
 Analyzing the fitness landscape is essential to understand the characteristics of DOPs and may provide guidance for the algorithm design.
 Existing measures for analyzing the dynamic fitness landscape.
Dynamic priority pollutant (PP) fate models are being developed to assess appropriate strategies for limiting the release of PPs from urban sources and for treating PPs on a variety of spatial scales.
 Different possible sources of PP releases were mapped and both their release pattern and release factors were quantified as detailed as possible.
 This paper focuses on the link between the gathered PP sources data and the dynamic models of the urban environment.
 This link consists of (1) a method for the quantitative and structured storage of temporal emission pattern information.
Dynamic programming is a demanding algorithm design technique.
 In this article.
Dynamic programming is an algorithm design technique that is very difficult to learn and apply.
 In this paper.
Dynamic projection mapping for moving objects has attracted much attention in recent years.
Dynamic simulation for transient stability assessment is one of the most important.
Dynamic systems are becoming steadily more important with the profusion of mobile and distributed computing devices.
 Coincidentally incremental computation is a natural approach to deal with ongoing changes.
 We explore incremental computation in the parameterized complexity setting and show that incrementalization leads to non-trivial complexity classifications.
 Interestingly.
Dzhafarov and Kujala (2015) have introduced a contextual probability theory called Contextuality-by-Default (C-b-D) which is based on three principles.
 The first of these principles states that each random variable should be automatically labelled by all conditions under which it is recorded.
 The aim of this article is to relate this principle to block structured computer programming languages where variables are declared local to a construct called a scope.
 Scopes are syntactic constructs which correspond to the notion of condition used by C-b-D.
 In this way a variable declared in two scopes can be safely overloaded meaning that they can have the same label but preserve two distinct identities without the need to label each variable in each condition as advocated by C-b-D.
 By means of examples.
Early work on call centers focused on opinion collection.
Earthquake-induced landslides are serious natural hazards that shocked us with tremendous casualties and great economic losses in many mountainous areas around the world.
Echo state networks (ESNs).
Economics of information security has recently become a rapidly growing field of research that is vitally important for managing the decisions and behaviors in cyberspace security.
 This field provides valuable insights not only for security experts.
Ecopath with Ecosim (EWE) is a widely applied food web model that is mostly known as desktop software for the Microsoft Windows platform.
 The freely available Microsoft.
NET source code of EwE.
Ecosystem modeling is a critically important tool for environmental scientists.
Edge detection is an integral component of image processing to enhance the clarity of edges in an image.
 Detection of edges for an image may help for image segmentation.
Edge-preserving weighted median filtering is a fundamental operator in a great variety of image processing and computer graphics applications.
 This paper presents a novel real-time weighted median filter which effectively smoothes out high-frequency details while preserving major edges.
 We propose an edge-aware data structure.
Educating our future engineers so that they can gain high proficiency in computational thinking is essential for their career prospects.
 As educators.
Educational systems are incorporating in their official curricula new knowledge related to computational thinking Education authorities consider that there are economic.
Educators and parents alike are seeking innovative ways to introduce young students to computer programming.
 The hope is to capture children's attention and foster learning at the same time.
 The goal of this work was to not only introduce elementary students to the fundamentals of computer programming.
Effective software effort estimation is one of the challenging tasks in software engineering.
 There have been various alternatives introduced to enhance the accuracy of predictions.
 In this respect.
Effective team communication is a prerequisite for software quality and project success.
 It implies correctly elicited customer requirements.
Effectively exploring and browsing document collections is a fundamental problem in visualization.
 Traditionally.
Effects of the quintic nonlinearity for the ultrashort optical pulse propagation in a non-Kerr medium.
Efficiency improvement is of great significance for simulation-driven antenna design optimization methods based on evolutionary algorithms (EAs).
 The two main efficiency enhancement methods exploit data-driven surrogate models and/or multi-fidelity simulation models to assist EAs.
Efficient airport detection and aircraft recognition are essential due to the strategic importance of these regions and targets in economic and military construction.
 In this paper.
Efficient and precise real-time video-based vehicles supervision systems for traffic surveillance are ever demanding.
 Developing an accurate detection and tracking algorithm by incorporating the effects of vehicles shadow or visual obstacles/occlusion including road signs.
Efficient computational models that retain essential physics of the associated continuous mathematical models are important for several applications including acoustic horn optimization.
 For heterogeneous wave propagation models that are naturally posed on unbounded domains.
Efficient processing of aggregated range queries on two-dimensional grids is a common requirement in information retrieval and data mining systems.
Efficiently and effectively detecting shell-like structures of particular shapes is an important task in computer vision and image processing.
 This paper presents a generalized possibilistic c-means algorithm (PCM) for shell clustering based on the diversity index of degree-lambda proposed by Patil and Taillie [Diversity as a concept and its measurement.
 J Amer Statist Assoc.
 1982;77:548-561].
 Experiments on various data sets in Wang [Possibilistic shell clustering of template-based shapes.
 IEEE Trans Fuzzy Syst.
 2009;17:777-793] show that the the proposed generalized PCM performs better than Wang's [Possibilistic shell clustering of template-based shapes.
 IEEE Trans Fuzzy Syst.
 2009;17:777-793] possibilistic shell clustering method according two two criteria: (i) the 'grade of detection' g(d) for each target cluster; (ii) the amount of computation.
Efficiently simulating light transport in various scenes with a single algorithm is a difficult and important problem in computer graphics.
 Two major issues have been shown to hinder the efficiency of the existing solutions: light transport due to multiple highly glossy or specular interactions.
EGFR-mutated NSCLC is a genetically heterogeneous disease that includes more than 200 distinct mutations.
 The implications of mutational subtype for both prognostic and predictive value are being increasingly understood.
 Although the most common EGFR mutations exon 19 deletions or L858R mutations predict sensitivity to EGFR tyrosine kinase inhibitors (TKIs).
Eigen-filters with attenuation response adapted to clutter statistics in color flow imaging (CFI) have shown improved flow detection sensitivity in the presence of tissue motion.
 Nevertheless.
Elastic optical networks (EONs) play an important role for the next generation core networks.
E-learning is a new learning model based on computer.
Electrocorticogram (ECoG) has great potential as a source signal.
Electromagnetic methods are commonly employed to detect wire rope discontinuities.
Electronic countermeasure (ECM) attack has been an emerging threat to radar network in recent years.
 It is necessary to design a secured radar network against ECM attack.
 In this paper.
Electronic networks of practice have become a prevalent means for acquiring new knowledge.
 Knowledge seekers commonly turn to online repositories constructed by these networks to find solutions to domain-specific problems and questions.
 Yet little is understood about the process by which such knowledge is evaluated and adopted by knowledge seekers.
 This study examines how individuals filter knowledge encountered in online forums.
Electronic storage of patient-related data will replace paper-based patient records in the near future.
 Some steps in medical practice can even now not be achieved without electronic data processing.
 Both systems.
Eliciting sufficient high-quality knowledge from individuals to build a robust and useful Artificial Intelligence or Intelligent System solution is a very time-consuming and expensive activity.
Elliptic curve cryptography (ECC) has been widely used for the digital signature to ensure the secu-rity in communication.
 It is important for the ECC processor to support a variety of ECC standards to be compatible with different security applications.
Elliptic curve cryptography (ECC) is a branch of Public-Key cryptography that is widely accepted for secure data exchange in many resource-limited devices.
 This paper presents a novel hardware cryptographic processor for ECC over general prime field GF(p).
 It is optimized on circuit level by introducing new parallel modular multiplication algorithm with its efficient hardware architecture.
Elliptic curve cryptography (ECC) is one of the most popular public key cryptosystems in recent years due to its higher security strength and lower resource consumption.
Elliptic curve cryptography is used as a public-key cryptosystem for encryption and decryption in such a way that if one has to encrypt a message.
Elliptic curve cryptosystems proved to be well suited for securing systems with constrained resources like embedded and portable devices.
 In a fault-based attack.
Elliptic curves over a finite field F-q with j-invariant 0 or 1728.
Embedded real-time software systems (ESS) play an important role in almost every aspect of our daily lives.
 We do rely on them to be functionally correct and to adhere to timing-constraints ensuring that their computational results are always delivered in time.
 Violations of the timing-constraints of a safety-critical ESS.
Embedded systems are routinely deployed in critical infrastructures nowadays.
Embedded-class processors found in commodity palmtop computers continue to become increasingly capable.
Embryos extend their anterior-posterior (AP) axis in a conserved process known as axis elongation.
 Drosophila axis elongation occurs in an epithelial monolayer.
Emerging byte-addressable non-volatile memory technologies.
Emerging cloud applications are growing rapidly and the need for identifying and managing service requirements is also highly important and critical at present.
 Software Engineering and Information Systems has established techniques.
Emerging evidence suggested genetic factor attributed as a major determinant for the complex pathogenic mechanism of gestational diabetes mellitus (GDM).
Emerging non-volatile memory (NVRAM) technologies offer the durability of disk with the byte-addressability of DRAM.
 These devices will allow software to access persistent data structures directly in NVRAM using processor loads and stores.
Emerging stakeholder needs and a changing environment drive increasing demands for the constant adaption of software through maintenance and new capability development.
 A more evolutionary software engineering approach is sought to improve engineering responsiveness; Open System Development appears to offer a partial contribution but presents many challenges.
 This exploratory research proposes a new definition of the evolution of complex engineered systems.
Emerging technologies are often not part of any official industry.
Empirical studies in software reliability have predominantly focused on end-user applications.
 Given the intrinsic dependency of user programs on the operating system (OS) software.
Employers require software engineers to work in teams when developing software systems.
 It is therefore important for graduates to have experienced teamwork before they enter the job market.
 We describe an experiential learning exercise that we designed to teach the software engineering process in conjunction with teamwork skills.
 The underlying teaching strategy applied in the exercise maximises risks in order to provide maximal experiential learning opportunities.
 The students are expected to work in fairly large.
End-to-end learning machines enable a direct mapping from the raw input data to the desired outputs.
Energy conservation is crucial in Wireless Sensor Networks (WSNs) for prolonging sensor node's life.
 This research attempts to gain benefits of Bayesian classifier through development of a Bayesian Classifier-Based Energy Aware Routing algorithm (BBEAR) for mobile WSNs.
 Through implementing a Bayesian classifier for routing process.
Energy efficiency have always been a priority while designing wireless sensor networks.
 Introduction of mobile agent technology in wireless sensor networks for collaborative signal and information processing has provided the new scope for efficient processing and aggregation of data.
 Mobile agent based distributed computing paradigm offers numerous benefits over the existing and commonly used client/server computing paradigm in wireless sensor networks.
 Mobile agent performs the task of data processing and data aggregation at the node level rather than at the sink.
Energy efficiency is a major concern in today's data centers that house large scale distributed processing systems such as data parallel MapReduce clusters.
 Modern power aware systems utilize the dynamic voltage and frequency scaling mechanism available in processors to manage the energy consumption.
 In this paper.
Energy prediction of machine tools can deliver many advantages to a manufacturing enterprise.
Energy transparency is a concept that makes a program's energy consumption visible.
Engineering designs are typically constrained by requirements and specifications.
 The Design Analogy Performance Parameters System (D-APPS) project seeks to provide engineers with a means to transform these product requirements and specifications into functional analogies.
 D-APPS employs design by-Analogy (DbA) via critical functionality and the engineering performance parameters from the specifications to produce functional alternative options to design engineers.
 Repositories.
Engineers able to produce creative products are very much required everywhere.
Enhancing the fidelity of quantum state transmission in noisy environments is a significant subject in the field of quantum communication.
 In this paper.
Ensemble classification is a well-established approach that involves fusing the decisions of multiple predictive models.
 A similar ensemble logic has been recently applied to challenging feature selection tasks aimed at identifying the most informative variables (or features) for a given domain of interest.
 In this work.
Enterprise network security solution optimization is a complex and challenging problem given the large number of components that are involved.
 This paper describes our project that has the ultimate goal of providing optimized solutions for enterprise network security.
 We describe our approach for implementing an optimized security assessment using Genetic Algorithm (GA).
 Because of the dynamic nature of an enterprise network.
Entity Resolution (ER) concerns identifying pairs of entities that refer to the same underlying entity.
 To avoid O(n(2)) pairwise comparison of n entities.
Entity resolution (ER).
Epigenomics is one of the leading frontiers of postgenomics medicine.
 The challenges and prospects ahead in epigenomics are related not merely to technology innovation and clinical implementation but also to science communication.
 In this context.
Equilibrium reconstruction consists in identifying.
Erasable itemset mining is an approach for mining itemsets with low profits from large-scale product databases in order to solve financial crises of plants in manufacturing industries.
 Previous erasable itemset mining methods deal with static product databases only.
ESL examination.
Establishing a research strategy that is suitable for undertaking research on software engineering is vital if we are to guarantee that research products are developed and validated following a systematic and coherent method.
 We took this into account as we carried out the COMPETISOFT research project.
Establishing correspondences is a fundamental task in many image processing and computer vision applications.
 In particular.
Estimating observation error covariance matrix properly is a key step towards successful seismic history matching.
Estimating the positions of a set of moving objects captured from a network of cameras is still an open problem in Computer Vision.
 In this paper.
Estimating transformations from degraded point sets is necessary for many computer vision and pattern recognition applications.
 In this paper.
Estimation of marginal or partial effects of covariates x on various conditional parameters or functionals is often a main target of applied microeconometric analysis.
 In the specific context of probit models.
Ethnography is a qualitative research method used to study people and cultures.
 It is largely adopted in disciplines outside software engineering.
Ethnopharmacological relevance: Scutellariae radix (Scutellaria baicalensis Georgi) and Coptidis rhizoma (Coptis chinensis Franch).
EVA1A is an autophagy-related protein.
Evacuation simulation has the potential to be used as part of a decision support system during large-scale incidents to provide advice to incident commanders.
 To be viable in these applications.
Evaluating link prediction methods is a hard task in very large complex networks due to the prohibitive computational cost.
Evaluating the performance of distributed systems through real experimentation is resource-consuming and by essence very difficult to reproduce.
 Conversely.
Even a rough literature review reveals that there are many alternative ways of implementing a binary heap.
Ever-increasing size and complexity of data sets create challenges and potential tradeoffs of accuracy and speed in learning algorithms.
 This paper offers progress on both fronts.
 It presents a mechanism to train the unsupervised learning features learned from only one layer to improve performance in both speed and accuracy.
 The features are learned by an unsupervised feature learning (UFL) algorithm.
Every complex problem now days require multicriteria decision making to get to the desired solution.
 Numerous Multi-criteria decision making (MCDM) approaches have evolved over recent time to accommodate various application areas and have been recently explored as alternative to solve complex software engineering problems.
 Most widely used approach is Analytic Hierarchy Process that combines mathematics and expert judgment.
 Analytic Hierarchy Process suffers from the problem of imprecision and subjectivity.
 This paper proposes to use Fuzzy AHP (FAHP) instead of traditional AHP method.
 The usage of FAHP helps decision makers to make better choices both in relation to tangible criteria and intangible criteria.
 The paper provides a clear guide on how FAHP can be applied.
Every performance.
Evolution equation is among one of the important nonlinear partial differential equations and its exact solutions are of great interest for the researchers around the globe.
 In this article.
Evolution of low-energy nuclear physics publications over the last 120 years has been analyzed using nuclear physics databases.
 An extensive study of Nuclear Science References.
Evolution of systems during their operational life is mandatory and both updates and upgrades should not impair their dependability properties.
 Dependable systems must evolve to accommodate changes.
Evolution of the long water waves and small-amplitude surface waves with the weak nonlinearity.
Evolutionary algorithms are one of the most popular forms of optimization algorithms.
 They are comparatively easy to use and were successfully employed for a wide variety of practical applications.
Execution plans constitute the traditional interface between DBMS front-ends and back-ends; similar networks of interconnected operators are found also outside database systems.
 Tasks like adapting execution plans for distributed or heterogeneous runtime environments require a plan transformation mechanism which is simple enough to produce predictable results while general enough to express advanced communication schemes required for instance in skew-resistant partitioning.
 In this paper.
Existing adaptive educational hypermedia systems have been using learning resources sequencing approaches in order to enrich the learning experience.
 In this context.
Existing computational pipelines for quantitative analysis of high-content microscopy data rely on traditional machine learning approaches that fail to accurately classify more than a single dataset without substantial tuning and training.
Existing methods for extracting titles from HTML web page mostly rely on visual and structural features.
Existing mountain permafrost distribution models generally offer a good overview of the potential extent of this phenomenon at a regional scale.
 They are however not always able to reproduce the high spatial discontinuity of permafrost at the micro-scale (scale of a specific landform; ten to several hundreds of meters).
 To overcome this lack.
Existing network security technology may not against most of intrusion so that we need to study intrusion tolerance technology.
 On the basis of existing model of intrusion tolerance.
Existing resource allocation approaches for nowadays stochastic networks are challenged to meet fast convergence and tolerable delay requirements.
 The present paper leverages online learning advances to facilitate online resource allocation tasks.
 By recognizing the central role of Lagrange multipliers.
Experimental or operational modal analysis traditionally requires physically-attached wired or wireless sensors for vibration measurement of structures.
 This instrumentation can result in mass-loading on lightweight structures.
Experiments in the computer teaching process often need to use a number of operating systems.
Explicit prediction of the suspended sediment loads in rivers or streams is very crucial for sustainable water resources and environmental systems.
 Suspended sediments are a governing factor for the design and operation of hydraulic structures.
Exploration of the search space through the optimisation of phenotypic diversity is of increasing interest within the field of evolutionary robotics.
 Novelty search and the more recent MAP-Elites are two state of the art evolutionary algorithms which diversify low dimensional phenotypic traits for divergent exploration.
 In this paper we introduce a novel alternative for rapid divergent search of the feature space.
 Unlike previous phenotypic search procedures.
Exploration of the structural balance of social networks is of great importance for theoretical analysis and practical use.
 This study modeled the structural balance of social networks as a mathematical optimization problem by using swarm intelligence.
Exponential rational function method is a relatively new mechanism to get exact solutions of nonlinear fractional differential equations.
 In this paper.
eXtensible Markup Language (XML) is gaining popularity and is being used widely on internet for storing and exchanging data.
 Large XML files when transferred on network create bottleneck and also degrade the query performance.
External Difference families (EDFs) are a new type of combinatorial designs originated from cryptography.
 In this paper.
External difference families (EDFs) are a type of combinatorial designs that originated from cryptography.
 Many combinatorial objects are closely related to EDFs.
Extracting information from a database system becomes a primary obligation.
 More and more we are forced to recognize the importance of providing easy access to information stored in a database system.
 However existing tools that allow users to query database using database query languages such as SQL (Structured Query Language) are difficult for non-experts users.
 Wherefore asking questions to databases in natural language is a very simple method that can provide powerful improvements to the use of data stored in databases.
 This paper presents the Architecture and the implementation of a generic natural language interface based on machine learning approach for a relational database.
 The advantage of this interface is that it functions independently of the database domain and automatically improves through experience its knowledge base.
Extracting OWL ontologies from relational databases is extremely helpful for realising the Semantic Web vision.
Extracting powerful image features plays an important role in computer vision systems.
 Many methods have previously been proposed to extract image features for various computer vision applications.
Extracting the underlying low-dimensional space where high-dimensional signals often reside has been at the center of numerous algorithms in the signal processing and machine learning literature during the past few decades.
Extremal Optimization is a nature-inspired optimization method which features small computational and memory complexity.
 Due to these features it can be efficiently used as an engine for processor load balancing.
 The paper presents how improved Extremal Optimization algorithms can be applied to processor load balancing.
 Extremal Optimization detects the best strategy of tasks migration leading to balanced application execution and reduction in execution time.
 The proposed algorithm improvements cover several aspects.
 One is algorithms parallelization in a multithreaded environment.
 The second one is adding some problem knowledge to improve the convergence of the algorithms.
 The third aspect is the enrichment of the parallel algorithms by inclusion of some elements of genetic algorithms - namely the crossover operation.
 The load balancing based on improved Extremal Optimization aim at better convergence of the algorithm.
Extreme apprenticeship.
Extreme degree of parallelism in high-end computing requires low operating system noise so that large scale.
Extreme hydrometeorological events such as flash floods have caused considerable loss of life and damage to infrastructure over recent years.
 Flood events in the Mediterranean region between 1990 and 2006 caused over 4.
Extreme Learning Machine (ELM) is a well-known algorithm for the training of neural networks for two modes of functionality: regression and classification.
 This paper presents a novel model using ELM in ciphering.
 The study begins with an investigation of the Real-time Recurrent Neural Network (RRNN) derived from the gradient-based learning for symmetric cipher.
 The weakness of this cipher is that the error converges to zero.
Eye irritation.
Face recognition has been widely used in many application areas such as photo album management and information security.
 Rapid growth of handheld devices and social networks bring new challenges to face recognition algorithm design and system engineering.
 To be effective on a handheld device.
Face sketch-photo synthesis technique has attracted growing attention in many computer vision applications.
Face-recognition is becoming common among the section of study in computer-vision.
Facial composite construction is one of the most successful applications of interactive evolutionary computation.
 In spite of this.
Facial expression reconstruction is an important issue in the field of computer graphics.
 While it is relatively easy to create an animation based on meshes constructed through video recordings.
Facial images embed age.
Facial landmark detection is an important issue in many computer vision applications about faces.
 It is very challenging as human faces in wild conditions often present large variations in shape due to different poses.
Facing the increasing environment crisis.
Facing with plethora network security events.
Factor analysis and target transformation techniques were applied to the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) spectral dataset to identify spectral endmembers.
Factorization of polynomials is one of the foundations of symbolic computation.
 Its applications arise in numerous branches of mathematics and other sciences.
Failure instances in distributed computing systems (DCSs) have exhibited temporal and spatial correlations.
Failure to understand evolutionary dynamics has been hypothesized as limiting our ability to control biological systems.
 An increasing awareness of similarities between macroscopic ecosystems and cellular tissues has inspired optimism that game theory will provide insights into the progression and control of cancer.
 To realize this potential.
Fast discovery of association rules from millions of transactions in a variety of large databases has now become a major challenge in data mining domain.
 Frequent itemsets and frequent closed itemsets are key sources of mining association rules.
 Association rules can be mined efficiently from lattice of itemsets.
 Non-redundancy.
Fast Fourier transform algorithms on large data sets achieve poor performance on various platforms because of the inefficient strided memory access patterns.
 These inefficient access patterns need to be reshaped to achieve high performance implementations.
 In this paper we formally restructure 1D.
Fast nearest neighbor search is becoming more and more crucial given the advent of large-scale data in many computer vision applications.
 Hashing approaches provide both fast search mechanisms and compact index structures to address this critical need.
 In image retrieval problems where labeled training data is available.
FAT file system is one of the most common file systems on various targets and operating systems.
FBD (function block diagram) has been widely used to implement safety-critical software for PLC (programmable logic controller)-based digital nuclear reactor protection systems.
 The software should be developed strictly in accordance with safety programming guidelines such as NUREG/CR-6463.
 Software engineering tools of PLC vendors enable us to present structural analyses using FBD programs.
Feature description for the 3D local shape in the presence of noise.
Feature extraction and tracking is a fundamental operation used in many geoscience applications.
 In this paper.
Feature extraction is one of the most important steps in computer vision tasks such as object recognition.
Feature selection is a significant task in data mining and machine learning applications which eliminates irrelevant and redundant features and improves learning performance.
 In many real-world applications.
Feature selection is one of the most significant steps in machine learning that reduces the features space in order to achieve faster learning and yielding simpler models with high accuracy and interpretability.
 With rapid development in technologies.
Feature selection problem in data mining is addressed here by proposing a bi-objective genetic algorithm based feature selection method.
 Boundary region analysis of rough set theory and multivariate mutual information of information theory are used as two objective functions in the proposed work.
Feature subset selection with the aim of reducing.
Feature weighting is a vital step in machine learning tasks that is used to approximate the optimal degree of influence of individual features.
 Because the salience of a feature can be changed by different queries.
Features extracted from real world applications increase dramatically.
Felsenstein's PHYLIP package of molecular phylogeny tools has been used globally since 1980.
 The programs are receiving renewed attention because of their character-based user interface.
Fibromyalgia is a common chronic pain condition that exerts a considerable impact on patients' daily activities and quality of life.
 Objectives: The main objective of the present study was to evaluate kinematic parameters of gait.
Field inversion in dominates the cost of modern software implementations of certain elliptic curve cryptographic operations.
Figure-ground segmentation is a crucial preprocessing step for many image processing and computer vision tasks.
 Since different object classes need specific segmentation rules.
File I/O buffer caching plays an important role to narrow the wide speed gap between main memory and secondary storage.
File is a super abstraction of hypothetically huge volume of information stored in containers (disk blocks) which is persists for very long time (memory abstraction).
 File is linear Array of bites and blocks of bytes accessed by multiple clients which is controlled by File system.
 File System function is disk seek.
Film coating of pharmaceutical tablets is often applied to influence the drug release behaviour.
 The coating characteristics such as thickness and uniformity are critical quality parameters.
Filters with slowly decaying impulse responses have many uses in computer graphics.
 Recursive filters are often the fastest option for such cases.
 In this paper.
Finding connected components is a fundamental task in applications dealing with graph analytics.
Finding pharmaceutically relevant target conformations from an arbitrary set of protein conformations remains a challenge in structure-based virtual screening (SBVS).
 The growth in the number of available conformations.
Finding proper design patterns has always been an important research topic in the software engineering community.
 One of the main responsibilities of the software developers is to determine which design pattern fits best to solve a particular problem.
 Design patterns support the effort of exploring the use of artificial intelligence in better management of software development and maintenance process by providing faster.
Findings from information-seeking behavior research can inform application development.
 In this report we provide a system description of Spark.
Fine-grained image recognition.
Firewall configuration is an important activity for any modern day business.
 It is particularly a critical task for the supervisory control and data acquisition (SCADA) networks that control power stations.
Firms today average forecasts collected from multiple experts and models.
 Because of cognitive biases.
First of all.
First-order and high-order correlation-power-analysis attacks have been shown to be a severe threat to cryptographic devices.
Fitting geometric or algebraic surfaces to 3D data is a pervasive problem in many fields of science and engineering.
 In particular.
Five homologous acetylated acylglycerols of 3-hydroxyfatty acids (chain lengths C(14) - C(18)).
Five-body Moshinsky brackets that relate harmonic oscillator wavefunctions in two different sets of Jacobi coordinates make it straightforward to calculate some matrix elements in the variational calculations of five-body systems.
 The analytical expression of these transformation coefficients and the computer code written in the Mathematica language are presented here for accurate calculations.
 Program summary Program title: FBMB Catalogue identifier: AEZT_v1_0 Program summary URL: http://cpc.
uk/summaries/AEZT_v1_0.
html Program obtainable from: CPC Program Library.
Fixed placements of inertial sensors have been utilized by previous human activity recognition algorithms to train the classifier.
Fixed-parameter algorithms.
Flexible business processes can often be modelled more easily using a declarative rather than a procedural modelling approach.
 Process mining aims at automating the discovery of business process models.
 Existing declarative process mining approaches either suffer from performance issues with real-life event logs or limit their expressiveness to a specific set of constaint types.
Flexible querying has been developed with the aim of proposing powerful and user-friendly ways of setting and solving queries.
 Within the context of flexible querying.
Flipped classroom is a pedagogical model in which the typical lecture and homework elements of a course are reversed.
 Teachers provide online materials to students for preparation of the lecture and students dedicate to practice in the course.
 We have put into practice the flipped classroom in our computer programming courses teaching C and Java languages since 2013 as a way of utilizing our e-learning system and contents.
 As a problem in computer programming education.
Flood area mapping is an integral part of disaster management operation which gets value when the details about inundated region has been made available in real time mode as well as when the much required temporal information is shared to the disaster mitigation authorities at right time.
 The challenges of such real time flood area mapping operations can be met by spaceborn Synthetic Aperture Radar (SAR) technology which is capable of capturing the critical information of large and hard-to-reach territories during all weather and all time situations.
 Mapping the flood related information of SAR images require much attention as the pixels associated with the inundated regions exhibit similar reflectance with major part of the pixels associated with high altitude region.
Flood modeling and forecasting using hydraulic models are computationally expensive for high-resolution.
Floods are natural disasters which cause the most economic damage at the global level.
Fluid-structure interaction (FSI) affects the dynamic characteristics and behaviors of the fluid and structure.
Focusing on the leak-points in petrochemical industries.
Follicular thyroid carcinoma (FTC) is a more aggressive form of thyroid cancer than the common papillary type.
Following an idea of spectral graph theory.
Following the wired network virtualization.
For a class of nonlinear systems with uncertainty.
For a decision table.
For a given septic Bezier curve with a distinct ordered sequence of control points.
For a large class of functions to the group of points of an elliptic curve (typically obtained from certain algebraic correspondences between E and ).
For centuries.
For conclusively predicting the quality of any software system.
For cryptographic algorithms.
For decades.
For High Dynamic Range (HDR) content.
For Lorenz system we investigate multiple Hopf bifurcation and centerfocus problem of its equilibria.
 By applying the method of symbolic computation.
For many applications.
For many tiller crops.
For modern Automatic Test Equipment.
For more flexibility of environmental perception by artificial intelligence it is needed to exist the supporting software modules.
For more than two centuries.
For multi-output Boolean functions (also called S-boxes).
For ordinary differential equations which are polynomial in the dependent variable and its derivatives.
For over forty years.
For successful software system.
For the (2 + 1)-dimensional Gardner equation.
For the consideration of the special application environment of the electronic products used in aerospace and to further more improve the human-computer interaction of the manned aerospace area.
 The research is based on the design and implementation way of the high resolution spaceborne infrared touch screen on the basis of FPGA and DSP frame structure.
 Beside the introduction of the whole structure for the high resolution spaceborne infrared touch screen system.
For the data storing and sharing requirement.
For the network security problem.
For the protection of critical infrastructures against complex virus attacks.
For the purpose of population pharmacometric modeling.
For the purposes of computer graphics.
For time-dependent control problems.
Forecasting stock returns and their risk represents one of the most important concerns of market decision makers.
 Although many studies have examined single classifiers of stock returns and risk methods.
Forecasting the output power of solar systems is required for the good operation of the power grid or for the optimal management of the energy fluxes occurring into the solar system.
 Before forecasting the solar systems output.
Forensic tools assist analysts with recovery of both the data and system events.
Forests represent an important economic resource for mountainous areas being for a few region and mountain communities the main form of income.
Formal Concept Analysis.
Formal language theory plays.
Formal verification plays a crucial role when dealing with correctness of systems.
 In a previous work.
Formation damage due to incompatibility between the formation and injected low-salinity water decreases the relative permeabilities of oil and brine.
Formulas for incremental or parallel computation of second order central moments have long been known.
Forward and adjoint Monte Carlo (MC) models of radiance are proposed for use in model-based quantitative photoacoustic tomography.
 A two-dimensional (2-D) radiance MC model using a harmonic angular basis is introduced and validated against analytic solutions for the radiance in heterogeneous media.
 A gradientbased optimization scheme is then used to recover 2-D absorption and scattering coefficients distributions from simulated photoacoustic measurements.
 It is shown that the functional gradients.
Forward Reservoir Simulation (FRS) is a challenging process that models fluid flow and mass transfer in porous media to draw conclusions about the behavior of certain flow variables and well responses.
 Besides the operational cost associated with matrix assembly.
Fostering students' computer programming ability has been recognized as being an important and challenging educational issue which is highly related to the process of problem solving.
FPGA-based accelerators have recently evolved as strong competitors to the traditional GPU-based accelerators in modern high-performance computing systems.
 They offer both high computational capabilities and considerably lower energy consumption.
 High-level synthesis (HLS) can be used to overcome the main hurdle in the mainstream usage of the FPGA-based accelerators.
Fractional derivatives are powerful tools in solving the problems of science and engineering.
 In this paper.
Free viewpoint video (FVV) offers compelling interactive experience by allowing users to switch to any viewing angle at any time.
 An FVV is composed of a large number of camera-captured anchor views.
Frequent items in high-speed streaming data are important to many applications like network monitoring and anomaly detecting.
 To deal with high arrival rate of streaming data.
Freshwater lake sediments support a variety of submerged macrophytes that may host groups of bacteria exerting important ecological functions.
 We collected three kinds of commonly found submerged macrophyte species (Ceratophyllum demersum.
From a geoinformation science perspective real estate portals apply non-spatial methods to analyse and visualise rental price data.
 Their approach shows considerable shortcomings.
 Portal operators neglect real estate agents' mantra that exactly three things are important in real estates: location.
Full human body shape scans provide valuable data for a variety of applications including anthropometric surveying.
Full implementation of the Semantic Web requires widespread availability of (fuzzy) ontologies.
 In this paper.
Full packet analysis on firewalls and intrusion detection.
Full Waveform Inversion (FWI) aims at recovering the elastic parameters of the Earth by matching recordings of the ground motion with the direct solution of the wave equation.
 Modeling the wave propagation for realistic scenarios is computationally intensive.
Full-spectrum dependent types promise to enable the development of correct-by-construction software.
Full-surround augmented reality.
Functional data analysis has become a major branch of nonparametric statistics and is a fast evolving field.
 Peter Hall has made fundamental contributions to this area and its theoretical underpinnings.
 He wrote more than 25 papers in functional data analysis between 1998 and 2016 and from 2005 on was a tenured faculty member with a 25% appointment in the Department of Statistics at the University of California.
Functional data analysis has become an important area of research because of its ability of handling high-dimensional and complex data structures.
Functional dependencies (FD's) are a powerful concept in data organization.
 They have been proven very useful in e.
Functional dependencies (FDs) represent potentially novel and interesting patterns existent in relational databases.
 The discovery of functional dependencies has a wide range of applications such as database design.
Funded by the US National Science Foundation.
Fusion technology has been demonstrated to be a good method for generating a large-scale entangled coherent W or GHZ state from two small ones in QED system.
 It is of importance to study how to fuse small-scale entangled coherent W or GHZ states via optical system.
 In this paper.
Future home networks are expected to become extremely sophisticated.
Future low-end embedded systems will make an increased use of heterogeneous MPSoCs.
 To utilize these systems efficiently.
Fuzzy clustering is a useful segmentation tool which has been widely used in many applications in real life problems such as in pattern recognition.
Fuzzy logic has been used for flexible database querying for more than 30 years.
 This paper examines some of the issues of flexible querying which seem to have potential for further research and development from theoretical and practical points of view.
 More precisely.
Fuzzy set theory can effectively manage the vagueness and ambiguity of the images being degraded by poor illumination component.
 In this study.
Fuzzy similarity is a powerful concept.
 It has been shown that it can be applied in many areas.
 This paper shows another use of fuzzy similarity for the function computation model.
Fuzzy systems have been used widely thanks to their ability to successfully solve a wide range of problems in different application fields.
Gabor wavelet can extract most informative and efficient texture features for different computer vision and multimedia applications.
 Features extracted by Gabor wavelet have similar information as visualized by the receptive field of simple cells in the visual cortex of the mammalian brains.
 This motivates researchers to use Gabor wavelet for feature extraction.
 Gabor wavelet features are used for many multimedia applications such as stereo matching.
Gait recognition is a challenging problem in computer vision.
Gambling help services typically evaluate treatment outcomes using self-reported responses and measurements.
Game designers have to deal with the complex task of monitoring the emotional state of players in games.
 There are different elements with the game.
Game development is an interdisciplinary concept that embraces artistic.
Game development is an interdisciplinary concept that embraces software engineering.
Game science has become a research field.
Game-based learning is considered as a very motivational tool to accelerate active learning of students.
 As such learning environments usually follow a computer-assisted instruction concept that offers no adaptability to each student.
Games became popular.
Games have not received the full attention of the requirements engineering community.
 This scenario is becoming more critical as we move towards newer forms of games.
Gamma spectrometric field measurements may provide high resolution information on topsoil texture.
Gas-liquid mass transfer around Taylor bubbles moving in a meandering millimetric square channel was locally visualized and characterized in the present study.
Gaussian noise is an important problem in computer vision.
 The novel methods that become popular in recent years for Gaussian noise reduction are Bayesian techniques in wavelet domain.
 In wavelet domain.
Gaussian Process (GP) models provide a very flexible nonparametric approach to modeling location-and-time indexed datasets.
Gaussian process (GP) regression is a popular statistical kernel method for learning the relationship hidden in data.
Gaussian process (GP) regression models make for powerful predictors in out of sample exercises.
Gene expression can be quantified in high throughput using microarray technology.
 Here we describe how to accurately detect differential expression and splicing using a probe-level expression change averaging (PECA) method.
 PECA is available as an R package from Bioconductor (https://www.
bioconductor.
Gene regulation modulates RNA expression via transcription factors.
 Posttranscriptional gene regulation in turn influences the amount of protein product through.
General purpose data parallel computing with graphical processing unit (GPU) is much structured today with NVIDIA (R) CUDA and other parallel programming frameworks.
 Exploiting the CUDA programming framework.
Generalized linear image processing systems have been developed from physical image formation models.
Generalized spatial modulation (GSM) is a spectral and energy efficient multiple-input-multiple-output transmission technique.
 The low-complexity detection algorithm design with near maximum likelihood (ML) performance at the receiver is very challenging.
General-purpose computing on GPUs (Graphics Processing Units) is emerging as a new paradigm in several fields of science.
General-purpose computing on GPUs is emerging as a new paradigm in several fields of science.
Generating discriminative input features is a key requirement for achieving highly accurate classifiers.
 The process of generating features from raw data is known as feature engineering and it can take significant manual effort.
 In this paper we propose automated feature engineering to derive a suite of additional features from a given set of basic features with the aim of both improving classifier accuracy through discriminative features.
Generating synthetically mixed data from library spectra provides a direct means to train empirical regression models for subpixel mapping.
 In order to best represent the subpixel composition of image data.
Genetic improvement has been used to improve functional and nonfunctional properties of software.
 In this paper.
Genetic sequences of multiple genes are becoming increasingly common for a wide range of organisms including viruses.
Genomic prediction relies on genotypic marker information to predict the agronomic performance of future hybrid breeds based on trial records.
 Because the effect of markers may vary substantially under the influence of different environmental conditions.
Geodynamics simulations are characterized by theological nonlinearity.
Geographers of technology illustrate software code's contexts.
Geographic Object-Based Image Analysis (GEOBIA) mostly uses proprietary software.
Geometrical shock dynamics (GSD) theory is an appealing method to predict the shock motion in the sense that it is more computationally efficient than solving the traditional Euler equations.
Geoscience gives insights into our surroundings and benefits many aspects of our life.
Germline mutations in POLE and POLD1 have been shown to cause predisposition to colorectal multiple polyposis and a wide range of neoplasms.
Getting higher occurrence of cybercrimes by means of hacking.
Given a database table with records that can be ranked.
Given a graph G and a non-negative integer h.
Given a graph G that admits a perfect matching.
Given a high-order large-scale tensor.
Given a matrix of size N.
Given a planar point set sampled from an object boundary.
Given a set P of n coloured points on the real line.
Given a set P of n uncertain points on the real line.
Given an initial assignment of processes to machines.
Given that next generation networks are expected to be populated by a large number of devices.
Given the continuous evolution of hardware capabilities for embedded systems.
Given the relentless growing number of mobile devices.
Given two linear projections of maximal rank from P-k to P-h1 and P-h2 with k >= 3 and h(1) + h(2) >= k+1 the Grassmann tensor introduced by Hartley and Schaffalitzky (Int J Comput Vis 83(3):274-293.
Glioblastoma multiforme (GBM) is the most malignant brain tumor with limited therapeutic options.
 Temozolomide (TMZ) is a novel cytotoxic agent used as first-line chemotherapy for GBM.
Global optimisation of unknown noisy functions is a daunting task that appears in domains ranging from games to control problems to meta-parameter optimisation for machine learning.
 We show how to incorporate heuristics to Stochastic Simultaneous Optimistic Optimization (STOSOO).
Global Software Development (GSD) is a major direction in software engineering.
 There is interest in applying scrum practices in distributed projects.
 Project stakeholder distribution in GSD is represented by geographical distance.
Global software development (GSD).
Global software development exposes projects to the challenges arising from geo-cultural spread of the team and delegation of project ownership.
Global tractography estimates brain connectivity by organizing signal-generating fiber segments in an optimal configuration that best describes the measured diffusion weighted data.
Glucosinolates (GIs) constitute a major group of natural metabolites represented by three major classes (aliphatic.
Gluten-induced aggregation of K562 cells represents an in vitro model reproducing the early steps occurring in the small bowel of celiac patients exposed to gliadin.
 Despite the clear involvement of TG2 in the activation of the antigen-presenting cells.
GNSS-based navigation technology for lunar mission with more than 60.
Goal models represent requirements and intentions of a software system.
 They play an important role in the development life cycle of software product lines (SPLs).
 In the domain engineering phase.
Golgi apparatus (GA) is a center for lipid metabolism and the final target of ceramide pathway.
Good parametrizations of affine transformations are essential to interpolation.
GPU is widely used in various applications that require huge computational power.
 In this paper.
Graph coloring-in a generic sense-is used to identify subsets of independent tasks in parallel scientific computing applications.
 Traditional coloring heuristics aim to reduce the number of colors used as that number also corresponds to the number of parallel steps in the application.
Graph colouring is the labelling of the elements of a graph subject to certain constraints.
 It is divided into vertex and edge colouring.
 In both cases.
Graph databases can be defined as databases that use graph structures with nodes.
Graph theory plays many important roles in modern physics and in many different contexts.
Graph-based semi-supervised learning (SSL).
Graphic clipping algorithm is a hotspot all the time in computer graphics.
 Based upon non-intersect polygon boundary.
Graphic Processing Unit (GPU).
Graphics processing unit (GPU) has been applied successfully in many computation and memory intensive realms due to its superior performances in float-pointing calculation.
Graphics processing unit (GPU) has been applied successfully in many scientific computing realms due to its superior performances on float-pointing calculation and memory bandwidth.
Graphics processing unit (GPU) has significantly increased the computing capacity of state-of-the-art high performance computing systems.
 This paper presents a novel GPU acceleration approach for transient stability-constrained optimal power flow (TSCOPF).
Graphics processing units (GPUs) are increasingly used for high-performance computing.
 Programming frameworks for general-purpose computing on GPUs (GPGPU).
Graphics processing units (GPUs) are now widely used in embedded systems for manipulating computer graphics and even for general-purpose computation.
Graphs are considered to be one of the best studied data structures in discrete mathematics and computer science.
Graphs are powerful and versatile data structures that can be used to represent a wide range of different types of information.
 In this article.
Greedy algorithms is an important class of algorithms.
 Teaching greedy algorithms is a complex task.
 Ensuring that students can design greedy algorithms for new problems is also complex.
 We have built a guided discovery based greedy algorithm tutor (GATutor).
Greedy Forward is a technique for data routing in Wireless Sensor Networks (WSNs) in which data packets are forwarded to the node that is geographically closer to the destination node.
 Two main concerns can be found in routing algorithms based on this technique: first.
Grid computing has emerged a new field.
Grid computing is a high performance distributed computing platform to solve complex and large-scale scientific problems.
 It consists of heterogeneous computing resources connected by a network across dynamic and geographically distributed organisations to create a distributed high performance computing infrastructure.
 Job scheduling in computational Grid is known as NP-complete problem owing to the problem complexity and intractable nature of the problem.
 Such a problem could be solved using heuristic algorithms.
 These types of algorithms have the ability to find a near optimal solution in reasonable time rather than the optimal solution in a very long processing time.
 The primary objective of the scheduling is to minimise the makespan of the system.
 In this paper.
Grinding employs an abrasive product.
Ground-penetrating radar (GPR) is used to image and detect subterranean objects.
Group signature scheme is a method of allowing a member of a group to sign a message anonymously on behalf of the group.
 The group administrator is in charge of adding group members and has the ability to reveal the original signer in the event of disputes.
 Based on controlled quantum teleportation with three-particle entangled W states.
Growing main memory capacity has fueled the development of in-memory big data management and processing.
 By eliminating disk I/O bottleneck.
GUI testing is essential to provide validity and quality of system response.
 Odum suggested a set of rules for simulating the dynamics of emergy flow (Dynamic Emergy Accounting-DEA).
Hacking - improving software through a process of trial and error - is a mode of rehearsal.
 Such is the claim made by Miguel Escobar Varela in this article.
Hadoop distributed file system (HDFS) and MapReduce model have become popular technologies for large-scale data organization and analysis.
 Existing model of data organization and processing in Hadoop using HDFS and MapReduce are ideally tailored for search and data parallel applications.
Hadoop has become a widely used open source framework for large scale data processing.
 MapReduce is the core component of Hadoop.
 It is this programming paradigm that allows for massive scalability across hundreds or thousands of servers in a Hadoop cluster.
 It allows processing of extremely large video files or image files on data nodes.
 This can be used for implementing Content Based Image Retrieval (CBIR) algorithms on Hadoop to compare and match query images to the previously stored terabytes of an image descriptors databases.
 This work presents the implementation for one of the well-known CBIR algorithms called Scale Invariant Feature Transformation (SIFT) for image features extraction and matching using Hadoop platform.
 It gives focus on utilizing the parallelization capabilities of Hadoop MapReduce to enhance the CBIR performance and decrease data input\output operations through leveraging Partitioners and Combiners.
 Additionally.
Hadoop HDFS is an open source project from Apache Software Foundation for scalable.
Hadoop is one of the most important Big Data processing and storage systems.
 In recent years.
Halftoning and inverse halftoning algorithms are very important image processing tools.
Haloferax alexandrinus Strain TM JCM 10717(T) = IFO 16590(T) is an extreme halophilic archaeon able to produce significant amounts of canthaxanthin.
 Its genome sequence has been analysed in this work using bioinformatics tools available at Expasy in order to look for genes encoding nitrate reductase-like proteins: respiratory nitrate reductase (Nar) and/or assimilatory nitrate reductase (Nas).
 The ability of the cells to reduce nitrate under aerobic conditions was tested.
 The enzyme in charge of nitrate reduction under aerobic conditions (Nas) has been purified and characterised.
 It is a monomeric enzyme (72 +/- 1.
8 kDa) that requires high salt concentration for stability and activity.
 The optimum pH value for activity was 9.
 Effectiveness of different substrates.
Hardware-based image processing offers speed and convenience not found in software-centric approaches.
Harmonising the metadata format alone does not solve the issue of efficient access to relevant information in heterogeneous environments.
Harnessing the benefits of cloud computing.
Hashing based approximate nearest neighbors (ANN) search has drawn considerable attraction owing to its low-memory storage and hardware-level logical computing which is doomed to be greatly applicable to quantities of large-scale and practical scenarios.
Hashing is an important technique to achieve high code performance in a variety of data processing applications.
 The concept is emphasized in Computer Science curricula.
Having an effective data structure regards to fast data changing is one of the most important demands in spatio-temporal data.
 Spatio-temporal data have special relationships in regard to spatial and temporal values.
 Both types of data are complex in terms of their numerous attributes and the changes exhibited over time.
 A data model that is able to increase the performance of data storage and inquiry responses from a spatio-temporal system is demanded.
 The structure of the relationships between spatio-temporal data mimics the biological structure of the hair.
Haze removal is useful in computational photography and computer vision applications.
 Although many haze removal algorithms have been proposed.
Health communications and baccalaureate nursing education are increasingly impacted by new technological tools.
 This article describes how an Accelerated Bachelor of Science in Nursing program incorporates an infographic assignment into a graduate-level online health information and technology course.
 Students create colorful.
Heap-based priority queues are very common dynamical data structures used in several fields.
Heisenberg-type models for spin-spin interactions have been used to explain magnetic ordering in ferromagnetic materials.
 In this paper.
Helicases play a critical role in processes such as replication or recombination by unwinding double-stranded DNA; mutations of these genes can therefore have devastating biological consequences.
Helping college freshmen to learn basic computer programming is a longstanding research topic.
 Various environments.
Hepatocellular carcinoma (HCC) is an invasive form of hepatic cancer arising from the accumulation of multiple genetic alterations.
 In this study.
Hepatocellular carcinoma (HCC) is one of the malignant and lethal cancers.
 Single nucleotide polymorphisms (SNPs) in microRNAs(miRNAs) can affect the expression and target identification of miRNAs and lead to the formation of malignant tumors.
Hepatocellular carcinoma (HCCa) is a primary malignancy of the liver.
 Many different proteins are involved in HCCa including insulin growth factor (IGF) II.
Heterogeneous computing.
Heterogeneous parallel and distributed computing systems frequently must operate in environments where there is uncertainty in system parameters.
 Robustness can be defined as the degree to which a system can function correctly in the presence of parameter values different from those assumed.
 In such an environment.
Heterogeneous System Architecture (HSA) is an architecture developed by the HSA foundation aiming at reducing programmability barriers as well as improving communication efficiency for heterogeneous computing.
 For example.
Heuristics and metaheuristics have achieved great accomplishments in various fields.
Hidden persistent malware in guest virtual machine instances are among the most common internal threats in cloud computing.
Hidden suspicious accounts are sparsely connected in social graphs; however.
Hierarchical data are found in a variety of database applications.
Hierarchical grids appear in various applications in computer graphics such as subdivision and multiresolution surfaces.
Hierarchical search structures satisfying good memory and update performance demands.
Hierarchically structured Automatic Voltage Control (AVC) architecture enables wide-area closed-loop Coordinated Voltage Regulation (CVR).
 Owing to the inherent complexity of the task.
High concentrations of fluoride in the body may cause toxic effects.
High dimensionality and classification of imbalanced data sets are two of the most interesting machine learning challenges.
 Both issues have been independently studied in the literature.
 In order to simultaneously explore the both issues of feature selection and oversampling.
High Energy Physics (HEP) experiments at the LHC collider at CERN were among the first scientific communities with very high computing requirements.
High performance.
High resolution reconstruction technology is developed to help enhance the spatial resolution of observational images for ground-based solar telescopes.
High spatial resolution images as well as image processing and object detection algorithms are recent technologies that aid the study of biodiversity and commercial plantations of forest species.
 This paper seeks to contribute knowledge regarding the use of these technologies by studying randomly dispersed native palm tree.
High utility sequential pattern (HUSP) mining has emerged as a novel topic in data mining.
 Although some preliminary works have been conducted on this topic.
High-definition video is becoming a standard in clinical endoscopy.
 State-of-the-art systems for medical endoscopy provide 1080p video streams at 60 Hz.
 For such high resolutions and frame rates.
High-dimensional quantum key distribution (HD-QKD) can generate more secure bits for one detection event so that it can achieve long distance key distribution with a high secret key capacity.
 In this Letter.
High-end computing is increasingly I/O bound as computations become more data-intensive.
Higher-level cognition depends on the ability to learn models of the world.
 We can characterize this at the computational level as a structure-learning problem with the goal of best identifying the prevailing causal relationships among a set of relata.
High-frequency surface wave radar (HFSWR) has a vital civilian and military significance for continuous maritime surveillance of activities within exclusive economic zone.
High-level abstractions separate algorithm design from platform implementation.
High-Level Synthesis (HLS) promises a significant shortening of the FPGA design cycle by raising the abstraction level of the design entry to high-level languages such as C/C++.
Highly optimised reconfigurable hardware architecture is proposed of 64 bit block ciphers MISTY1 and KASUMI for wide-area cryptographic applications.
 The reconfigurable hardware architecture is comprised of reconfigurable components consisting of FL function.
High-quality services over wired and wireless networks require high bitrate network content delivery.
 Multipath transport protocols utilize multiple paths to support increased throughput for network applications.
 Ubiquitous mobile devices with multiple interfaces such as WiFi and 4G/5G cellular.
High-throughput screening generates large volumes of heterogeneous data that require a diverse set of computational tools for management.
Hill is a classical cipher which is generally believed to be resistant against ciphertext-only attack.
 In this paper.
His article to include Arabic Numbers.
HISTIH3D gene encodes histone H3.
1 and is involved in gene-silencing and heterochromatin formation.
 HIST1H3D expression is upregulated in primary gastric cancer tissue.
 In this study.
Histone methylation plays important roles in regulating chromatin dynamics and transcription in eukaryotes.
 Implication of histone modifications in fungal pathogenesis is.
Hole and crack filling is the most important issue in depth-image-based rendering (DIBR) algorithms for generating virtual view images when only one view image and one depth map are available.
 This paper proposes a priority patch inpainting algorithm for hole filling in DIBR algorithms by generating multiple virtual views.
 A texture-based interpolation method is applied for crack filling.
Holoscopic 3D imaging (also known as Integral imaging) is a true light field imaging system that mimics fly's eye technique for capturing fitll colour spatial images using a single aperture holoscopic 3D camera.
 The fly's eye microlens array view the scene at a slightly different angle to its adjacent lens that records three-dimensional information onto two-dimensional surface.
 As a result.
Honeypots and honeynets are popular tools in the area of network security and network forensics.
 The deployment and usage of these tools are influenced by a number of technical and legal issues.
Host cardinality is defined as the number of distinct peers that a host communicates with in the network.
 There have been several algorithms proposed to monitor network traffic and identify high-cardinality hosts at a centralized network operation center (NOC).
 Due to massive amounts of distributed data and limitations on transforming and processing them at the NOC.
Host defence peptides (HDPs) are short.
How can we analyze large-scale real-world data with various attributes? Many real-world data (e.
How can we find a general way to choose the most suitable samples for training a classifier? Even with very limited prior information? Active learning.
How do people appropriate their virtual hand representation when interacting in virtual environments? In order to answer this question.
How to cultivate the students' capacity of computational thinking during the educational process of computer programming is one of the basic goals of computer basic education.
 Considering the generally reflected problems in education of programming courses such as boring.
How to develop a trust management model and then to efficiently control and manage nodes is an important issue in the scope of social network security.
 In this paper.
How to provide cost effective.
How to suppress speckle noise effectively has become one of the key problems in remote sensing image processing.
 This problem also restricts the development of key technology severely.
How to timely and precisely identify attack behaviors in network without dealing with a large number of traffic features and historical data.
How to transfer the trained detector into the target scenarios has been an important topic for a long time in the field of computer vision.
 Unfortunately.
How to use software effectively and efficiently in the chemical.
How we can describe the similarity relationship between the biological sequences is a basic but important problem in bioinformatics.
 The first graphical representation method for the similarity relationship rather than for single sequence is proposed in this article.
HRDBMS is a novel distributed relational database that uses a hybrid model combining the best of traditional distributed relational databases and Big Data analytics platforms such as Hive.
 This allows HRDBMS to leverage years worth of research regarding query optimization.
HTS (Hash Type System) is a type system designed for component-based high performance computing (CBHPC) platforms.
Human action recognition (HAR) is a core technology for human-computer interaction and video understanding.
Human action recognition has been well explored in applications of computer vision.
 Many successful action recognition methods have shown that action knowledge can be effectively learned from motion videos or still images.
 For the same action.
Human activity recognition (HAR) is an important research area in computer vision due to its vast range of applications.
 Specifically.
Human activity recognition (HAR) is an important research area in the fields of human perception and computer vision due to its wide range of applications.
 These applications include: intelligent video surveillance.
Human body customization is an important task in computer graphics and computer simulation recently.
 The existing human body modeling methods may generate distorted model or generate a human body model that does not match the target measurements.
 These shortcomings are improved in this paper.
 This human body modeling module includes two parts: analyzing the linear relationship between human body shapes and measurements.
Human emotions are often expressed by facial expressions and are generated by facial muscle movements.
 In recent years.
Human gait recognition.
Human identification at a distance has recently become a hot research topic in the fields of computer vision and pattern recognition.
 Since gait patterns can operate from a distance without subject cooperation.
Human movement patterns have been shown to be particularly variable if many combinations of activity in different muscles all achieve the same task goal (i.
Human target characteristic parameter extraction is an important approach of behavior monitoring.
 The extraction of the characteristic can be applied in various backgrounds.
Humanitarian aid information.
Human-like motion prediction and simulation is an important task with many applications in fields such as occupational-biomechanics.
Humanoid robots have complex kinematic chains whose modeling is error prone.
 If the robot model is not well calibrated.
Humoral and cellular immune responses play an important role during Giardia lamblia infection.
 Several Giardia proteins have been identified as immunogenic antigens based on their elicited humoral immune response.
 Poorly is known about Giardia antigens that stimulate a cellular immune response.
 The main purpose of this study was to isolate and partial characterize an immunogenic antigen (5G8) of G.
 The 5G8 protein was isolated from G.
 lamblia trophozoite lysates by affinity chromatography using moAb 5G8-coupled CNBr-Sepharose.
 The isolated protein was analysed by electrospray tandem mass spectrometry (ESI-MS/MS).
Hundreds of papers on job scheduling for distributed systems are published every year and it becomes increasingly difficult to classify them.
 Our analysis revealed that half of these papers are barely cited.
 This paper presents a general taxonomy for scheduling problems and solutions in distributed systems.
 This taxonomy was used to classify and make publicly available the classification of 109 scheduling problems and their solutions.
 These 109 problems were further clustered into ten groups based on the features of the taxonomy.
 The proposed taxonomy will facilitate researchers to build on prior art.
Hybrid model between perceived risk and entrepreneurial opportunity is introduced in the Entrepreneurship recognition model in the paper.
hybridcheck is a software package to visualize the recombination signal in large DNA sequence data set.
Hydrological connectivity describes the physical coupling (linkages) of different elements within a landscape regarding (sub-) surface flows.
 A firm understanding of hydrological connectivity is important for catchment management applications.
Hydrological Dispersion Module (HDM) of JRODOS includes the chain of the models describing the washing out of the accidental deposition from the watersheds to rivernet and radionuclide transport in rivers and reservoirs.
 This modelling chain of HDM was renewed by the integration of two models.
 One- dimensional model RIVTOX_SV provides possibilities for the computing radionuclide transport in the branched river networks.
 Two-dimensional model for shallow reservoirs and river floodplains COASTOX_UN used algorithms of the parallel computing on the unstructured mesh of the finite volume method.
 The renewed modelling chain was tested for the sub-watersheds of the Danube river and the Dnieper river.
Hydrological models are always related to time and spatial domains.
Hyperfine Structure Fitting (HfS) is a tool to fit the hyperfine structure of spectral lines with multiple velocity components.
 The HfS_nh3 procedures included in HfS simultaneously fit the hyperfine structure of the NH3 (J.
Hypergraph is a powerful representation for several computer vision.
Hypergraph partitioning is commonly used in solving very large scale integration (VLSI) placement problem.
Hyperspectral anomaly detection (AD) is an important problem in remote sensing field.
 It can make full use of the spectral differences to discover certain potential interesting regions without any target priors.
 Traditional Mahalanobis-distance-based anomaly detectors assume the background spectrum distribution conforms to a Gaussian distribution.
Hyperspectral image superresolution is a highly attractive topic in computer vision and has attracted many researchers' attention.
Hyperspectral Raman images of human prostatic cells have been collected and analysed with several approaches to reveal differences among normal and tumor cell lines.
 The objective of the study was to test the potential of different chemometric methods in providing diagnostic responses.
 We focused our analysis on the v(C-H) region (2800-3100 cm(-1)) owing to Its optimal Signal-to-Noise ratio and because the main differences between the spectra of the two cell lines were observed in this frequency range.
 Multivariate analysis identified two principal components.
Hyperspectral remote sensing data offer the opportunity to map urban characteristics in detail.
Hyperspectral signatures can provide abundant information regarding health status of crops; however it is difficult to discriminate between biotic and abiotic stress.
 In this study.
Hyperspectral unmixing is one of the most important techniques in the remote sensing image analysis.
 In recent years.
Hypervisors and Operating Systems support vertical elasticity techniques such as memory ballooning to dynamically assign the memory of Virtual Machines (VMs).
Hypervisors enable cloud computing model to provide scalable infrastructures and on-demand access to computing resources as they support multiple operating systems to run on one physical server concurrently.
 This mechanism enhances utilization of physical server thus reduces server count in the data center.
 Hypervisors also drive the benefits of reduced IT infrastructure setup and maintenance costs along with power savings.
 It is interesting to know different hypervisors' performance for the consolidated application workloads.
 Three hypervisors ESXi.
Hypothesis: Drying constitutes a key step in the production of thin.
I present a family of algorithms to reduce noise in astrophysical images and image sequences.
Identification of antigen for inducing specific class of antibody is prime objective in peptide based vaccine designs.
Identifying dependencies that hold in relational databases is essential to produce good databases designs.
 In particular.
Identifying network traffics at their early stages accurately is very important for network management and security.
 Recent years.
Identifying similarities in large datasets is an essential operation in several applications such as bioinformatics.
Identifying the processes by which human cultures spread across diffferent populations is one of the most topical objectives shared among diffferent fields of study.
 Seminal works have analyzed a variety of data and attempted to determine whether empirically observed patterns are the result of demic and/or cultural difffusion.
 This special issue collects articles exploring several themes (from modes of cultural transmission to drivers of dispersal mechanisms) and contexts (from the Neolithic in Europe to the spread of computer programming languages).
IDS Internet worms.
4 is a wireless standard that specifies physical layer and media access control while focusing on low-cost and low-power transmissions between devices.
If we apply the developed local polar edge detection method.
Ill-posed inverse problems call for some prior model to define a suitable set of solutions.
 A wide family of approaches relies on the use of sparse representations.
 Dictionary learning precisely permits to learn a redundant set of atoms to represent the data in a sparse manner.
 Various approaches have been proposed.
Image coaddition is one of the most basic operations that astronomers perform.
Image contrast enhancement algorithms play a crucial role in image processing and computer vision.
 The main challenge in contrast enhancement is that an algorithm suitable for low contrast distorted images does not suit for high contrast distorted images.
 In this paper.
Image definition will be influenced by alignment errors of mirrors in an optical system consisting of hyperbolic.
Image denoising is one of the fundamental and essential tasks within image processing.
 In medical imaging.
Image encryption has been an attractive research field in recent years.
 The chaos-based cryptographic algorithms have suggested some new and efficient ways to develop secure image encryption techniques.
 This paper proposes a novel image encryption scheme.
Image enhancement and edge-preserving denoising are relevant steps before classification or other postprocessing techniques for remote sensing images.
Image enhancement plays a very crucial role in many image processing applications.
 It aims at improving the visual and informational quality of the distorted images.
 Histogram equalization is one of the most frequently used techniques for image contrast enhancement.
Image feature representation is a hot topic in the computer vision field.
 Inspired by Weber's law and local graph structure (LGS).
Image inpainting is a dynamic field with different image processing and computer graphics applications.
 Most of the existing image inpainting methods lead to significant results in different applications but fail in difficult situations with high local structural variations.
 In this paper.
Image registration deals with establishing correspondences between images of the same scene or object.
 An image registration algorithm should handle the variations introduced by the imaging system capturing the scene.
 Scale Invariant Feature Transform (SIFT) is an image registration algorithm based on local features in an image.
 Compared to the previous registration algorithms.
Image registration has become a crucial step in a wide range of imaging domains.
Image segmentation is a crucial step in image processing.
Image segmentation is a fundamental problem in computer vision.
Image segmentation is an important processing in many applications such as image retrieval and computer vision.
 One of the most successful models for image segmentation is the level set methods which are based on local context.
 The methods.
Image set representation and classification is an important problem in computer vision and pattern recognition area.
 It has been widely used in many computer vision applications.
 In this paper.
Image sparse representation methods have been widely applied in many image processing fields.
Image splicing is a common and widespread type of manipulation.
Image-based visual discomfort analysis has strong potential to detect glare in order to predict occupant satisfaction with a space.
Immersive multi-user virtual environments give good support for closely-coupled collaboration between co-located users.
 More complex collaborative tasks may require individual user navigation to achieve loosely-coupled collaboration.
 We designed a navigation framework based on the human joystick metaphor with some alterations for cohabitation management.
 This model allows each user to navigate independently using a human joystick based control while avoiding physical obstacles and staying within the usable part of the system.
 We conducted a series of user studies to investigate the influence of each alteration by testing their combinations on various navigation tasks.
 The results show that modified transfer functions and adaptive neutral orientations improve users' cohabitation performance.
Immunodominance clone selection algorithm (ICSA) is a robust and effective metaheuristic method for feature selection problem.
Immunotherapy is an entirely advanced class of cancer treatment which has been highly active and exciting field in clinical therapeutics.
 In numerous procedures.
Implementing cloud computing empowers numerous paths for Web-based service offerings to meet diverse needs.
Implicit representations have gained an increasing popularity in geometric modeling and computer graphics due to their ability to represent shapes with complicated geometry and topology.
Implicit representations of geometry have found applications in shape modeling.
Improper functioning of traffic signals at the intersections result in extreme congestion leading to increase in overall journey time and wastage of precious fuel.
 Various algorithms have been proposed in literature for alleviating the problem of congestion.
 Fixed-time.
Improving disaster management and recovery techniques is one of national priorities given the huge toll caused by man-made and nature calamities.
 Data-driven disaster management aims at applying advanced data collection and analysis technologies to achieve more effective and responsive disaster management.
Improving the safety of transportation systems attracts lots of attention.
 Researchers introduced their methods to detect and analyze the vehicle and the pedestrian on the road to accomplish this goal.
Improving thermostability of an enzyme can accelerate the relevant chemical reaction.
In a brief glance at an object or shape.
In a context where networks grow larger and larger.
In a digital multisignature scheme.
In a magnetized e-p-i plasma with two-electron temperatures.
In a network of interdependent users.
In a node replication attack.
In a peer to peer self-stabilizing message passing system.
In a previous study we developed a Machine Learning procedure for the automatic identification and classification of spontaneous cord dorsum potentials (CDPs).
 This study further supported the proposal that in the anesthetized cat.
In a relational database.
In a screen climate where graphic violence (once commonly excoriated in its social effects) is increasingly a matter of violent computer graphics (CGI).
In a strong designated verifier signature scheme.
In a strong designated verifier signature with message recovery (SDVSWMR) scheme.
In a two-server password-authenticated key exchange (PAKE) protocol.
In a typical.
In a variety of research areas.
In acquired immunodeficiency syndrome (AIDS) studies it is quite common to observe viral load measurements collected irregularly over time.
In active sonar processing.
In addition to algorithm-or concept-oriented training of problem solving by computer programming.
In all kinds of Internet security incidents.
In an attempt to tackle shortcomings of current approaches to collaborating on the development of structured data sets.
In animal behavior research.
In aquaculture.
In architectural project review.
In big data era.
In birefringent optical fi bers.
In centralized cellular network architecture.
In certain situations.
In clinical practice.
In CMOS-based application-specific integrated circuit (ASIC) designs.
In complex network.
In complex software development projects.
In component-based development of object-oriented software.
In computer graphics and related fields.
In computer graphics.
In Computer Science Education.
In computer vision research.
In computer vision.
In computer vision.
In connection with the rapid increase in the volume semistructured and unstructured data.
In context-aware mobile systems.
In cryptography one needs large families of binary sequences with strong pseudorandom properties.
 In the last decades many families of this type have been constructed.
In Data Mining.
In data-parallel skeleton libraries.
In distributed computing environment.
In distributed computing environments.
In distribution and transmission electric power companies.
In e-Learning research.
In Euclidean plane geometry.
In experiments with crossover design subjects apply more than one treatment.
 Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects.
In face of high partial and complete disk failure rates and untimely system crashes.
In fish reproduction.
In five experimentally characterized arterivirus species.
In forensic computing.
In free-viewpoint video.
In Gentry's fully homomorphic encryption scheme.
In gesture recognition systems.
In grid computing environment.
In high-end data processing systems.
In hot and humid regions.
In image processing.
In image processing.
In interactive evolutionary computation (IEC).
In interactive evolutionary computation (IEC).
In large-scale wireless sensor networks (WSNs).
In last years.
In line with a growing need for data and information transmission in a safe and quick manner.
In many application fields such as social networks.
In many applications.
In many circumstances.
In many institutions relational databases are used as a tool for managing information related to day to day activities.
 Institutions may be required to keep the information stored in relational databases accessible because of many reasons including legal requirements and institutional policies.
In many kinds of on-orbit space robot tasks.
In many research and application areas.
In MapReduce environment.
In medical decision support systems.
In modern days.
In modern operating systems.
In modern operating systems.
In modern RAID-structured storage systems.
In most areas of computer science (CS).
In most of quantum key distribution schemes.
In multi-label classification.
In multimedia forensics.
In multiradio multichannel (MR-MC) networks with significantly expanded network resource space.
In multi-relational data mining (MRDM).
In multi-sensor data fusion based on multiple features.
In multivariate or spatial extremes.
In Named Data Networking (NDN) architecture.
In novel forms of the Social Internet of Things.
In order to develop a computationally efficient implementation of the maximally permissive deadlock avoidance policy (DAP) for complex resource allocation systems (RAS).
In order to effectively prevent direct and indirect losses the lightning disasters bring to coastal ports and guarantee the security of production.
In order to efficiently obtain all frequencies of the solution.
In order to ensure the reliable operation of the electric power grid.
In order to facilitate interaction in computermediated communication and enrich user experience in general.
In order to have a competitive software industry.
In order to improve energy efficiency of computation-intensive workloads in Cloud Radio Access Network (C-RAN).
In order to improve the dynamic performance of DC-DC converter.
In order to improve the efficiency of quality tests and optimize the process of the quality control.
In order to improve the positioning and navigation performance of Global Navigation Satellite System (GNSS) receivers.
In order to improve the prediction ability of welding quality during high-power disk laser welding.
In order to improve the safe level of network security.
In order to locate source signal more accurately in authorized frequency bands.
In order to model the design process of pressure hull.
In order to monitor carbon monoxide in industrial production.
In order to obtain high-precision CCD positions of Himalia.
In order to overcome the defects of traditional methods on measuring plant leaf diseases and achieve accurate detection of blade relative lesion area.
In order to reduce the security risk of commercial aircraft.
In order to reduce the time complexity of situation element acquisition and to cope with the low detection accuracy of small class samples caused by imbalanced class distribution of attack samples in wireless sensor networks.
In order to solve the problem of designing the specific paths for the air-rail network.
In order to solve the problem of embedding the watermark into the quantum color image.
In order to specify databases completely at the conceptual level.
In order to tackle the ambiguities of geometrical product specification (GPS).
In order to understand luminous efficiency of the visible wavelength and the infrared wavelength range generated by hypervelocity impact natural dolomite plate.
In order to understand the security level of an organization network.
In our previous study.
In our research we use rigid-body dynamics and optimal control methods to generate 3-D whole-body walking motions.
 For the dynamics modeling and computation we created RBDL-the Rigid Body Dynamics Library.
 It is a self-contained free open-source software package that implements state of the art dynamics algorithms including external contacts and collision impacts.
 It is based on Featherstone's spatial algebra notation and is implemented in C++ using highly efficient data structures that exploit sparsities in the spatial operators.
 The library contains various helper methods to compute quantities.
In our studies of global software engineering (GSE) teams.
In our study.
In pair-programming.
In pair-programming.
In parallel with the development of nucleotide sequencing an equally important interest in further describing the sequence in terms of function arose and the latter represents the current bottleneck in the overall research question.
 Sequencing the transcriptome allows determination of expressed nucleotide sequences and using mass spectrometry allows sequencing on the protein level.
 Both approaches can only sequence a subset of the existing transcripts.
In paravirtualization.
In Part I of this paper.
In photogrammetry.
In previous works.
In production processes that use surfacemount technology (SMT) for the assembly of printed circuit boards.
In quantitative trait locus (QTL) mapping significance of putative QTL is often determined using permutation testing.
 The computational needs to calculate the significance level are immense.
In quantum computing.
In rainbow trout farming.
In real quadratic number field Q (root d).
In recent days.
In recent decades.
In recent times MOOCs has become a technology enabled platform which allows delivery of content in locations geographically separated out.
 In Birla Institute of Technology and Science (BITS).
In recent times.
In recent years it has become apparent that schema matching is a labor intensive process that is very costly in resources; this has led to the development of various automated tools to substitute the human experts involved in it.
 To this end we propose two new ideas.
 The first is the separation of matching techniques into strong and weak ones.
In recent years parallel computing has been widely employed for both science research and commercial applications.
 For parallel systems such as many-core or computer clusters.
In recent years smart mobile devices have bolstered new interaction scenarios that require more sophisticated human-machine interfaces.
 The leading developers of operating systems for these devices now provide APIs (Application Programming Interface) for developers to implement their own applications.
In recent years the GIS has been increasingly used as a fundamental survey and data management tool in multi-disciplinary fields.
 The GIS was originally designed for environmental use in territorial management while today it is more frequently used for analysis.
In Recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In recent years.
In relational databases and their applications.
In reliability growth models in particular.
In remote sensing image processing.
In response to the requirements of applications that work with large amounts of data.
In scientific modelling and computation.
In self-hosted environments it was feared that business intelligence (BI) will eventually face a resource crunch situation due to the never ending expansion of data warehouses and the online analytical processing (OLAP) demands on the underlying networking.
 Cloud computing has instigated a new hope for future prospects of 131.
In simulations running in parallel.
In software development process.
In Software Product Line (SPL).
In solving of boundary value problems the shapes of the boundary can be modelled by the curves widely used in computer graphics.
 In parametric integral equations system (PIES) such curves are directly included into the mathematical formalism.
 Its simplify the way of definition and modification of the shape of the boundary.
 Until now in PIES the B-spline.
In space applications.
In statistics.
In System and Software Engineering development.
In terms of capturing software requirements.
In terms of the theories and techniques of network security.
In the Design4All project.
In the age of increasing digital data transmission through network.
In the applications of computer graphics.
In the article the detection of missing or hidden data during the processing of the well-defined queries to database is discussed and on terms of quality the importance is justified.
In the business process of reengineering.
In the class of strictly positive functions strong localization results are obtained in approximation by the Bernstein max-product operators.
 The results allow to approximate locally bounded strictly positive functions with very good accuracy.
In the classical secret-key generation model.
In the computer graphics industry.
In the context of electronic management.
In the context of large engineering projects the effective and efficient exchange and versioning of information from different engineering disciplines is essential.
 Semantic data integration approaches provide the necessary means to overcome the gap between heterogeneous local engineering tool concepts and common project-level concepts which enable the mapping of engineering data coming from different disciplines.
In the context of optical interferometry.
In the convergence analysis of numerical methods for solving partial differential equations (such as finite element methods) one arrives at certain generalized eigenvalue problems.
In the course of the last decade.
In the current era of computer and communication rapid development.
In the development of the applications described in NCL language.
In the domain of information technologies.
In the early stages of learning computer programming.
In the early years of college.
In the era of big data and cloud.
In the era of cloud computing and big data.
In the era of the rapidly developing network.
In the event of natural or man-made disasters.
In the field of data analysis.
In the field of hydrological modelling.
In the field of manufacturing engineering.
In the field of network security.
In the field of software engineering.
In the field of weapon system of systems (WSOS) simulation.
In the following paper.
In the framework of further development of a unified computational tool for the needs of biomedical optics.
In the framework of models generated by compositional expressions.
In the framework of the EU-FP7 SCOUT project.
In the industry 4.
In the information and knowledge society in which we live.
In the information filtering paradigm.
In the Internet of things.
In the last 10 years.
In the last decade.
In the last decade.
In the last decade.
In the last decade.
In the last decades Exp-function method has been used for solving fractional differential equations.
 In this paper.
In the last few years the cost and ease of massively parallel sequencing (MPS) has reduced dramatically to the point that it can now be considered as a tool for use in forensic case work.
 An important consideration for the implementation of any new forensic technology is the ability to remain compatible with previous technology.
 During this study we sequenced the amplicons of two commercial forensic short tandem repeat (STR) multiplexes AmpFlSTR Identifiler and PowerPlex Y using the Illumina MiSeq and Ion PGM Sequencer (Life Technologies) and characterised the sequence data from a forensic perspective.
 Using the MPS data from both platforms we determined the STR genotypes of forensic samples and found previously undocumented sequence variation in seven STR alleles.
 By characterising features of the DNA sequence profiles.
In the last few years.
In the last two decades.
In the last years.
In the literature.
In the modern information and network society.
In the near future we know that vehicles will communicate with each other to make Vehicular ad hoc network and gives the concept of intelligent transportation system.
 In this paper we presented the review of security in VANET.
 Consequently.
In the new era of cyber-physical systems.
In the Oil and Gas industry.
In the ontology engineering field.
In the paper by Gleim et al (2016 Opt.
 Express 24 2619).
In the paper.
In the past decade.
In the past decades.
In the past few decades.
In the past few decades.
In the past few years many choreographers have focused upon implementation of computer technology to enhance their artistic skills.
 Computer vision technology presents new methods for learning.
In the past year.
In the past.
In the past.
In the path minimum problem.
In the Polynomial Global Approach to Time Series Analysis.
In the present paper.
In the present paper.
In the present research.
In the present work.
In the presented work we aimed at improving confocal imaging to obtain highest possible resolution in thick biological samples.
In the process of social development.
In the quest for energy efficient Information and Communication Technology.
In the real world.
In the real world.
In the recent years the number of mobile device have significantly increase and the high bandwidth have led to the demand for Mobile Ad hoc Network (MANET).
 The network offers mobile access to users with minimal configuration to operate.
 Nonetheless.
In the reentry process of hypersonic vehicles.
In the Russian cards problem.
In the scheme of reconstruction.
In the setting of relational databases.
In the supervised classification.
In the today's high-tech world the amount of data and especially spatial data is growing from day to day.
 Databases are one of the best ways to store data.
 Several database concepts exist for storing data.
In the twenty-first century now.
In the video monitoring system.
In the wake of recent advances in experimental methods in neuroscience.
In the wireless sensor networks domain.
In their celebrated paper (Furst et al.
In this article the relevance of using test methods of pattern recognition while developing intelligent systems for decision making support for various problem areas is discussed.
 The advantage of fault-tolerant diagnostic tests used in intelligent systems is shown.
In this article various deferred rendering algorithms are investigated and a classification that formalizes the comparison between these popular rendering techniques is introduced.
 This classification consists of measuring functions that can be used to determine the expected algorithm performance in various situations.
 Multiple analysis spaces are defined that better express the strengths and weaknesses of each algorithm.
 Given the abundance of deferred rendering methods and the performance tradeoffs implied by different hardware targets.
In this article we discuss our implementation of a polyphase filter for real-time data processing in radio astronomy.
 The polyphase filter is a standard tool in digital signal processing and as such a well established algorithm.
 We describe in detail our implementation of the polyphase filter algorithm and its behaviour on three generations of NVIDIA GPU cards (Fermi.
In this article we examine possible advantages to apply Discovery Learning in the first programming computer course in Systems Engineering during the practical sessions.
 The methodology was framed in educational research with qualitative character based in multiple cases study.
 The data were collected during the practical sessions from the opinion of the students and the observation of their advances in the resolution of computational problems.
 The results show more significant and meaningful learning taking the previous knowledge and with the possibility to inferring new knowledge.
 In the same way.
In this article we make a critical assessment of the relation between online and print design.
In this article we propose a novel framework for the modelling of non-stationary multivariate lattice processes.
 Our approach extends the locally stationary wavelet paradigm into the multivariate two-dimensional setting.
 As such the framework we develop permits the estimation of a spatially localised spectrum within a channel of interest and.
In this article we study a classical single-item economic lot-sizing problem.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this article.
In this case report.
In this chapter.
In this commentary.
In this contribution.
In this digital world every one seeks for the information on the internet.
 As the time passed every one placed their digital content on the web.
 On the web the website developers support to place the data in the relational databases.
 The databases from different organizations and agencies are available online and are accessible through web query interfaces.
 The web query interfaces act as the front door to access the information from these relational databases.
 But these web forms or web query interfaces need human intervention to submit the form data.
 So in order to make search engines capable of accessing the relational databases i.
 data behind query interfaces.
In this era of large-scale data.
In this letter.
In this Letter.
In this letter.
In this letter.
In this letter.
In this letter.
In this manuscript.
In this note.
In this note.
In this paper a generalized fractional modified Korteweg-de Vries (FmKdV) equation with time-dependent variable coefficients.
In this paper a novel and effective approach for automated audio classification is presented that is based on the fusion of different sets of features.
In this paper a novel Tensor-Based Image Segmentation Algorithm (TBISA) is presented.
In this paper a pipelined architecture of a high speed network security processor (NSP) for SSL/TLS protocol is implemented on a system on chip (SoC) where hardware information of all encryption.
In this paper a system is proposed to combine small finger movements with the large scale body movements captured from a motion capture system.
 The strength of the proposed work over previous research is in the real-time and natural interactions that the virtual hands have with their environment.
 By being able to conform to physics.
In this paper a system is proposed to combine small finger movements with the large-scale body movements captured from a motion capture system.
 The strength of the proposed work over previous research is in the real-time performance of integrating small-scale finger and hand animation with the full-body skeletal animation.
 This provides a higher degree of immersion and interactivity when compared to more traditional virtual reality systems which use traditional user interfaces.
 A number of experiments are conducted with humanoid skeletons that are both similar to an actual human-e.
 a SWAT officer.
In this paper a work related to using Web3D-based technology at k-12 levels and stimulating individuals' lifelong learning is presented.
 This work has addressed a gap on showing longitudinal investigation results referent to stimulating young children's lifelong learning with support of emerging digital technologies since k-12 levels and its impact on individuals' lifelong development.
 Based on qualitative data collected through action-research and ethnographic techniques in natural settings.
In this paper an analysis of a technical support data with the goal of identifying process improvement actions for reducing interrupts is presented.
 A technical support chat is established and used to provide internal developer support to other development teams.
In this paper are shown quantitative and qualitative changes in the drinking water distribution system.
In this paper is presented a criterion for identification of heat transfer regime through convection ( natural.
In this paper it is shown that the SVD of a matrix can be constructed efficiently in a hierarchical approach.
 The proposed algorithm is proven to recover the singular values and left singular vectors of the input matrix A if its rank is known.
In this paper of computer network information security.
In this paper the application of machine learning techniques for the development of constitutive material models is being investigated.
 A flow stress model.
In this paper the effect of reclaimed asphalt pavement (RAP) on the low-temperature properties of asphalt mixture is experimentally investigated and modelled.
 Two test methods are used: bending beam rheometer (BBR) mixture creep test.
In this paper the networked routers decision support tables are presented as big data structures.
 Routing tables ease the modeling of interconnected networks traffic and update costs.
In this paper three metaheuristics are used to solve a smart grid multi-objective energy management problem with conflictive design: how to maximize profits and minimize carbon dioxide (CO2) emissions.
In this paper two approaches for the computation of the.
In this paper two types of tensor product finite macro-elements are contrasted.
In this paper was proposed an algorithm based on Hierarchical Reinforcement Learning (HRL) and Parallel Computing to solve an online computing problem.
In this paper we analyze and extend mesh-free algorithms for three-dimensional data transfer problems in partitioned multiphysics simulations.
 We first provide a direct comparison between a mesh-based weighted residual method using the common-refinement scheme and two mesh-free algorithms leveraging compactly supported radial basis functions: one using a spline interpolation and one using a moving least square reconstruction.
 Through the comparison we assess both the conservation and accuracy of the data transfer obtained from each of the methods.
 We do so for a varying set of geometries with and without curvature and sharp features and for functions with and without smoothness and with varying gradients.
 Our results show that the mesh-based and mesh-free algorithms are complementary with cases where each was demonstrated to perform better than the other.
 We then focus on the mesh-free methods by developing a set of algorithms to parallelize them based on sparse linear algebra techniques.
 This includes a discussion of fast parallel radius searching in point clouds and restructuring the interpolation algorithms to leverage data structures and linear algebra services designed for large distributed computing environments.
 The scalability of our new algorithms is demonstrated on a leadership class computing facility using a set of basic scaling studies.
 These scaling studies show that for problems with reasonable load balance.
In this paper we are interested in exploring a performance potential of raw disk to reduce disk I/O overhead of web proxy sever without interaction of the conventional file system.
 To show the superior performance potential of raw disk approach.
In this paper we compare different technologies that support distributed computing as a means to address complex tasks.
 We address the task of n-gram text extraction which is a big computational given a large amount of textual data to process.
 In order to deal with such complexity we have to adopt and implement parallelization patterns.
 Nowadays there are several patterns.
In this paper we conduct research on the general computer graphics and psychology and the applications on animation design.
 Digital media art is on the basis of digital technology and modern communication technologies.
In this paper we consider the binary transfer learning problem.
In this paper we consider the mutual exclusion problem on a multiple access channel.
 Mutual exclusion is one of the fundamental problems in distributed computing.
 In the classic version of this problem.
In this paper we deal with relevance and possibilities of fuzzy relational databases regarding its applications in data managing and manipulation.
 Authors have presented contributions.
In this paper we demonstrate the ability of a derivative-driven Monte Carlo estimator to accelerate the propagation of uncertainty through two high-level non-linear finite element models.
 The use of derivative information amounts to a correction to the standard Monte Carlo estimation procedure that reduces the variance under certain conditions.
 We express the finite element models in variational form using the high-level Unified Form Language (UFL).
 We derive the tangent linear model automatically from this high-level description and use it to efficiently calculate the required derivative information.
 To study the effectiveness of the derivative-driven method we consider two stochastic PDEs; a one-dimensional Burgers equation with stochastic viscosity and a three-dimensional geometrically non-linear Mooney-Rivlin hyperelastic equation with stochastic density and volumetric material parameter.
 Our results show that for these problems the first-order derivative-driven Monte Carlo method is around one order of magnitude faster than the standard Monte Carlo method and at the cost of only one extra tangent linear solution per estimation problem.
 We find similar trends when comparing with a modern non-intrusive multi-level polynomial chaos expansion method.
 We parallelise the task of the repeated forward model evaluations across a cluster using the ipyparallel and mpi4py software tools.
 A complete working example showing the solution of the stochastic viscous Burgers equation is included as supplementary material.
 (C) 2017 Published by Elsevier B.
In this paper we describe a fast algorithm that creates a wavelet tree for a sequence of symbols.
 We show that a wavelet tree can be constructed in O(n [log sigma/root logn]) time where n is the number of symbols and a is the alphabet size.
 (C) 2015 Elsevier B.
 All rights reserved.
In this paper we describe ENSEMBLE-ROLLER.
In this paper we discuss an important integrity constraint called multivalued dependency (mvd).
In this paper we discuss how to generate inductive invariants for safety verification of hybrid systems.
 A hybrid symbolic-numeric method is presented to compute inequality inductive invariants of the given systems.
 A numerical invariant of the given system can be obtained by solving a parameterized polynomial optimization problem via sum-of-squares (SOS) relaxation.
 And a method based on Gauss-Newton refinement and rational vector recovery is used to obtain the invariants with rational coefficients.
In this paper we explore why incumbent firms fail to identify new markets in the face of disruptive technologies.
 We cross research on disruptive innovation with research on managerial cognition and we focus on the role of managerial beliefs about customer needs in directing the search for new markets and product features.
 We show that a primary reason why incumbents lose their leadership is the inability to recognize either the rising 'social' market.
In this paper we extend several well-known document listing problems to the case when documents contain a substring that approximately matches the query pattern.
 We study the scenario when the query string can contain a wildcard symbol that matches any alphabet symbol; all documents that match a query pattern with one wildcard must be enumerated.
 We describe a linear space data structure that reports all documents containing a substring Pin O(vertical bar P vertical bar + sigma root logloglog n + docc) time.
In this paper we focus on gender classification from face images.
 Despite advances in equipment as well as methods.
In this paper we give an explicit construction of basis matrices for a (k.
In this paper we have described a hybrid automaton based approach towards the development of a simplified Medium Voltage Direct-Current (MVDC) Shipboard Power System (SPS) model.
 SPS is considered a good subject of hybrid modeling and hybrid control system design as it combines a variety of continuous component dynamics.
In this paper we introduce pwnPr3d.
In this paper we introduce SAWD.
In this paper we investigate the case of ambiguous shape reconstruction from two light-source photometric stereo based on illuminating the unknown Lambertian surface.
 So-far this problem is merely well-understood for two linearly independent light-source directions with one illumination assumed as overhead.
 As already established.
In this paper we investigate univariate algebraic attacks on filter generators over extension fields with focus on the Welch-Gong (WG) family of stream ciphers.
 Our main contribution is to reduce the general algebraic attack complexity on such cipher by proving new and lower bounds for the spectral immunity of such ciphers.
 The spectral immunity is the univariate analog of algebraic immunity and instead of measuring degree of multiples of a multivariate polynomial.
In this paper we present a framework that generates network Indicators of Compromise (IOC) automatically from a malware sample after dynamic runtime analysis.
 The framework addresses the limitations of manual Indicator of Compromise generation and utilises sandbox environment to perform the malware analysis in.
 We focus on the generation of network based IOCs from captured traffic files (PCAPs) generated by the dynamic malware analysis.
 The Cuckoo Sandbox environment is used for the analysis and the setup is described in detail.
 Accordingly.
In this paper we present a learning-based 3D detection of a highly challenging specular object exposed to a direct sunlight at very close-range.
 An object detection is one of the most important areas of image processing.
In this paper we present a mathematical knot/braid diagram interface that exploits 3D computer graphics.
In this paper we present a model transformation language based on logic programming.
 The language.
In this paper we present a novel GPU-based data structure for spatial indexing.
 Based on Fenwick trees-a special type of binary indexed trees-our data structure allows construction in linear time.
 Updates and prefixes can be computed in logarithmic time.
In this paper we present a Smart Push system with feedback enabled flow control suitable for web enabled mobile and IoT devices.
 Our push architecture incorporates a gateway client and gateway server component.
 The flow of the web push notifications is controlled so that they are delivered to the device when the user is most likely to open them.
 We present an overview and implementation details of the Smart Push system with enhanced web push features.
In this paper we present an EEG detection system.
In this paper we present CodeCraft.
In this paper we present different optimization techniques on look-up table based algorithms for double precision floating point arithmetic.
 Based on our analysis of different look-up table based algorithms in the literature.
In this paper we present PMORSya new parallel software package for symmetric sparse matrix ordering on shared memory systems.
 The NP-complete fill-in minimization problem is solved by means of multilevel nested dissection algorithm with modifications for vertex separators.
 Parallel processing is done in a task-based fashion with the granularity tuning.
 We employ threading techniques on shared memory using OpenMP 3.
0 technology as opposed to the Message Passing Interface-based approach widely used for parallel sparse matrix ordering.
 Experimental results on symmetric matrices from the University of Florida Sparse Matrix Collection and matrices from finite-element analysis of three-dimensional strength problems show that our implementation is competitive to the ParMETIS and PT-Scotch libraries both in ordering quality and performance.
 The PMORSy library is publicly available from the Lobachevsky State University Supercomputing Center web-site.
In this paper we propose a logic-based.
In this paper we propose a software architecture that allows for processing of large geospatial data sets in the cloud.
 Our system is modular and flexible and supports multiple algorithm design paradigms such as MapReduce.
In this paper we propose a workflow to detect and track mitotic cells in time-lapse microscopy image.
In this paper we propose an optimization framework for interior carving of 3D fabricated shapes.
 Interior carving is an important technique widely used in industrial and artistic designs to achieve functional purposes by hollowing interior shapes in objects.
 We formulate such functional purpose as the objective function of an optimization problem whose solution indicates the optimal interior shape.
 In contrast to previous volumetric methods.
In this paper we report about an approach to establish safety properties of cooperating systems in polynomial time.
In this paper we show how a thoughtful reusing of libraries can provide concise proofs of non-trivial mathematical results.
 Concretely.
In this paper we show how to apply Grobner bases to compute the Drazin inverse of a matrix with multivariate rational functions as entries.
 When the coefficients of the rational functions depend on parameters.
In this paper we show how to construct a data structure for a string S of size N compressed into a context-free grammar of size n that supports efficient Karp-Rabin fingerprint queries to any substring of S.
In this paper we study a generalized coupled variable-coefficient modified Korteweg-de Vries (CVCmKdV) system that models a two-layer fluid.
In this paper we survey all known (including own recent results) properties of the longest-edge n-section algorithms.
 These algorithms (in classical and recently designed conforming form) are nowadays used in many applications.
In this paper we will try to present a comparative study of non-relational databases and relational databases.
 We mainly focus our presentation on one implementation of the NoSQL database technology.
In this paper(1) we argue that flexible algorithm frameworks can be useful to capture the wide variety of algorithmic components for heuristic algorithms and serve as basic experimental frameworks.
 One of the utilities is that they can implement the wide variety of different algorithm components and their alternative choices for single stochastic local search methods and we are currently extending existing frameworks in that direction.
 We exemplify this approach considering the example of Simulated Annealing (SA).
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this paper.
In this research both the microstructure and thickness of the interfacial transition zone (ITZ) in concretes of Portland cement and lightweight aggregates (LWA) are studied.
 It has been established that the microstructure in the ITZ strongly depends on the nature of the aggregate.
In this research we present a Preoperative Robotic Surgery Simulator platform as a means of training for Raven-II Surgical system.
 The simulator employs PHANTOM Omni (now Geomagic (R) Touch(TM)) as a master device to control the arms of the 3D simulated model of Raven-II in virtual environment.
 The earlier simulators for Raven-II were built primarily to work over Debian-based Linux Operating Systems.
 This is the first attempt towards development of a simulator in MATLAB which.
In this research.
In this study an explicit Finite Difference Method (FDM) based scheme is developed to solve the Maxwell's equations in time domain for a lossless medium.
 This manuscript focuses on two unique aspects - the three dimensional time-accurate discretization of the hyperbolic system of Maxwell equations in three-point non-staggered grid stencil and it's application to parallel computing through the use of Graphics Processing Units (GPU).
 The proposed temporal scheme is symplectic.
In this study high-performance and high-speed field-programmable gate array (FPGA) implementations of polynomial basis Itoh-Tsujii inversion algorithm (ITA) over GF(2(m)) constructed by irreducible trinomials and pentanomials are presented.
 The proposed structures are designed by one field multiplier and k-times squarer blocks or exponentiation by 2(k).
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this study.
In this two-part paper.
In this type functional variable method has been used to private type of nonlinear fractional differential equations.
 The main property of the method demonstrate in its flexibility and ability to solve nonlinear equations accurately.
In this work we aim to detect faces in violence scenes.
In this work we consider optical flow on evolving Riemannian 2-manifolds which can be parametrised from the 2-sphere.
 Our main motivation is to estimate cell motion in time-lapse volumetric microscopy images depicting fluorescently labelled cells of a live zebrafish embryo.
 We exploit the fact that the recorded cells float on the surface of the embryo and allow for the extraction of an image sequence together with a sphere-like surface.
 We solve the resulting variational problem by means of a Galerkin method based on vector spherical harmonics and present numerical results computed from the aforementioned microscopy data.
In this work we present a multi-source uncertain entity resolution model and show its implementation in a use case of Yad Vashem.
In this work we present an efficient implementation of Canonical Monte Carlo simulation for Coulomb many body systems on graphics processing units (GPU).
 Our method takes advantage of the GPU Single Instruction.
In this work we present an efficient interferometric processing chain.
In this work we present an interactive and collaborative 3D object manipulation system using off the shelf mobile devices coupled with Augmented Reality (AR) technology that allows multiple users to collaborate concurrently on a scene.
 Each user interested in participating in this collaboration uses both a mobile device running android and a desktop (or laptop) working in tandem.
 The 3D scene is visualized by the user in the desktop system.
 The changes in the scene viewpoint and the object manipulation are performed using a mobile device through object tracking.
 Multiple users can collaborate on object manipulation by using a laptop and a mobile device each.
 The system leverages user's knowledge of common tasks performed on current mobile devices using gestures.
 We built a prototype system that allows users to complete the requested tasks and performed an informal user study with experienced VR researchers to validate the system.
In this work we present an interactive and collaborative 3D object manipulation system using the shelf mobile devices coupled with Augmented Reality (AR) technology that allows multiple users to collaborate concurrently on a scene.
 Each user interested in participating in this collaboration uses both a mobile device running android and a desktop (or laptop) working m tandem.
 The 3D scene was visualized by the user in the desktop system.
 The changes in the scene viewpoint changes and the object manipulations were performed using a mobile device through the AR.
 The system leverages user's knowledge of common tasks performed on current mobile devices such as pinching for zooming in and out; swiping with one or two fingers for object rotation and press -and -hold for 2 seconds for object translation.
 As you will see in this video.
In this work we present and analyze three approaches for the adaptive control of the operating point of a cascade of erbium-doped fiber amplifiers (EDFAs).
In this work we present the first design and implementation of a wait-free hash map.
 Our multiprocessor data structure allows a large number of threads to concurrently insert.
In this work we present the functional specifications.
In this work we propose a novel text mining approach for assisting academic staff in their course design and curriculum development activities.
 The proposed approach is based on mining the textual content of technical and academic Q&A websites and online forums to find the most up-to-date and challenging topics whose inclusion in the curriculum would enhance the quality of related courses offered in higher education institutes.
 In specific.
In this work we report more recent results of a long-lasting educational project that we have been carrying on for several years and is evolving continuously.
 The objective of the mentioned project is making students work on the production of small.
In this work.
In this work.
In this work.
In this work.
In this work.
In this work.
In this work.
In this work.
In this work.
In this work.
In this work.
In this work.
In this work.
In this work.
In today's data centers supporting Internet-scale computing and input/output (I/O) services.
In today's enterprise world.
In today's highly intertwined network society.
In today's industrial environment.
In today's Internet-connected world.
In today's world.
In traditional approach.
In traditional framework.
In traditional teaching methods of teaching science and engineering.
In view of the biomass gasification processes a nonlinear.
In view of the practical application problems of the traditional parallel processing technology existing in the processing of the massive data.
In view of the special structure of isolated grid.
In visual cryptography.
In visual tracking.
In viticulture.
In widely used mobile operating systems a single vulnerability can threaten the security and privacy of billions of users.
In wire and arc additive manufacture (WAAM).
In wireless communication systems.
In wireless communication systems.
In wireless sensor networks (WSNs).
In wireless sensor networks.
In work reported here.
Inadequate skid resistance of pavement surface is a substantial reason for traffic accidents.
 There is a close relationship between sliding resistance and characteristics of texture morphology.
Incident angle of light source is a key factor that affects imaging resolution of direct optical imaging for a reflected target.
 For a large incident angle.
Incomplete information has been studied in-depth in relational databases and knowledge representation.
 In the context of the Web.
Incompressible fluids are of current interest.
 Considering a (3+1)-dimensional variable-coefficient Boiti-Leon-Manna-Pempinelli model for an incompressible fluid.
Incorporating the quantity and variety of observations in atmospheric and oceanographic assimilation and prediction models has become an increasingly complex task.
 Data assimilation allows for uneven spatial and temporal data distribution and redundancy to be addressed so that the models can ingest massive data sets.
 Traditional data assimilation methods introduce Kalman filters and variational approaches.
 This study introduces a family of algorithms.
Increased knowledge of DNA-binding proteins would enhance our understanding of protein functions in cellular biological processes.
 To handle the explosive growth of protein sequence data.
Increasing availability of multicore systems has led to greater focus on the design and implementation of languages for writing parallel programs.
 Such languages support various abstractions for parallelism.
Increasing evidence has shown that miRNAs are implicated in carcinogenesis and can function as on-cogenes or tumor suppressor genes in human cancers.
 In this study.
Increasing evidence suggests that numerous forkhead transcription factors are required to repress the mammalian cells phenotype.
 Among them.
Increasing habitat connectivity is important for mitigating the effects of climate change.
Increasing interest in simultaneously optimizing many objectives (typically more than three objectives) of problems leads to the emergence of various many-objective algorithms in the evolutionary multi-objective optimization field.
Increasing networking performances as well as the emergence of Mixed Reality (MR) technologies make possible providing advanced interfaces to improve remote collaboration.
 In this paper.
Increasing studies have demonstrated that interferon gamma (IFN-gamma).
Incremental computation has been broadly applied in computer software field.
Indexed data types allow us to specify and verify many interesting invariants about finite data in a general purpose programming language.
 In this paper we investigate the dual idea: indexed codata types.
Indoor scene classification is usually approached from a computer vision perspective.
Indoor space 3D visual reconstruction has many applications and.
Industrial Internet of Things (IIoT) is emerging as a new communication paradigm for a class of low-power wireless networks used in critical applications.
Infectious diseases caused by multidrug-resistant (MDR) Enterobacteriaceae have exponentially increased in the past decade.
Information granulation is a powerful tool for data analysis and processing.
Information hiding is an active area of research where secret information is embedded in innocent-looking carriers such as images and videos for hiding its existence while maintaining their visual quality.
 Researchers have presented various image steganographic techniques since the last decade.
Information hiding technology is very important to wireless sensor network security.
Information leakage and memory disclosures are significant threats to the security of modern operating systems.
 If an attacker is able to obtain the binary-code of a program.
Information over the web is increasingly retrieved from relational databases in which the query language is based on exact matching.
Information retrieval and web search present a Challenging Question to researches.
 Today users urge for accurate and precise hands on information from Search Machine.
 Interpreting of user query goal is major challenge in past and present.
 Numerous algorithms and Frameworks have be proposed.
Information security can be achieved using cryptography.
Information Security in the recent years has become an important area.
 There is always a necessity for exchanging information secretly between the sender and the receiver.
 Cryptography reveals its deep roots of history in providing a way to transmit the sensitive information across networks.
Information security is key important when we are trying to interconnect the wireless body sensor network with the healthcare social network via mobile facilities.
 In this paper.
Information sources such as relational databases.
Information technology has been contributing to various areas of knowledge; in particular.
Infrastructure as a service with desktops (DIaaS) based on the extensible mark-up language (XML) is herein proposed to utilize surplus resources.
 DIaaS is a traditional surplus-resource integrated management technology.
 It is designed to provide fast work distribution and computing services based on user service requests as well as storage services through desktop-based distributed computing and storage resource integration.
 DIaaS includes a nondisruptive resource service and an auto-scalable scheme to enhance the availability and scalability of intra-cloud computing resources.
 A performance evaluation of the proposed scheme measured the clustering performance time for surplus resource utilization.
 The results showed improvement in computing and storage services in a connection of at least two computers compared to the traditional method for high-availability measurement of nondisruptive services.
 Furthermore.
Infrastructure systems in the United States are in need of urgent maintenance and rehabilitations.
 Preservation of culvert networks.
Inner Source (IS) is the use of open source software development practices and the establishment of an open source-like culture within organizations.
 The organization may still develop proprietary software but internally opens up its development.
 A steady stream of scientific literature and practitioner reports indicates the interest in this research area.
Innovation scholars have long recognized entrepreneurship is imitative'.
Innovative systems and automated computational procedures.
Insertion propagation problem is a class of view update problem in relational databases [1].
 Given a source database D.
Inspired by progresses in cognitive science.
Inspired by social networks and complex systems.
Inspired by the biological immune system against outside invasion in nature.
Inspired by the special properties of the graph state.
Instant messenger.
Integrals are important to many applications ranging from physics over engineering to statistics.
 While systematic methods for symbolic computation of integrals have a long history.
Integrated information systems are important safeguards for the utilisation and development of land resources.
 Information technologies.
Integrated urban drainage modelling combines different aspects of the urban water system into a common framework.
 With increasing pressures of a changing climate.
Integrating computer vision and natural language processing is a novel interdisciplinary field that has received a lot of attention recently.
 In this survey.
Intel Xeon Phi accelerators are one of the newest devices used in the field of parallel computing.
Intelligent fault-diagnosis methods using machine learning techniques like support vector machines and artificial neural networks have been widely used to distinguish bearings' health condition.
Intelligent tutoring and personalization are considered as the two most important factors in the research of learning systems and environments.
 An effective tool that can be used to improve problem-solving ability is an Intelligent Tutoring System which is capable of mimicking a human tutor's actions in implementing a one-to-one personalized and adaptive teaching.
 In this paper.
Intelligent Tutoring Systems (ITSs) have been producing consistent learning gains for decades.
Intentional or unintentional leakage of confidential data is undoubtedly one of the most severe security threats that organizations face in the digital era.
 The threat now extends to our personal lives: a plethora of personal information is available to social networks and smartphone providers and is indirectly transferred to untrustworthy third party and fourth party applications.
 In this work.
Interactions between decision variables typically make an optimization problem challenging for an evolutionary algorithm (EA) to solve.
 Exploratory landscape analysis (ELA) techniques can be used to quantify the level of variable interactions in an optimization problem.
Interactive 3D terrain visualization plays an important role in multiple networked applications like virtual worlds visualization.
Interactive database exploration is a key task in information mining.
 Relational databases have been long used as a critical infrastructure component to access and analyze large volumes of data in a variety of applications.
Interactive image segmentation has remained an active research topic in image processing and graphics.
Interactivity in e-learning environment is an innovative approach in teaching-learning.
 Predominantly theoretical justification of interactive learning environment has been discussed on the basis of the process of visual and auditory information in the memory system emphasizing the result oriented perspective in the sense that they have given importance on computer response to learner action rather than learner activity and engagement in computer programming.
Inter-basin water transfer projects might cause complex hydro-chemical and biological variation in the receiving aquatic ecosystems.
 Whether machine learning models can be used to predict changes in phytoplankton community composition caused by water transfer projects have rarely been studied.
 In the present study.
Interest in higher-order tensors has recently surged in data intensive fields.
Interference alignment (IA) is a method to improve the capacity of cell-edge users and thus attracts an intense research interest.
 We focus on the IA extended to the multiple-input multiple-output (MIMO) interference network.
 In this method.
Intermittency of wind energy pose a great challenge for power system operation and control.
 Wind curtailment might be necessary at certain operating condition to keep the line flow within limit.
 Remedial Action Scheme (RAS) offers quick control action mechanism to keep reliability and security of the power system operation with high wind energy integration.
 In this paper.
Internal users are the main causes of anomalous and suspicious behaviors in a communication network.
 Even when traditional security middleboxes are present.
Internal validation techniques can be used to check the predictive ability of the developed models.
 The most common internal validation techniques are split sample methods.
Internet and organizational network security is still threatened by devastating malicious activities.
 Given the continuous escalation of such attacks in terms of their frequency.
Internet banking is one of many services provided by financial institutions that have become very popular with an increasing trend.
 Due to the increased amount of usage of the service.
Internet is playing a more and more important role in today's society.
 Wireless network and mobile devices are becoming part of our lives.
 So it's significant to learn about information of the users or network usage for both network researchers and managers of fixed/wireless network.
 Previous methods of user characterizing and network usage are mostly based on TCP traffic study.
 But when facing to a large amount of users.
Internet of Things (IoT) is an environment in which everywhere and every device became smart in a smart world.
 Internet of Things is growing vastly.
Internet of things is based on sensors.
Internet protocol (IP) spoofing is a serious problem on the Internet.
 It is an attractive technique for adversaries who wish to amplify their network attacks and retain anonymity.
 Many approaches have been proposed to prevent IP spoofing attacks; however.
Internet security problems are still a big challenge as there are many security events occurred.
Internet technology today is not free from many problems or security holes.
 This security holes could be exploited by an unauthorized person to steal important data.
 The case of the attacks occurred because the party that was attacked also did not realize the importance of network security to be applied to the system.
 Honeypot is a system that is designed to resemble the original production system and is made with the intention to be attacked or compromised.
 In this research.
Internet worm infection continues to be one of top security threats and has been widely used by botnets to recruit newbots.
 In order to defend against future worms.
Interoperability among heterogeneous software systems is a software quality sub-characteristic.
 Some methods for dealing with interoperability exhibit differences in aspects like generality.
Intersections are known for their integral and complex nature due to a variety of the participants' behaviors and interactions.
 This paper presents a review of recent studies on the behavior at intersections and the safety analysis for three types of participants at intersections: vehicles.
Intrinsic image decomposition is an important topic in computer vision and computer graphics applications.
Introduction and objective: Computer Aided Diagnosis (CAD) systems based on Medical Imaging could support physicians in several fields and recently are also applied in histopathology.
 The aim of this work is to discuss in detail the design and testing of a CAD system for segmentation and discrimination of blood vessels versus tubules from biopsies in the kidney tissue through the elaboration of histological images.
 Materials and methods: Materials consist in 10 Kidney Biopsy Slides (KBS) with Periodic Acid Schiff (PAS) staining.
 The Regions Of Interest (ROI) identified by expert are in total 221:71 vessels and 150 tubules.
 KBS preparation and digital acquisition have been conducted by expert technicians at the Department of Emergency and Organ Transplantation (DETO) of the University of Bari Aldo Moro (Italy).
 Each slice is a Red Green Blue (RGB) format image with a resolution of 0.
50 mu m/pixel.
 Starting from KBS images.
Introduction The alternative lengthening of telomeres (ALT) mechanism was first observed in the model organism S.
 cerevisiae.
 Interestingly.
Introduction Worldwide the transport sector faces several issues related to the rising of traffic demand such as congestion.
Introduction.
 Proposed concept for training Internet of Things technology within the framework of the speciality Telecommunications and Radio Engineering.
 It is given a practice of organization and sequencing courses for students.
Introduction: Casein201 is one of the human milk sourced peptides that differed significantly in preterm and full-term mothers.
 This study is designed to demonstrate the biological characteristics.
Introduction: Evolutionarily selected over billions of years for their interactions with biomolecules.
Introduction: Huntington's disease (HD) is a genetic neurodegenerative disorder that primarily affects striatal neurons.
 Striatal volume loss is present years before clinical diagnosis; however.
Introduction: Placental dysfunction characterized by vascular endothelial inflammation is one of the most notable responses to fetal cardiac bypass.
 Regulator of calcineurin 1 (RCAN1) is an important regulator of inflammatory responses.
 MicroRNAs (miRNAs) are essential post-transcriptional modulators of gene expression.
Introduction: The aberrant or misfolded forms of the prion protein have been described as the causative agents of rare transmissible spongiform encephalopathies.
 In addition.
Introduction: The aim of this study was to clarify the microRNA (miRNA) expression profiles of RAW264.
7 macrophages infected by Candida albicans to elucidate the roles of differentially expressed miRNAs and to further explore the mechanisms underlying the immune response to C.
 albicans infection.
 Methods: High-throughput miRNA microarray analysis was performed to detect differentially expressed miRNAs in control and C.
 albicans-infected RAW264.
 Quantitative real-time PCR analysis was used to verify the microarray results.
 Target genes of differentially expressed miRNAs were predicted with bioinformatics software.
 The cell biological processes and signaling pathways of these miRNA-targeted genes involved in C.
 albicans infection were predicted by gene ontology (GO) enrichment and pathway analyses.
 Results: Significant upregulation of eight miRNAs (mmu-miR-140-5p.
Introduction: The Alzheimer's Disease Neuroimaging Initiative (ADNI) has continued development and standardization of methodologies for biomarkers and has provided an increased depth and breadth of data available to qualified researchers.
 This review summarizes the over 400 publications using ADNI data during 2014 and 2015.
 Methods: We used standard searches to find publications using ADNI data.
 Results: (1) Structural and functional changes.
Introduction: The effectiveness of lantibiotics against MDR pathogens and the progression of agents MU1140.
Introduction: The PEI Programme in the WHO African region invested in recruitment of qualified staff in data management.
Introduction: Use of computers is generally encouraged; this is to keep up with the fast-moving world of technology.
Introduction: With an increase in the number of mentally demanding jobs.
Introductory computer programming courses can be dry.
Intrusion detection has become essential to network security because of the increasing connectivity between computers.
 Several intrusion detection systems have been developed to protect networks using different statistical methods and machine learning techniques.
 This study aims to design a model that deals with real intrusion detection problems in data analysis and classify network data into normal and abnormal behaviors.
 This study proposes a multi-level hybrid intrusion detection model that uses support vector machine and extreme learning machine to improve the efficiency of detecting known and unknown attacks.
 A modified K-means algorithm is also proposed to build a high-quality training dataset that contributes significantly to improving the performance of classifiers.
 The modified K-means is used to build new small training datasets representing the entire original training dataset.
Intrusion detection has been playing a crucial role for making a computer network secure for any transaction.
 An intrusion detection system (IDS) detects various types of malicious network traffic and computer usage.
Intrusion detection in wireless mobile networks without infrastructure is of great importance because of the dynamic structure.
Intrusion detection is the essential part of network security in combating against illegal network access or malicious attacks.
 Due to constantly evolving nature of network attacks.
Intrusion detection system (IDS) is one of the important elements for providing the security in networks.
 Increasing the number of network-based applications on the one hand and increasing the data volumes on the other hand forced the designers to conduct some research on the novel methods for improving network security.
 One of the recent efforts to improve IDS performance is developing the machine learning algorithms.
 Random forest is one of the powerful algorithms employed in data mining.
 It operates based on classifier fusion principles and is implemented as detection engine in some anomaly-based IDSs.
 In this paper.
Intrusion detection system using honeypot being increasingly used technique for intrusion detection and recovery of data disrupted by intruders.
 This paper presents survey of intrusion detection system with different honeypot techniques.
Intrusion Detection Systems (IDS) are one of the primary components in keeping a network secure.
 They are classified into different forms based on the nature of their functionality such as Host based IDS.
Intrusion response system (IRS) is one of the most important components in the network security solution that selects appropriate countermeasures to handle the intrusion alerts.
Intuitionistic fuzzy databases are used to handle imprecise and uncertain data as they represent the membership.
Investigated in this paper is a variable-coefficient higher-order nonlinear Schrodinger equation.
Investigation in this paper is a discrete Ablowitz-Ladik equation.
IP identification (IPID) is an IP header field which is designed to identify a packet in a communication session.
 The main purpose of IPID is to recover from IP fragmentation.
 To the best of our knowledge.
IPv6 provides more address space.
IPv6 was designed to solve the issue of adopting IPv4 addresses by presenting a large number of address spaces.
is increasingly apparent that the traditional scene graph is not fulfilling the requirements of real-time interactive systems.
 The use of a single graph as a representation of the current state of the world means that display systems.
It goes without saying that the use of FIDO based services.
It has always been challenging to reconstruct magnetic resonance (MR) images from a limited set of k-space data due to the ill-posed nature.
 An effective way to compensate for the data incompleteness is through the use of the sparsity-based regularisation.
 Recent work in image processing suggests that exploiting structured sparsity may lead to improved results.
 In this study.
It has been more than 40 years since the first digital imaging sensor was invented.
 For the last four decades.
It is a common expectation that analytical rotations of factors and components aiming for a simple structure allow dimensions of intercorrelated.
It is always computing intensive and time consuming to extract polygons from massive classified images.
 Although parallel computing can improve the efficiency of geographical data processing.
It is difficult to extend a grayscale morphological approach to color images because total vector ordering is required for color pixels.
 To address this issue.
It is essential to design a protocol to allow sensor nodes to attest to their trustworthiness for mission-critical applications based on Wireless Sensor Networks (WSNs).
It is hard to estimate optical flow given a realworld video sequence with camera shake and other motion blur.
 In this paper.
It is highly recognized that difficulties involved in teaching programming in an introductory course.
It is important to establish the forecasting model of the network security situation.
 But the network security situation cannot be observed directly and can only be measured by other observable data.
 In this paper the network security situation is considered as a hidden behavior.
 In order to predict the hidden behavior.
It is known that the logic BI of bunched implications is a logic of resources.
 Many studies have reported on the applications of BI to computer science.
 In this paper.
It is now several years since scientists in Poland can use the resources of the distributed computing infrastructure - PLGrid.
 It is a flexible.
It is quite a challenge to remember or privately store the key for encrypting the secret image.
It is significant to perform damage identification of wind turbine using running condition data for guaranteeing its safe operation.
 Because acquired condition data is usually mixed with heavy background noise.
It is traditionally assumed that requirements specification.
It is well known that cells in tissue display a large heterogeneity in gene expression due to differences in cell lineage origin and variation in the local environment.
 Traditional methods that analyze gene expression from bulk RNA extracts fail to accurately describe this heterogeneity because of their intrinsic limitation in cellular and spatial resolution.
It is well known that computer has been widely used in almost all fields.
 In this paper.
It is well known that distance estimation in immersive virtual reality environments suffers from compression when viewed from an egocentric perspective with a head mounted display.
 While previous research indicates that providing the user with a virtual avatar with high geometric and motion fidelity may alleviate this problem.
It is well known that laser scanner has better accuracy than stereo vision in detecting the distance and velocity of the obstacles.
It is well known that multipath effect remains a dominant error source that affects the positioning accuracy of Global Navigation Satellite System (GNSS) receivers.
 Significant efforts have been made by researchers and receiver manufacturers to mitigate multipath error in the past decades.
It is well known that there are linear-space data structures for 2-D orthogonal range counting with worst-case optimal query time O(log n/ loglog n).
 We give an O(n log log n)-space adaptive data structure that improves the query time to O(log log n + log k/ log log n).
It is widely accepted that developing the ability to solve problems is essential.
 Computational thinking is based on problem solving using basic concepts of computing.
 An introductory course to programming is a direct way to develop the ability to solve problems using computer concepts.
 This paper presents our thinking about initiating students into the field of computer programming.
 This work does not detail the contents to be taught.
It is widely recognized that the public-key cryptosystems are playing tremendously an important role for providing the security services.
 In majority of the cryptosystems the crucial arithmetic operation is modular exponentiation.
 It is composed of a series of modular multiplications.
It remains incompletely understood whether there is an association between the transcriptome profiles of skeletal muscle and blood leukocytes in response to exercise or other physiological stressors.
 We have previously analyzed the changes in the muscle and blood neutrophil transcriptome in eight trained men before and 3.
It was an important part of the aggregation simulation model building that optimization of the simulation model.
 In this paper.
It was demonstrated that the entanglement evolution of a specially designed quantum state in the bistochastic channel is asymmetric.
 In this work.
It was proposed that magnetic fields (MFs) can influence gene transcription via CTCT motif located in human HSP70 promoter.
 To check the universality of this mechanism.
It's essential to establish a dynamic model of a wheeled tractor with suspended driver seat including air spring and Magnetorheological (MR) damper to analyze its vibration characteristics and performance.
 The equivalent linearized model of the wheeled tractor always neglects the effect of nonlinear stiffness for scissors linkage driver seat and air spring with auxiliary chamber as well as dynamic characteristics of real tractor tyre and considers the driver seat as linear spring and damper elements.
 Prynne's 1997 book For the Monogram arguably marks the beginning of what has been termed his 'late style': a period of avant-garde poetic output characterised by a shift towards monolithic blocks of text featuring highly disrupted syntax and a vocabulary drawn from a range of increasingly technical and specialist fields.
 This paper considers whether such 'high-tech' writing requires a similarly high-tech approach to reading.
Java programming is a lot of colleges and universities have set up a computer programming language courses.
Job scheduling is a very important topic in parallel systems.
 Although there exist algorithms that theoretically provide optimal performance.
Job scheduling strategy of rendering was studied according to the scheduling model of heterogeneous cluster rendering system.
Job shop scheduling problem (JSP) which is widespread in the real-world production system is one of the most general and important problems in various scheduling problems.
Joint injury causes post-traumatic osteoarthritis (PTOA).
 About approximate to 50% of patients rupturing their anterior cruciate ligament (ACL) will develop PTOA within 1-2 decades of the injury.
Joint Polar Satellite System (JPSS) data product improvement requires a substantial record of on-orbit data extending over years.
 Such a data record now exists for the JPSS instruments developed for the Suomi National Polar Orbiting Partnership (SNPP) risk reduction mission.
 SNPP has now provided well over three years of on-orbit data from four JPSS instruments and derived data products.
 It is now feasible to apply a system engineering approach to data product improvement.
 In this approach on-orbit performance characteristics are traced back to instrument and algorithm design characteristics and pre-launch test data.
 During the instrument development phase system engineering was applied in the reverse direction.
Joint service involving several clouds is an emerging form of cloud computing.
 In hybrid clouds.
Joint sparse representation (JSR) has shown great potential in various image processing and computer vision tasks.
 Nevertheless.
k nearest neighbor (kNN) is one of the basic processes behind various machine learning methods In kNN.
Kelly et al.
 (Coast Eng J 57(4):1-30.
Kernel discriminant subspace learning technique is effective to exploit the structure of image dataset in the high-dimensional nonlinear space.
Key encapsulation mechanism (KEM) is an important key distribution mechanism that not only allows both sender and receiver to safely share a random session key.
Key predistribution schemes for resource-constrained networks are methods for allocating symmetric keys to devices in such a way as to provide an efficient trade-off between key storage.
Key-value (KV) stores have become a backbone of large-scale applications in today's data centers.
 Writeoptimized data structures like the Log-Structured Merge-tree (LSM-tree) and their variants are widely used in KV storage systems like BigTable and RocksDB.
 Conventional LSM-tree organizes KV items into multiple.
Keyword search in relational databases provides a simple and interactive query interface for retrieving data from databases.
 In recent year's keyword search over structured or unstructured data received significant attention.
 KWS performs on enterprise applications based on various forms which can take some values.
Keyword search over relational databases has been widely studied for the exploration of structured data in a userfriendly way.
Kinship verification is an interesting and challenging problem in human face analysis.
Knowing faulty modules prior to testing makes testing more effective and helps to obtain reliable software.
Knowledge discovery is the process of discovering useful knowledge in a broad range of sources.
Knowledge management within companies and institutions is one of the main problems when defining and developing new solutions to improve information systems.
Knowledge representation is the core of artificial intelligence research.
 Knowledge representation methods include predicate logic.
Korteweg-de Vries (KdV)-type equation can be used to characterise the dynamic behaviours of the shallow water waves and interfacial waves in the two-layer fluid with gradually varying depth.
 In this article.
Korteweg-de Vries (KdV)-type equations are used as approximate models governing weakly nonlinear long waves in fluids.
Korteweg-de Vries (KdV)-type equations can describe the nonlinear waves in fluids.
Kriging interpolation provides the best linear unbiased estimation for unobserved locations.
Labeling children's social play behavior is an important process in children's peer-play analysis which is traditionally done by experienced coders.
 With the growing volume of data.
Laboratory experiments were conducted to study the dynamics of sand jets passing through two immiscible fluids.
 Different oil layer thicknesses.
Lambda-SF-calculus can represent programs as closed normal forms.
Lameness is a major issue in dairy herds and its early and automated detection offers animal welfare benefits together with potentially high commercial savings for farmers.
 Current advancements in automated detection have not achieved a sensitive measure for classifying early lameness.
 A novel proxy for lameness using 3-dimensional (3D) depth video data to analyse the animal's gait asymmetry is introduced.
 This dynamic proxy is derived from the height variations in the hip joint during walking.
 The video capture setup is completely covert and it facilitates an automated process.
 The animals are recorded using an overhead 3D depth camera as they walk freely in single file after the milking session.
 A 3D depth image of the cow's body is used to automatically track key regions such as the hooks and the spine.
 The height movements are calculated from these regions to form the locomotion signals of this study.
Lameness is one of the most important causes of poor welfare in poultry.
 Previous studies have documented approximately 30% of the chickens were seriously lame.
 In this research.
Land degradation and environmental impacts are major impediments to the utilization of land in many arid and semiarid regions of the world and are a major issue in the Northwestern Sinai (NWS).
Landslides are considered as one of the natural hazards responsible for casualties.
Language standards such as C99 and C11.
Laparoscopic surgery is a complex minimum invasive operation that requires long learning curve for the new trainees to have adequate experience to become a qualified surgeon.
 With the development of virtual reality technology.
Large 3D model repositories of common objects are now ubiquitous and are increasingly being used in computer graphics and computer vision for both analysis and synthesis tasks.
Large holes are unavoidably generated in depth image based rendering (DIBR) using a single color image and its associated depth map.
 Such holes are mainly caused by disocclusion.
Large multicenter acute stroke trials demand a randomization procedure with a high level of treatment allocation randomness.
Large scale multi-label learning.
Large scale parallel computing system is becoming more and more failure-prone due to the increasing number of computational nodes.
 This results in serious reliability problems in parallel computing.
 To ensure successfully running of parallel tasks such as Meta tasks and DAG tasks.
Large-scale computing technologies have enabled high-throughput virtual screening involving thousands to millions of drug candidates.
 It is not trivial.
Large-scale data-intensive web services are evolving faster than ever.
Large-scale dynamic systems are becoming highly pervasive in their occurrence with applications ranging from system biology.
Large-scale e-science features complex DAG-structured workflows comprised of computing modules with intricate inter-module dependencies.
 Mapping such workflows in heterogeneous network environments and optimizing their end-to-end performance are crucial to the success of scientific collaborations that require fast system response and smooth data flow.
 We construct analytical cost models and formulate workflow mapping as optimization problems for minimum end-to-end delay and maximum frame rate.
 The difficulty of these problems essentially arises from the topological matching nature in the spatial domain.
Large-scale network and graph analysis has received considerable attention recently.
 Graph mining techniques often involve an iterative algorithm.
Laser zona drilling (LZD) is a required step in many embryonic surgical procedures.
Last few years have seen introduction of more and more advanced manycore processors.
 Both very well known devices like GPGPU and Intel MIC and less popular.
Latency is a pressing problem in Virtual Reality (VR) applications.
 Low latencies are required for VR to reduce perceptual artifacts and cyber sickness.
 Additionally.
Latency jitter is a pressing problem in Virtual Reality (VR) applications.
 This paper analyzes latency jitter caused by typical interprocess communication (IPC) techniques commonly found in today's computer systems used for VR.
 Test programs measure the seal ability and latencies for various IPC techniques.
Lateral diffusion and compartmentalization of plasma membrane proteins are tightly regulated in cells and thus.
Latest variants of denial-of-service attack like low-rate denial-of-service attack require very few packets for launching an attack.
 As a result.
Lattice problems are considered as the key elements in many areas of computer science as well as in cryptography; the most important of which is the shortest vector problem and its approximate variants.
 Algorithms for this problem are known as lattice reduction algorithms.
Leaf area is an important plant canopy structure parameter with important ecological significance.
 Light detection and ranging technology (LiDAR) with the application of a terrestrial laser scanner (TLS) is an appealing method for accurately estimating leaf area; however.
Learning and teaching computer programming have been acknowledged as being difficult and challenging.
 The metacognitive learning environment is needed for learning success in Computer Programming problem solving.
 In designing a support learning tool.
Learning programming is an inherently exploratory activity.
 Supporting this process is not a trivial task and requires resources.
 This paper proposes the utilisation of an intelligent Exploratory Learning Environment to enhance teaching and learning in this context with minimal cost.
Learning to program in computer code has been considered one of the pillars of contemporary education with benefits that reach well beyond the skills required by the computing industry.
Leishmaniasis is a parasitic disease caused by the protozoan of the Leishmania genus.
 While no human vaccine is available.
Leptospira spp.
 are diderm (two membranes) bacteria that infect mammals causing leptospirosis.
Let A(x) = A(0)+x(1)A(1)+.
+x(n)A(n) be a linear matrix.
Let be a collection of D string documents of n characters in total.
Let D be a collection of D documents.
Let f*(z)= Sigma(infinity)(j=0) a(j)*z(j) be a convergent series in which {a(j)*}(j=0)(infinity) are known real numbers.
 In this paper.
Let p(m)(x) = P-m((lambda))(x)/P-m((lambda))(1) he the m-th ultraspherical pohnomial noronalioed by p(m)(1) = 1.
 We prove the inequality vertical bar x vertical bar p(n)(2)(x)-p(n-1)(x)p(n+1)(x) >= 0.
Let S be a set of n points on an n x n integer grid.
 The maximal layer of S is a set of points in S that are not dominated by any other point in S.
 Considering Q as an axes parallel query rectangle.
Let X = {f(1).
Lexicographic preferences on a set of attributes provide a cognitively plausible structure for modeling the behavior of human decision makers.
LibCoopt is an open-source matlab code library which provides a general and convenient tool to approximately solve the combinatorial optimization problems on the set of partial permutation matrices.
Libraries are a collection of implementations of behavior written in a computer programming language providing a well-defined interface by which the behavior can be invoked.
 Although a majority of the code in numerous applications comes from libraries.
Life Cycle Assessment (LCA).
Life of many people and vital social activities depend on good functioning of civil structures such as nuclear power plants.
Lifecycle structural health monitoring (SHM) systems provide an abundance of information that is greatly beneficial for securing structural safety over the whole service life.
 In application to large-scale structures.
Light Emitting Diode (LED) luminaires provide compelling benefits such as long-term cost savings.
Lightweight cipher designs try to minimize the implementation complexity of the cipher while maintaining some specified security level.
 Using only a small number of AND gates lowers the implementation costs.
Lightweight devices such as smart cards and RFID tags have a very limited hardware resource.
Lightweight multi-kernel architectures.
Like all software systems.
Like natural climate change on the planet earth.
Like other neurodegenerative diseases.
Like the traditional machine learning.
limmersive displays for virtual reality systems can be roughly classified into spatially immersive displays (similar to CAVE-like displays or large-screen simulators) or head-mounted displays.
 The former type is usually static in spatial configuration and configured to support a small group of users.
 The latter supports only a single user.
 We propose a new class of actuated.
Linear Hashing is an efficient and widely used version of extendible hashing.
 LH is its distributed version that stores key-value pairs on up to hundreds of thousands of sites in a distributed system.
 LH implements the dictionary data structure efficiently by not using a central component and allows the key-based operations of insertion.
Linear max-plus systems describe the behavior of a large variety of complex systems.
 It is known that these systems show a periodic behavior after an initial transient phase.
 Assessment of the length of this transient phase provides important information on complexity measures of such systems.
Linear regression models depend directly on the design matrix and its properties.
 Techniques that efficiently estimate model coefficients by partitioning rows of the design matrix are increasingly becoming popular for large-scale problems because they fit well with modern parallel computing architectures.
 We propose a simple measure of concordance between a design matrix and a subset of its rows that estimates how well a subset captures the variance-covariance structure of a larger data set.
 We illustrate the use of this measure in a heuristic method for selecting row partition sizes that balance statistical and computational efficiency goals in real-world problems.
Linguistic summaries of databases provide quick insight in the stored data and are important facilities for understanding and grasping the meaning of large data collections.
 This is especially relevant in the context of big data.
Link prediction is a fundamental task in social networks.
Linked lists play an important role in learning basic Computer Science (CS) concepts among a number of different data structures.
 They are the basis for more complex data structures such as tree data structures.
Linux operating system (LOS) is a sophisticated man-made system and one of the most ubiquitous operating systems.
Linux-based operating systems and runtimes (OS/Rs) have emerged as the environments of choice for the majority of HPC systems.
 While Linux-based OS/Rs have advantages such as extensive feature sets and developer familiarity.
Lipid bilayer membranes have been extensively studied by coarse-grained molecular dynamics simulations.
 Numerical efficiencies have been reported in the cases of aggressive coarse-graining.
Liquids with gas bubbles are commonly seen in medical science.
Live migration of virtual machines (VMs) is useful for resource management of data centers and cloud platforms.
 The precopy algorithm is widely used for memory migration.
Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease.
Load balancing algorithms and task scheduling are one of the most important tasks in parallel application design and implementation.
 Proper task assignment to processor cores can minimize execution time and increase the performance of a parallel application.
 In this paper.
Local binary pattern (LBP) has proved to be an efficient local image descriptor used in computer vision.
 LBP operator has successfully been applied in many biometrics applications.
 Face recognition is one such application where performance degrades due to varying illumination.
Location verification has witnessed significant attention in vehicular communication due to the growth in number of location based intelligent transport system (ITS) applications.
 The traditional cryptography based techniques have been suggested to secure and verify location of vehicles.
 The traditional techniques increase protocol complexity and computational overhead due to the ad hoc nature of vehicular network environments.
 In this context.
Location-allocation modeling is an important area of research in spatial optimization and GIScience.
 A large number of analytical models for location-allocation analysis have been developed in the past 50years to meet the requirements of different planning and spatial-analytic applications.
Lock-free concurrent algorithms guarantee that some concurrent operation will always make progress in a finite number of steps.
 Yet programmers prefer to treat concurrent code as if it were wait-free.
Log messages.
Logging has become a fundamental feature within the modern computer operating systems because of the fact that logging may be used through a variety of applications and fashion.
Logic Programming (LP) is an essential part of many academic curricula and it is extensively employed in the field of Artificial Intelligence.
Logitech made the following statement in 2009: Since the displacements of a mouse would not give any useful information to a hacker.
Long noncoding RNAs (IncRNAs) have been proved to play important roles in cellular processes of cancer.
Long noncoding RNAs (lncRNAs).
Loosely coupled applications composed of a potentially very large number (from tens of thousands to even billions) of tasks are commonly used in high-throughput computing and many-task computing paradigms.
 To efficiently execute large-scale computations which can exceed the capability in a single type of computing resources within expected time.
Lots of evaluations in work life are made under uncertain conditions and conventional databases that do not have effective records; users could not always have the information they need.
Low temperature cracking potential is a primary concern for asphalt mixtures with high RAP contents.
 To minimize the thermal cracking.
Low-dose X-ray computed tomography (LDCT) imaging is highly recommended for use in the clinic because of growing concerns over excessive radiation exposure.
Low-rank representation (LRR) is a useful tool for seeking the lowest rank representation among all the coefficient matrices that represent the images as linear combinations of the basis in the given dictionary.
Luminescence imaging is a versatile characterisation technique used for a broad range of research and industrial applications.
Lump solutions are rationally localized in all directions in the space.
 A general class of lump solutions to the (2 + 1)-dimensional B-Kadomtsev-Petviashvili (BKP) equation is presented through symbolic computation with Maple.
 The Hirota bilinear form of the equation is the starting point in the computation process.
 Like the KP equation.
Lump-type solutions.
Lysine crotonylation on histones is a recently identified post-translational modification that has been demonstrated to associate with active promoters and to directly stimulate transcription.
 Given that crotonyl-CoA is essential for the acyl transfer reaction and it is a metabolic intermediate widely localized within the cell.
Machine learning (ML) is continuously unleashing its power in a wide range of applications.
 It has been pushed to the forefront in recent years partly owing to the advent of big data.
 ML algorithms have never been better promised while challenged by big data.
 Big data enables ML algorithms to uncover more fine-grained patterns and make more timely and accurate predictions than ever before; on the other hand.
Machine learning algorithms applied to text categorization mostly employ the Bag of Words (BoW) representation to describe the content of the documents.
 This method has been successfully used in many applications.
Machine learning algorithms perform differently in settings with varying levels of training set mislabeling noise.
Machine learning is continuing to gain popularity due to its ability to solve problems that are difficult to model using conventional computer programming logic.
 Much of the current and past work has focused on algorithm development.
Machine learning models for site of metabolism (SoM) prediction offer the ability to identify metabolic soft spots in low-molecular-weight drug molecules at low computational cost and enable data-based reactivity prediction.
 SoM prediction is an atom classification problem.
 Successful construction of machine learning models requires atom representations that capture the reactivity-determining features of a potential reaction site.
 We have developed a descriptor scheme that characterizes an atom's steric and electronic environment and its relative location in the molecular structure.
 The partial charge distributions were obtained from fast quantum mechanical calculations.
 We successfully trained machine learning classifiers on curated cytochrome P450 metabolism data.
 The models based on the new atom descriptors showed sustained accuracy for retrospective analyses of metabolism optimization campaigns and lead optimization projects from Bayer Pharmaceuticals.
 The results obtained demonstrate the practicality of quantum-chemistry-supported machine learning models for hit-to-lead optimization.
Machine learning-based computational intelligence methods are widely used to analyze large-scale data sets in this age of big data.
 Extracting useful predictive modeling from these types of data sets is a challenging problem due to their high complexity.
 Analyzing large amount of streaming data that can be leveraged to derive business value is another complex problem to solve.
 With high levels of data availability (i.
Machine vision technologies hold the promise of enabling rapid and accurate fruit crop yield predictions in the field.
 The key to fulfilling this promise is accurate segmentation and detection of fruit in images of tree canopies.
 This paper proposes two new methods for automated counting of fruit in images of mango tree canopies.
Macro-hybrid mixed variational models of two-phase flow.
Macrophage lipoprotein lipase (LPL) induces lipid accumulation and promotes atherosclerosis.
Magur Clarias batrachus is an indigenous catfish.
Maintaining multivariate calibrations involves keeping models developed on an instrument applicable to predicting new samples over time.
Major advances in memory forensics in the past decade now allow investigators to efficiently detect and analyze many types of sophisticated kernel-level malware.
 With operating systems vendors now routinely enforcing driver signing and integrating strategies for protecting kernel data.
Making decisions using judgements of multiple non-deterministic indicators is an important task.
Malaria in human is a serious and fatal tropical disease.
 This disease results from Anopheles mosquitoes that are infected by Plasmodium species.
 The clinical diagnosis of malaria based on the history.
Male infertility is a multifactorial disorder with impressively genetic basis; besides.
Malicious software activities have become more and more clandestine.
Malware and forensic analyses of embedded cyber-physical systems are tedious.
Management of uncertain data in spatial queries has drawn extensive research interests to consider the granularity of devices and noises in the collection and the delivery of data.
 Most previous works usually model and handle uncertain data to find the required results directly.
Managing context-dependent behavior in devices that are constantly on the move is a laborious task.
Managing servers integration to realize distributed data computing framework is an important concern.
 Regardless of the underlying architecture and the actual distributed system's complexity.
MANET (Mobile ad hoc network) is wireless network.
Many agitation and mixing processes utilize various sensors for real-time monitoring and control.
Many applications have been developed or are under development in the areas of environmental monitoring.
Many applications require specialized data structures not found in the standard libraries.
Many applied problems arising in agricultural monitoring and food security require reliable crop maps at national or global scale.
 Large scale crop mapping requires processing andmanagement of large amount of heterogeneous satellite imagery acquired by various sensors that consequently leads to a Big Data problem.
 The main objective of this study is to explore efficiency of using the Google Earth Engine (GEE) platform when classifying multi-temporal satellite imagery with potential to apply the platform for a larger scale (e.
Many approaches have been proposed aiming to reduce the cost of join operations.
 Such join operations represent the key factor of the inquiry process to retrieve related information from different data tables in large relational databases.
Many automated finite state machine (FSM) based test generation algorithms require that a characterising set or a set of harmonised state identifiers is first produced.
 The only previously published algorithms for partial FSMs were brute-force algorithms with exponential worst case time complexity.
 This paper presents polynomial time algorithms and also massively parallel implementations of both the polynomial time algorithms and the brute-force algorithms.
 In the experiments the parallel algorithms scaled better than the sequential algorithms and took much less time.
 Interestingly.
Many big data applications are usually categorized as irregular.
 Irregular problems feature unpredictable and unstructured properties in terms of the program flow.
Many changes occur in the structure and properties of a test material during drying as a consequence of moisture evaporation.
 The use of image acquisition and analysis techniques is proposed here as a potential method of obtaining a better understanding of the different phenomena that can happen during drying.
 Such techniques can quantify the morphological.
Many computer applications require solving difficult problems (NP or PSPACE).
Many computer graphics rendering algorithms and techniques use ray tracing for generation of photo-realistic images.
Many computer vision and medical imaging problems are faced with learning from large-scale datasets.
Many computer vision applications require finding corresponding points between images and using the corresponding points to estimate disparity.
 Today's correspondence finding algorithms primarily use image features or pixel intensities common between image pairs.
 Some 3-D computer vision applications.
Many computer vision problems involve exploring the synthesis and classification models that map images from the observed source space to a target space.
Many consumer electronics firms are adopting an ecosystem-centric approach for supporting third-party applications.
 In an emerging market.
Many conventional computer vision object tracking methods are sensitive to partial occlusion and background clutter.
 This is because the partial occlusion or little background information may exist in the bounding box.
Many existing techniques for reversing data structures in C/C++ binaries are limited to low-level programming constructs.
Many fields of computing such as Deep Packet Inspection (DPI) employ string matching modules (SMM) that search for a given set of positive strings in their input.
 An SMM is expected to produce correct outcomes while scanning the input data at high rates.
 Furthermore the string sets that are searched for are usually large and their sizes increase steadily.
 Bloom Filters (BFs) are hashing data structures which are fast but their false positive results require further processing.
Many graphics and also non-graphics applications need efficient techniques to find the nearest neighbors of a given query point.
 There are two approaches to address this problem: space-partitioning and data partitioning.
 We present a data-partitioning error-controlled strategy for solving the nearest neighbor search (NNS) problem using spatial sorting as the basic building block.
 We improve on the neighborhood grid method by doing an extensive study on novel spatial sorting strategies for bidimensional NNS.
Many hyperspectral image processing algorithms (e.
Many identity-based proxy signature (IBPS) schemes have been proposed.
Many important network functions require online membership lookup against a large set of addresses.
Many institutions are offering online courses.
Many large scale scientific simulations involve the time evolution of systems modelled as Partial Differential Equations (PDEs).
 The sparse grid combination technique (SGCT) is a cost-effective method for solve time-evolving PDEs.
Many large-scale machine learning problems-clustering.
Many large-scale online services use structured storage to persist metadata and sometimes data.
 The structured storage is typically provided by standard database servers such as Microsoft's SQL Server.
 It is important to understand the workloads seen by these servers.
Many maritime accidents have been caused by human-error including such things as inadequate watch keeping and/or mistakes in ship handling.
Many natural phenomena are intuitively represented as spatiotemporal data objects.
Many object instance retrieval systems are typically based on matching of local features.
Many of the students in our classrooms belong to the gamer generation.
 Because of the success of digital games as entertainment products as well as formidable motivators.
Many operations that need to be performed in modern finite element codes can be described as an operation that needs to be done independently on every cell.
Many phonetic phenomena that occur in connected speech are classified as phonetic periphery where anything can happen.
 A well-known convenient way to fix any phonetic phenomenon using certain symbols is transcription.
 The current paper aims at showing the model of predicting allophones by coordinating a number of factors that determine the choice of a particular allophone and visualizing the result changing certain letters into corresponding IPA symbols.
 Free Pascal compiler and Geany editor are used for programming purposes.
 The model is created for American English.
 It is tested for tap and glottal burst.
Many problems in computer aided geometric design and computer graphics can be turned into a root-finding problem of polynomial equations.
 Among various clipping methods.
Many problems.
Many pseudorandom sequence generators and stream ciphers are based on linear feedback shift registers with dynamic feedback (DLFSR) in order to increase the period and linear span of sequences.
 In this paper.
Many real world complex systems naturally map to network data structures instead of geometric spaces because the only available information is the presence or absence of a link between two entities in the system.
 To enable data mining techniques to solve problems in the network domain.
Many real world information can be represented by a graph with a set of nodes interconnected with each other by multiple type of relations called edge layers (e.
Many real-world applications like Video-On-Demand (VOD) and Web servers require prompt responses to access requests.
Many real-world data are naturally represented as tensors.
Many recent applications of computer graphics and human computer interaction have adopted both colour cameras and depth cameras as input devices.
Many recent software engineering papers have examined duplicate issue reports.
Many researchers have highlighted the scarcity of empirical studies that systematically examine the advantages and disadvantages of the use of visualization techniques for software understanding activities.
 Such studies are crucial for gathering and analyzing objective and quantifiable evidence about the usefulness of proposed visualization techniques and tools.
Many resource allocation problems can be formulated as a constrained maximization of a utility function.
 Network Utility Maximization (NUM) applies optimization techniques to achieve decomposition by duality or the primal-dual method.
 Several important problems.
Many resource-constrained embedded devices.
Many reverse engineering techniques for data structures rely on the knowledge of memory allocation routines.
Many senior citizens are keeping in touch with modern times and put modern technologies to good use.
 They attempt to adapt to this era and don't avoid computers or smart devices.
 Such senior citizens often also educate themselves further in ICT.
Many sinusoidal components different from power system frequency in instantaneous power occur in three phase induction motors during faults and unbalanced conditions.
 The amplitude and frequency of these components give an idea about faults and their level.
 Theoretical analysis indicates that side band components that changes with slip around twice of the power system frequency occur during broken rotor bars faults.
 The new detection algorithm that introduced is based on power spectral density changes between 90-110 Hz frequency band in three phase instantaneous power.
 Experimental results have demonstrated that the new algorithm detects the faults more reliable and effective than the other methods.
Many solutions have been developed to convert non-RDF data to RDF.
 A common task during this conversion is applying data manipulation functions to obtain the desired output.
 Depending on the data format of the source to be transformed.
Many studies have focused on supporting orchestral performers.
 In some of these studies.
Many studies on reverse skyline query processing have been done for various services.
 The existing reverse skyline query processing methods are based on dynamic skylines.
 There are no reverse skyline query processing algorithms based on metric spaces for location-based services.
 The existing methods for processing a reverse skyline query have the limitation of service domains and require the high costs of computation to provide various location-based services.
 In this paper.
Many types of servers exists which includes both publicly accessible servers and internal servers such as mail servers.
 colleges and universities are adopting cloud computing.
Many very difficult problems in applied mathematics and other scientific disciplines cannot be solved without powerful computational systems.
Many vocational schools and universities offer lectures on non-stereoscopic 3D computer graphics (3DCG) animation production.
Many women are not in Information Technology careers for multiple reasons.
 We wanted to create a workshop that targeted middle school girls who do not have many opportunities to learn computer programming.
 We decided that having female high school and college students with help teach this workshop would allow them to inspire and better connect with the middle school students.
 For our workshop.
Many-core architectures trade single-thread performance for a larger number of cores.
 Scalable throughput can be attained only by a high degree of parallelism and minimized synchronization.
 Whilst this is achievable for many applications.
Manycore processors are a way to face the always growing demand in digital data processing.
Mapping between ontologies and relational databases is a necessity in realising the Semantic-Web vision.
 Most of the work concerning this topic has either (1) extracted OWL schemas using a limited range of OWL modelling constructs from relational schemas.
Mapping from relational data to Linked Data (RDB2RDF) is an essential prerequisite for evolving the World Wide Web into the Web of Data.
 We propose a methodology to evaluate the quality of such mappings against a set of objective metrics.
 Our methodology.
Mapping the structure of the entropy region of at least four jointly distributed random variables is an important open problem.
 Even partial knowledge about this region has far reaching consequences in other areas in mathematics.
MapReduce Comes from its simplicity to preparing the input data.
MapReduce is a programming system for distributed processing of large-scale data in an efficient and fault tolerant manner on a private.
MapReduce is a promising distributed computing platform for large-scale data processing applications.
 Hadoop MapReduce has been considered as one of the most extensively used open-source implementations of MapReduce frameworks for its flexible customization and convenient usage.
 Despite these advantages.
MapReduce is the most widely used distributed computing framework due to its excellent parallelism and scalability in dealing with large-scale data.
 It is one of the most important research point in distributed computing field to improve the performance of MapReduce application in datacenter network.
 OpenFlow protocol makes it possible to schedule network resource dynamically to provide better link bandwidth for shuffle traffic.
 Current OpenFlow-based scheduling method runs on a single controller.
Marker-less motion capture has seen great progress.
Markov chain Monte Carlo (MCMC) methods have proven to be a very powerful tool for analyzing data of complex structures.
Mass spectrometry (MS) based proteomics have achieved a near- complete proteome coverage in humans and in several other organisms.
Massive MIMO is a promising technology and a strong candidate for future-generation wireless systems.
 This paper describes some aspects of realistic algorithm design of a 128-antenna prototype.
Massive public pressure is arising to achieve data privacy and protection.
 One way to maintain data privacy is to perform efficient data disposal processes.
 This minimizes the chances of data leakage over an extended period and through irresponsible actions.
 That is why adequate data disposal is essential in so many governmental and critical institutions.
 Simply deleting data does not erase it.
Massive XML data are increasingly generated for the representation.
Mass-market head mounted displays (HMDs) are currently attracting a wide interest from consumers because they allow immersive virtual reality (VR) experiences at an affordable cost.
 Flying over a virtual environment is a common application of HMD.
Matching the software industry requirements with the academy is a significant challenge that must be accomplished for the benefit of both sectors.
Material design is an important task in game and animation industry.
 It often requires artist's great efforts to create an elaborate material.
 Although several successful material editing systems already exist.
Matrix blocking is a common method for improving computational efficiency of PageRank.
Matrix computation is considered to be the core of many machine learning and graph algorithm workloads.
 In traditional single-node age.
Matrix computations are both fundamental and ubiquitous in computational science.
Matrix is commonly used in scientific computing and engineering calculation.
 We are not concerned about data itself in data-structures.
Matrix metalloproteinase (MMP)-14 is an important target for cancer treatment due to its critical roles in tumor invasion and metastasis.
 Previous failures of all compound-based broad-spectrum MMP inhibitors in clinical trials suggest that selectivity is the key for a successful therapy.
 With inherent high specificity.
Matrix metalloproteinases (MMPs).
Maxima is a free and open-source computer algebra system that can perform symbolic computations such as solving equations.
Maximum consensus is one of the most popular criteria for robust estimation in computer vision.
 Despite its widespread use.
Maximum distance separable (MDS) matrices are employed to create diffusion layers in block ciphers and hash functions.
 MDS matrices are generated by linear codes to reduce the cost for software or hardware implementations.
MAZE is an extension of the Object-Z specification language supporting the specification and development of multi-agent systems (MAS).
 Following recommendations from the agent-oriented software engineering community.
Maze puzzle usually has only one user.
 This research has adopted network technology to develop a maze game.
McEliece and Goldreich-Goldwasser-Halevi (GGH) cryptosystems are two instances of code and lattice-based cryptosystems whose security are based on the hardness of coding theoretic and lattice problems.
M-Commerce applications in the world have grown exponentially over the years.
 It had set up for mobile users to engage wirelessly of ad hoc network infrastructure in online business irrespective of place or time.
 Providing anonymous.
MDS matrices are of great importance in the design of block ciphers and hash functions.
 MDS matrices are not sparse and have a large description and thus induce costly implementation in software/hardware.
 To overcome this problem.
MDSplus is a data acquisition and analysis system used worldwide predominantly in the fusion research community.
 Development began 29 years ago on the OpenVMS operating system.
 Since that time there have been many new features added and the code has been ported to many different operating systems.
 There have been contributions to the MDSplus development from the fusion community in the way of feature suggestions.
Measurement programs in large software development organizations contain a large number of indicators.
Measuring 3D micro-features is a challenging task that is usually performed using high-cost systems generally based on technologies with narrow working ranges and very accurate control of the sensor positions.
 Well-known image analysis methods.
Measuring or estimating the number of errors in (i.
Measuring the impact of image based web technologies in educational communication is part of a larger theses.
Measuring the proportion of variance explained (R2) by a statistical model and the relative importance of specific predictors (semi-partial R2) can be essential considerations when building a parsimonious statistical model.
 The R2 statistic is a familiar summary of goodnessof- fit for normal linear models and has been extended in various ways to more general models.
 In particular.
Mechanism design is the study of algorithm design where the inputs to the algorithm are controlled by strategic agents.
Median filtering (MF) is a canonical image processing operation truly useful in many practical applications.
 The MF most appealing feature is its resistance to noise and errors in data.
Medical additive manufacturing requires standard tessellation language (STL) models.
 Such models are commonly derived from computed tomography (CT) images using thresholding.
 Threshold selection can be performed manually or automatically.
 The aim of this study was to assess the impact of manual and default threshold selection on the reliability and accuracy of skull STL models using different CT technologies.
 One female and one male human cadaver head were imaged using multi-detector row CT.
Medical cyber-physical systems (MCPS) monitor/control patients' physiologic dynamics with embedded/distributed computing process and wireless/wired communication network.
 MCPS greatly impact the society with high-quality medical services and low-cost ubiquitous healthcare.
 The major component that integrates the physical world with the cyber space is wireless body area network (WBAN) of medical sensors and actuators worn or implanted in a patient.
 The life-critical nature of MCPS mandates safe and effective system design.
 MCPS must operate safely under malicious attacks.
 Authentication ensures that a medical device is what it claims to be and does what it declares to do.
Medical image can provide valuable information for preclinical research.
Medical image visualization.
Medical imaging deals with visualising internal structures of the human body to diagnose and treat diseases.
 Several image processing schemes and algorithms are introduced to overcome the challenges regarding medical image analysis.
 In this paper.
Medical imaging has become an integral part of modern medical technology increasingly.
 The image information provided by multimode imaging technology can complement each other.
 Fusing computed tomography (CT) and magnetic resonance imaging (MRI) images can combine their unique information to diagnose brain diseases by radiation therapy.
 And medical imaging process is easy to produce noise.
Medical ultrasonic imaging has been utilized in a variety of clinical diagnoses for many years.
Medium and large construction projects typically involve multiple structural consultants who use a wide range of structural analysis applications.
 These applications and technologies have inadequate interoperability and there is still a dearth of investigations addressing interoperability issues in the structural engineering domain.
 This paper proposes a novel approach which combines an industry foundation classes (IFC)-based Unified Information Model with a number of algorithms to enhance the interoperability: (a) between architectural and structural models.
MEGASAT is software that enables genotyping of microsatellite loci using next-generation sequencing data.
 Microsatellites are amplified in large multiplexes.
Mega-urbanization presents researchers with a network of densely interwoven problems that completely elude disciplinary boundaries.
 We report on the development of a spatial knowledge management and agent simulation framework that is designed to integrate closely with the process of trans-disciplinary research into the dynamics of complex human-environment systems.
 We argue that our choice of knowledge representation languages facilitates cross-domain collaboration and direct involvement of domain experts without prior experience in computer programming.
 In a run-through application example.
Memory corruption vulnerability is prevalent in software that are written using languages that lack memory safety features.
Memory diagnostics are important to improving the resilience of DRAM main memory.
 As bit cell size reaches physical limits.
Memory-intensive implementations often require access to an external.
Merging computational thinking and an embodiment-centered curriculum.
Mesh parameterization is one of the fundamental operations in computer graphics (CG) and computeraided design (CAD).
 In this paper.
Mesh saliency was introduced and joined the community of computer graphics ten years ago.
Message middleware is an important branch of middleware.
Metadata management is an essential enabling factor for geospatial assets because discovery.
Metaheuristic algorithms (MHs) have a long history that can be traced back to genetic algorithms and evolutionary computing in the 1950s.
 Since February 2008.
Metal transport process in plants is a determinant of quality and quantity of the harvest.
 Although it is among the most important of staple crops.
Metalloproteins bind and utilize metal ions for a variety of biological purposes.
 Due to the ubiquity of metalloprotein involvement throughout these processes across all domains of life.
Metastasis accounts for the high mortality rate associated with colorectal cancer (CRC).
Methicillin-resistant Staphylococcus aureus (MRSA) and vancomycin resistant Enterococcus faecalis (VRE) are notorious pathogenic multidrug resistant (MDR) bacteria in both hospital and community sectors.
Methods for the polynomial eigenvalue problem sometimes need to be followed by an iterative refinement process to improve the accuracy of the computed solutions.
 This can be accomplished by means of a Newton iteration tailored to matrix polynomials.
 The computational cost of this step is usually higher than the cost of computing the initial approximations.
MHD free convection over an inclined plate in a thermally stratified high porous medium in the presence of a magnetic field has been studied.
 The dimensionless momentum and temperature equations have been solved numerically by explicit finite difference technique with the help of a computer programming language Compaq Visual Fortran 6.
 The obtained results of these studies have been discussed for the different values of well known parameters with different time steps.
MHD free convection over an inclined plate in a thermally stratified high porous medium in the presence of a magnetic field has been studied.
 The dimensionless momentum and temperature equations have been solved numerically by explicit finite difference technique with the help of a computer programming language Compaq Visual Fortran 6.
 The obtained results of these studies have been discussed for the different values of well known parameters with different time steps.
Microalgae are one of the most suitable subjects for testing the potentiality of light microscopy and image analysis.
Micro-blog as an important part of the network media has become an important access to information for Netizens.
 Some hot topics have an immeasurable impact on the public opinion formation and dissemination.
Microblogging platforms.
Micro-controllers such as Arduino are widely used by all kinds of makers worldwide.
 Popularity has been driven by Arduino's simplicity of use and the large number of sensors and libraries available to extend the basic capabilities of these controllers.
 The last decade has witnessed a surge of software engineering solutions for the Internet of Things.
Microorganisms are everywhere.
 Recent studies showed that the mixture of microbes or the microbiome on the human body plays important roles in human physiology and diseases.
 Metagenomic sequencing is a key technology for studying microbiomes.
 It produces massive amounts of data in the form of short sequencing reads.
 A single metagenomic sample can contain 107 to 108 reads of about 100-nucleotide (nt) length each in a typical shotgun metagenomic sequencing study.
 They contain rich information about microbiomes and their functions.
MicroRNA-194 (miR-194) is frequently dysregulated in many types of cancer.
MicroRNAs (miRNA) have been implicated in a variety of pathological conditions including infectious diseases.
 Knowledge of the miRNAs affected by poly(I: C).
MicroRNAs (miRNAs) are 20-24 nucleotides long non-coding RNAs known to play important regulatory roles during biotic and abiotic stresses by controlling gene expression.
 Blackgram (Vigna mungo).
MicroRNAs (miRNAs) are a class of endogenous.
MicroRNAs (miRNAs) are known to mediate post-transcriptional gene silencing in the cytoplasm and recent evidence indicates that may also possess nuclear roles in regulating gene expression.
 A previous study showed that miR-138 is involved in the multidrug resistance of leukemia cells through down regulation of the drug efflux pump P-glycoprotein (P-gp).
microRNAs (miRNAs) are small non-coding RNAs that control gene expression posttranscriptionally and are part of the giant non codifying genoma.
 Cumulating data suggest that miRNAs are promising potential biomarkers for many diseases.
MicroRNAs (miRNAs) are strongly implicated in various cancers.
MicroRNAs (miRNAs) have been reported to be critical players in osteosarcoma (OS).
 Among numerous cancer related miRNAs.
MicroRNAs (miRNAs) have been reported to play critical roles in tumor progression including hepatocellular carcinoma (HCC).
MicroRNAs (miRNAs) play key roles in regulating Cd toxicity tolerance in plants.
 Solanum torvum Sw.
 is a typical low Cd-accumulating plant that has a high Cd tolerance.
 Despite its importance.
MicroRNAs (miRs) have emerged as being important in cancer biology.
 miR-191 is a conserved miRNA.
MicroRNAs have emerged as important regulators of osteoclast differentiation in recent years.
Microscopic traffic simulators are important tools for studying transportation systems as they describe the evolution of traffic to the highest level of detail.
 A major challenge to microscopic simulators is the slow simulation speed due to the complexity of traffic models.
 We have developed the Scalable Microscopic Adaptive Road Traffic Simulator (SMARTS).
Microstructures at the scale of tens of microns change the physical properties of objects.
Migration is a key cellular function with important implications in cell physiology.
 Impairment of such function is observed in angiogenesis.
Military Heterogeneous Wireless Sensor Network (MHTWSN) is an emerging technology.
 Due to its extending services.
Mimicking the appearance of the real world is a longstanding goal of computer graphics.
Mindful Gnats is a computer game and App that introduces mindfulness and relaxation skills to young people aged nine years and older.
 In this paper the authors describe their model for using technology to support children with the development of psychological skills.
 This model combines a computer game to introduce and practice psychological skills played in the presence of an adult.
Mineral prospectivity mapping is a classification process because in a given study area.
Mini-batch optimization has proven to be a powerful paradigm for large-scale learning.
Mining important persons is significant to network security.
Mining with big data or big data mining has become an active research area.
 It is very difficult using current methodologies and data mining software tools for a single personal computer to efficiently deal with very large datasets.
 The parallel and cloud computing platforms are considered a better solution for big data mining.
 The concept of parallel computing is based on dividing a large problem into smaller ones and each of them is carried out by one single processor individually.
 In addition.
Mismatch removal is a key step in many computer vision problems.
 In this paper.
Mobile Ad hoc NETwork (MANET) is a type of wireless network consisting of a set of self-configured mobile hosts that can communicate with each other using wireless links without the assistance of any fixed infrastructure.
 This has made possible to create a distributed mobile computing application and has also brought several new challenges in distributed algorithm design.
 Checkpointing is a well explored fault tolerance technique for the wired and cellular mobile networks.
Mobile ad hoc networks (MANETs) are wireless networks that have a wide range of applications because of their dynamic topologies and ease of deployment.
 Owing to the independent and dynamic nature of mobile nodes.
Mobile Ad-hoc Network is a network in which mobile nodes communicate with each other by the help of wireless links and mobile nodes can move randomly in a network without any centralized management system.
 As MANET is a wireless network.
Mobile adhoc network is dynamic in nature and it operates completely in an infrastructure-less environment.
 It discovers the way routes dynamically to reach the destination.
 Securing a dynamic way route.
Mobile agent technology is becoming more popular and has been implemented in many distributed computing domains.
 Several research have been conducted to address its challenges including two most important ones which are agent spawning and agent mobility.
 This paper discusses the issues of mobile agent technology.
Mobile applications are becoming more and more popular with the prevalence of mobile operating systems and mobile Internet.
 Many of them consume services provided by the underlying infrastructure and platforms as a part of their application environmental contexts.
Mobile Cloud Computing (MCC) combines the features of mobile computing.
Mobile cloud computing (MCC) is the state-of-the-art mobile distributed computing model that incorporates multitude of heterogeneous cloud-based resources to augment computational capabilities of the plethora of resource-constraint mobile devices.
Mobile cloud computing uses features to deliver outsourcing data to remotely available mobile devices.
Mobile computing is a new computing technology which is developed after mobile communication and distributed computing technology.
 The analysis and research on the interaction with mobility can improve the system construction.
Mobile devices are widely used for uploading/downloading media files such as audio.
Mobile devices including popular smartphone contributes to efficiency improvement of on-site data processing.
 Mobile environment for real-time data processing needs some additional aspects besides desktop ones.
Mobile Edge Computing enables the deployment of services.
Mobile Healthcare (mHealth) systems use mobile smartphones and portable sensor kits to provide improved and affordable healthcare solutions to underserved communities or to individuals with reduced mobility who need regular monitoring.
 The architectural constraints of such systems provide a variety of computing challenges: the distributed nature of the system; mobility of the persons and devices involved; asynchrony in communication; security.
Mobile hotspots have made the dream of ubiquitous Internet access come true.
Mobile network operators (MNOs) are deploying carrier-grade Wireless Local Area Network (WLAN) as an important complementary system to cellular networks.
 Access network selection (ANS) between cellular and WLAN is an essential component to improve network performance and user quality-of-service (QoS) via controlled loading of these systems.
 In emerging heterogeneous networks characterized by different cell sizes and diverse WLAN deployments.
Mobile robot localization.
Mobile robot networks emerged in the past few years as a promising distributed computing model.
 Existing work in the literature typically ensures the correctness of mobile robot protocols via ad hoc handwritten proofs.
Mobility management has become a core function in internet services and networks as mobile devices have been widely used and their capabilities have dramatically advanced.
 It is expected that proxy mobile IPv6 (PMIPv6).
Mode division multiplexing (MDM) is a promising technology for increasing the aggregate bandwidth of multimode fiber (MMF) in conjunction with wavelength division multiplexing (WDM) in face of the impending capacity crunch in optical fiber networks.
 This paper investigates the effect of radial and azimuthal mode spacings in a 25-channel MDM-WDM system in MMF using a spatial light modulator-controlled VCSEL array for excitation of Laguerre-Gaussian (LG) modes.
 A data rate of 25Gbps is achieved at a central wavelength of 1550.
 The effects of different azimuthal and radial mode spacings of LG modes are analyzed in terms of the channel impulse response.
Model Driven Architecture (MDA) is one of the most prominent trends in software development nowadays.
 This approach has proven to alleviate many problems that usually appear in software development processes.
 In particular.
Model transformations are an important cornerstone of model-driven engineering.
Model transformations constitute the key technology for model-driven software development.
Model verification and validation (V&V) is one of the most important activities in simulation modelling.
 Model validation is especially challenging for Agent-Based Simulation (ABS).
 Techniques that can help to improve V&V in simulation modelling are needed.
 This paper proposes a V&V technique called Test Driven Simulation Modelling (TDSM) which applies techniques from Test-Driven Development in software engineering to simulation modelling.
 The main principle in TDSM is that a unit test for a simulation model has to be specified before the simulation model is implemented.
Model-Based Development has become an industry wide standard paradigm.
 As an open source alternative.
Model-based image analysis is indispensable in medical image processing.
 One key aspect of building statistical shape and appearance models is the determination of one-to-one correspondences in the training data set.
 At the same time.
Model-based treatment planning and exposimetry for high-intensity focused ultrasound requires the numerical simulation of nonlinear ultrasound propagation through heterogeneous and absorbing media.
 This is a computationally demanding problem due to the large distances travelled by the ultrasound waves relative to the wavelength of the highest frequency harmonic.
Model-driven engineering (MDE) and search-based software engineering (SBSE) are both relevant approaches to software engineering.
 MDE aims to raise the level of abstraction in order to cope with the complexity of software systems.
Model-Driven Engineering uses models in various stages of the software engineering.
 To reduce the cost of modelling and production.
Modeling and simulation of gene-regulatory networks (GRNs) has become an important aspect of modern systems biology investigations into mechanisms underlying gene regulation.
 A key task in this area is the automated inference or reverse-engineering of dynamic mechanistic GRN models from gene expression time-course data.
 Besides a lack of suitable data (in particular multi-condition data from the same system).
Modeling is one of the most important activities in engineering activity.
 In the article the authors positions on the model and its classifications is taken.
 Furthermore.
Modeling signal which forms complex values is a common scientific problem.
Modeling the spatial correlation of ground motion residuals.
Modelling of multi-million atomic semiconductor structures is important as it not only predicts properties of physically realizable novel materials.
Models are a useful tool to increase the developer's productivity and satisfaction when performing maintenance tasks.
Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences.
MODerate resolution atmospheric TRANsmission (MODTRAN) is a commercial remote sensing (RS) software package that has been widely used to simulate radiative transfer of electromagnetic radiation through the Earth's atmosphere and the radiation observed by a remote sensor.
Modern botnets such as Zeus and Conficker commonly utilize a technique called domain fluxing or a domain generation algorithm to generate a large number of pseudo-random domain names (PDNs) dynamically for botnet operators to control their bots.
 These botnets are becoming one of the most serious threats to Internet security on a global scale.
 How to prevent their destructive action is one of the most pressing issues of today.
 In this paper.
Modern cars are equipped with both active and passive sensor systems that can detect potential collisions.
 In contrast.
Modern competitive shooting is a strenuous test of the human perceptual and motor systems.
 Like driving a race car or piloting a high performance aircraft.
Modern computer graphics are capable of generating highly photorealistic images.
 Although this can be considered a success for the computer graphics community.
Modern cryptography increasingly employs random numbers generated from physical sources in lieu of conventional software-based pseudorandom numbers.
Modern data collection and analysis pipelines often involve a sophisticated mix of applications written in general purpose and specialized programming languages.
 Many formats commonly used to import and export data between different programs or systems.
Modern digital data production methods.
Modern discrete GPUs have been the processors of choice for accelerating compute-intensive applications.
Modern distributed systems require fast and scalable datastores for efficient processing of huge amounts of information.
 Traditional DBMSs are insufficient for such a purpose.
Modern embedded systems.
Modern experimental setups generate prolonged and intense data streams.
 For example.
Modern GPGPU's have enabled massively parallel computing with programmability that can exploit the highly parallel nature of LDPC decoding.
 Previous works customized the design on a GPGPU towards specific execution attributes of a particular LDPC decoding matrix.
 Supporting different LDPC decoding matrices requires either substantial rework on the current program.
Modern GPUs feature complex memory system designs.
 One GPU may contain many types of memory of different properties.
 The best way to place data in memory is sensitive to many factors (e.
Modern heuristics or metaheuristics are optimization algorithms that have been increasingly used during the last decades to support complex decision-making in a number of fields.
Modern knowledge base systems frequently need to combine a collection of databases in different formats: e.
Modern missile systems use infrared imaging for tracking or target detection algorithms.
 The development and validation processes of these missile systems need high fidelity simulations capable of stimulating the sensors in real-time with infrared image sequences from a synthetic 3D environment.
 The Extensible Multispectral Image Generation Toolset (EMIT) is a modular software library developed at MBDA Germany for the generation of physics-based infrared images in real-time.
 EMIT is able to render radiance images in full 32-bit floating point precision using state of the art computer graphics cards and advanced shader programs.
 An important functionality of an infrared image generation toolset is the simulation of thermal shadows as these may cause matching errors in tracking algorithms.
Modern multicore embedded systems often execute applications that rely heavily on concurrent data structures.
 The selection of efficient concurrent data structure implementations for a specific application is usually a complex and time consuming task.
Modern operating systems (OSs) are expected to be more secure as they integrate robust security measures to ensure that users can perform their daily tasks reliably.
 In this article.
Modern operating systems use hardware support to protect against control-flow hijacking attacks such as code-injection attacks.
Modern operating systems.
Modern parallel computing devices.
Modern parallel computing programming models.
Modern problems of optimization.
Modern radio telescopes are favouring densely packed array layouts with large numbers of antennas (N-A greater than or similar to 1000).
 Since the complexity of traditional correlators scales as O(N-A(2)).
Modern scientific experiments generate vast volumes of data which are hard to keep track of.
 Consequently.
Modern semiconductor detectors allow for charged particle tracking with ever increasing position resolution.
 Due to the reduction of the spatial hit uncertainties.
Modern service providers use virtualization in order to feasibly scale their applications.
 Their approaches exploit virtual machines to master the quality of service requirements.
Modifying flow rule attack posts a huge threat to controllers in network operating system (NOS) for software defined network (SDN) due to its concealment.
 Based on the previously proposed NOS architecture with multiple controllers which introduce heterogeneity and dynamic to defend above attack effectively.
Modular exponentiation and modular multiplications are two fundamental operations in various cryptographic applications.
Modular exponentiation is a time-consuming operation widely used in cryptography.
 Modular multi-exponentiation.
Modular exponentiation is an important operation in public-key cryptography.
 The Common-Multiplicand-Multiplication (CMM) modular exponentiation is an efficient exponentiation algorithm.
 This paper presents a novel method for speeding up the CMM modular exponentiation algorithm based on a Modified Montgomery Modular Multiplication (M4) algorithm.
 The M4 algorithm uses a new multi bit scan-multi bit shift technique by employing a modified encoding algorithm.
 In the M4 algorithm.
Modularity benefits.
Molecular diagnostic testing presents new challenges to information management that are yet to be sufficiently addressed by currently available information systems for the molecular laboratory.
 These challenges relate to unique aspects of molecular genetic testing: molecular test ordering.
Monitoring various hardware and software events for energy consumption is essential for energy management in mobile devices.
More than 20 genetic loci have been associated with risk for Alzheimer's disease (AD).
More than half of all new lung cancer diagnoses are made in patients with locally advanced or metastatic disease.
Morphological attribute profiles are multilevel decompositions of images obtained with a sequence of transformations performed by connected operators.
 They have been extensively employed in performing multiscale and region-based analysis in a large number of applications.
Morphological matrix method and its key legal point of view the main function of the program from portfolio analysis.
Morphological operators are widely used in binary image processing for several purposes.
Morphological profiles (MPs) are efficiently exploited for modelling the geometrical features of structures in a scene.
 They increase the discriminability between different classes.
 The degree of processing of images depends on the geometrical structure and shape of the used structure element (SE) in the transformation.
 Since the geometric structures of an image are not the same in the whole image.
Mosquito flight activity has been studied using a variety of different methodologies.
Most common end users those who are not professional users finds difficult to specify queries against querying structured databases.
 The most commonly used applications for data management and analysis is the spreadsheet.
 Still the spreadsheet computation lacks enough analysis.
 Combining the concepts of spreadsheets and the database system can result into new formation which can be easily handled by the end users.
 The main topic of this article is to offer a fully automated method.
Most complex tasks on the Internet-both malicious and benign-are collectively carried out by clusters of IP addresses.
 We demonstrate that it is often possible to discover such clusters by processing data sets and logs collected at various vantage points in the network.
Most computer systems authenticate users only once at the time of initial login.
Most concurrent data structures being designed today are versions of known sequential data structures.
Most diseases.
Most distributed computing applications require an effective scheduling algorithm to distribute and assign client's tasks running on a set of processors.
 The existing algorithms assumed that the scheduled tasks can be simultaneously and ideally sent and received from the processors without any latent delay.
Most existing methods for solving two-phase flow problems in porous media do not take the physically feasible saturation fractions between 0 and 1 into account.
Most IPv6 security issues are still the same as IPv4; IPv6 has its own unique design characteristics that have additional impact to system and network security.
Most laboratories interested in autophagy use different imaging software for managing and analyzing heterogeneous parameters in immunofluorescence experiments (e.
Most of approaches to salient object detection focused on two-dimensional images.
Most of the business applications on the Internet are dependent on web services for their transactions.
 Distributed denial of service (DDoS) attacks either degrade or completely disrupt web services by sending a flood of packets in the form of legitimate looking requests towards the victim web servers.
 Flash event (FE).
Most of the previously proposed schemes use temporary identities for mobile users to provide unlinkable anonymous authentication for mobile users to the satellite network control center (NCC).
Most of the recent mobile robot researchers focus on obstacle avoidance and path tracking in unknown environment.
 This paper presents a new algorithm using Straight-Line Equation adaptation mechanism that makes Robotino reaching its destination accurately.
Most of the research effort in the area of software analysis is focused on the perspective of the developer (as in software developing company) and the ways how the software development process could be improved.
Most of the users of a BitTorrent community participate in multiple swarms.
Most of the well-known supervised dimensionality reduction methods assume unimodal or Gaussian like-lihoods.
Most parallel efficient global optimization (EGO) algorithms focus only on the parallel architectures for producing multiple updating points.
Most published concurrent data structures which avoid locking do not provide any fairness guarantees.
Most students find computer programming difficult subject.
Most studies on the design of an automated canal pool have produced fairly good results.
Motion estimation is a key building block of image processing pipelines in many different contexts.
Motion segmentation and non-rigid structure from motion are two challenging computer vision problems that have attracted numerous research interests.
 While the previous works handle these two problems separately.
Motivated by applications to relational databases.
Motivated by the large number of vertices that future technologies will put in the front of path-search algorithms.
Motivated by the need to sequentially design experiments for the collection of data in batches or blocks.
Motivation is a very important factor for successful instruction.
 This factor is especially relevant in collaborative learning contexts.
Motorcycles are one of the most dangerous means of transportation.
 Its death toll is higher than in others.
Movement data collected through GPS or other technologies is increasingly common.
Movement primitive segmentation enables long sequences of human movement observation data to be segmented into smaller components.
Moving object tracking is one of the challenging problems in computer vision and it has numerous applications in surveillance system.
Moving objectives detection is the basis for the application of computer vision technology.
MPJ Express is a messaging system that allows application developers to parallelize their compute-intensive sequential Java codes on High Performance Computing clusters and multicore processors.
 In this paper.
Much has been written about optimization instance formats.
 The MPS standard for linear mixed-integer programs is well known and has been around for many years.
 Other extensible formats are available for other optimization categories such as stochastic and nonlinear programming.
Much ink has been spilled regarding the trials and tribulations of adapting formal methods to the needs of software engineering practitioners With the exception of computer scientists with a passion for algorithm design and optimization.
Much of Software Engineering research needs to provide an implementation as proof of -concept.
 Often such implementations are created as exploratory prototypes without polished user interfaces.
Much of the research on bitemporal databases has focused on the modeling of time-related data with either attribute or tuple timestamping.
 While the attribute-timestamping approach attaches bitemporal data to attributes.
Multi-agent-based simulation for artificial stock market (ASM) is an important method in behavioural finance.
 The social network in ASM will influence the coordination and decision making of the intelligent agents.
 To improve the performance of an ASM with evolving social networks in a distributed computing environment.
Multicast routing that meets multiple quality of service constraints is important for supporting multimedia com-munications in the Internet of Things (IoT).
 Existing multicast routing technologies for IoT mainly focus on ad hoc sensor net-working scenarios; thus.
Multicore architectures are becoming the most promising computing platforms thanks to their high performance.
 The soft error rate in multicore systems increases by the trend in the transistor sizes and the reduction of the voltage of the transistors.
 Evaluating the impact of soft errors on parallel applications is critical to understand the fault characteristics and to decide the fault tolerance strategies for the reliable execution.
 In this paper.
Multicore processors have become an integral part of modern large-scale and high-performance parallel and distributed computing systems.
 Unfortunately.
Multi-fractured horizontal wells (MFHWs) are an effective technique for developing shale gas reservoirs.
 After fracturing.
Multigranulation rough set theory is a relatively new mathematical tool for solving complex problems in the multigranulation or distributed circumstances which are characterized by vagueness and uncertainty.
 In this paper.
Multi-label feature selection involves the selection of relevant features from multi-labeled datasets.
Multi-label image classification is a challenging problem in computer vision.
 Motivated by the recent development in image classification performance using Deep Neural Networks.
Multi-label learning draws great interests in many real world applications.
 It is a highly costly task to assign many labels by the oracle for one instance.
Multilevel data structures are common in the social sciences.
Multi-level optimisation divides a problem into sections such that each can be addressed using the most appropriate evaluation and optimisation processes.
 A methodology is proposed to address the design and operation of a building and its energy system.
Multilevel spin toque transfer RAM (STT-RAM) is a suitable storage device for energy-efficient neural network accelerators (NNAs).
Multimedia contents are inherently sensitive signals that must be protected whenever they are outsourced to an untrusted environment.
 This problem becomes a challenge when the untrusted environment must perform some processing on the sensitive signals; a paradigmatic example is Cloud-based signal processing services.
 Approaches based on Secure Signal Processing (SSP) address this challenge by proposing novel mechanisms for signal processing in the encrypted domain and interactive secure protocols to achieve the goal of protecting signals without disclosing the sensitive information they convey.
 This paper presents a novel and comprehensive set of approaches and primitives to efficiently process signals in an encrypted form.
Multi-mode resource and precedence-constrained project scheduling is a well-known challenging real-world optimisation problem.
 An important variant of the problem requires scheduling of activities for multiple projects considering availability of local and global resources while respecting a range of constraints.
 A critical aspect of the benchmarks addressed in this paper is that the primary objective is to minimise the sum of the project completion times.
Multi-peer assessment has often been used by teachers to reduce personal bias and make the assessment more reliable.
 This study reviews the design and development of multi-peer assessment systems that detect and solve two common issues in such systems: non-consensus among group members and personal radicalness in some assessments.
 A multi-peer assessment model is proposed to address these issues.
 The model captures roles.
Multiple assessment directed novelty search (MADNS).
Multiple binomial sums form a large class of multi-indexed sequences.
Multiple instance (MI) learning aims at identifying the underlying concept from collectively labeled data.
 A training sample consists of a set.
Multiple instance learning (MIL) is a framework wherein training examples are provided in form of labeled bags rather than labeled instances.
 For visual object tracking.
Multiple object tracking is a fundamental step for many computer vision applications.
Multiple proteases in a system hydrolyze target substrates.
Multiple scan statistic is usually used by epidemiologist to test the uniformity or clustering of data.
 In this article.
Multiple sclerosis lesions influence the process of image analysis.
Multiprotein bridging factor 1 (MBF1) is a transcriptional co-activator that mediates transcriptional activation by bridging sequence-specific activator like proteins and the TATA-box binding protein (TBP).
 MBF1 has been well-studied in Arabidopsis thaliana.
Multi-proxy signature is a variant of proxy signature.
Multirater (multimethod.
Multi-relational data mining is a rapidly growing area used for mining relational databases.
 While traditional data mining approaches search patterns in a single data table.
Multiscale modeling by means of co-simulation is a powerful tool to address many vital questions in neuroscience.
 It can for example be applied in the study of the process of learning and memory formation in the brain.
 At the same time the co-simulation technique makes it possible to take advantage of interoperability between existing tools and multi-physics models as well as distributed computing.
Multi-server authentication enables the subscribers to enjoy an assortment of services from various service providers based on a single registration from any registration centre.
 Previously.
Multi-task learning employs a shared representation of knowledge for learning several instances of the same problem.
 Multi-step time series problem is one of the most challenging problems for machine learning methods.
 The performance of a prediction model face challenges for higher prediction horizons due to the accumulation of errors.
 Cooperative coevolution employs in a divide and conquer approach for training neural networks and has been very promising for single step ahead time series prediction.
Multi-vent regulators are widely used in Egypt.
 Operation system of gates affects the characteristics of the flow downstream (DS) of multi-vent regulators.
 The current study aimed toward introducing the artificial intelligence technique as a new modeling tool in the prediction of the characteristics of the flow downstream of multi-vent regulators under the management and operating systems of multi-gates.
 Specially Artificial Neural Network (ANN) is utilized in the current study in conjunction with experimental data to predict the relative length of the submerged hydraulic jump (HJ) occured DS of multi-vent regulators under the different cases of gates operation.
 The results show that ANN technique is very successful in simulating the relative submerged hydraulic jump length occurred in stilling basins downstream of multi-vent regulators better than multiple linear regression (MLR).
 (C) 2015 Faculty of Engineering.
Multiversion databases store both current and historical data.
 Rows are typically annotated with timestamps representing the period when the row is/was valid.
 We develop novel techniques to reduce index maintenance in multiversion databases.
Multi-view facial expression recognition is a challenging and active research area in computer vision.
 In this paper.
Multi-view image-based rendering consists in generating a novel view of a scene from a set of source views.
 In general.
Multi-wavelength generation system using an optical spin within the modified add-drop optical filter known as a PANDA ring resonator for molecular transport network security is proposed.
 By using the dark-bright soliton pair control.
MutS alpha is a key component in the mismatch repair (MMR) pathway.
 This protein is responsible for initiating the signaling pathways for DNA repair or cell death.
 Herein we investigate this heterodimer's post-recognition.
Mycobacterium tuberculosis.
Mycobacterium tuberculosis.
N-6-Methyladenosine (m(6)A) is a prevalent modification present in the mRNAs of higher eukaryotes.
 YTH domain family 2 (YTHDF2).
Name-based routing belongs to a routing category different from address-based routing.
Nanoparticles have a wide range of applications in science and technology.
NASA's New Horizons flyby mission of the Pluto-Charon binary system and its four moons provided humanity with its first spacecraft-based look at a large Kuiper Belt Object beyond Triton.
 Excluding this system.
Natural Language Processing (NLP) is a field of computer science.
Natural phenomena simulation attracts a lot of research attention and interest in virtual reality.
 The simulation for liquid has become a research focus in both computer graphics and computational physics.
Natural phenomena simulation has attracted a spurt of research attention and interest in both computer graphics and virtual reality technology domains.
 Water is one of the most common phenomena in real world.
 Acquisition and modeling of different water movements have become one of the important research topics in recent years.
 In this paper.
Natural source electromagnetic methods have the potential to recover rock property distributions from the surface to great depths.
 Unfortunately.
Nature-inspired computing algorithms (NICs in short) inherit a certain length of history tracing back to Genetic Algorithm and Evolutionary Computing in the 50's.
 Since February 2008 by the birth of Firefly Algorithm.
Near isogenic lines (NILs) of sweet sorghum genotype S35 into which individual stay green loci were introgressed.
Neighbour search (NS) is the core of any implementations of smoothed particle hydrodynamics (SPH).
 In this paper.
Nematic liquid crystals (NLCs) have proved to be excellent materials for nonlinear optics and its applications because of their large nonresonant nonlinearity and their extended spectral transparency.
 We demonstrate the exemplary collision scenario of both optical solitons and nematicons induced by the nonlinear refractive index in nematic liquid crystals.
 We invoke Hirota's bilinearization method for two coupled partial differential equations governing the dynamics of self-focusing of a laser light in a nematic liquid crystal system.
 We believe that this type of collision of optical solitons and nematicens reveals the many possibilities of all-optical switching schemes for spatial demultiplexing.
Neospora caninum.
Network and computer systems administrators are facing a serious problem of the big network traffic data analysis.
 It became difficult work of administrators to extract and analysis the abnormal and normal patterns from large amounts of the network traffic data.
Network attacks and cyber-security breaches may be the cause of huge monetary damages in the modern information-based economy; thus.
Network big data is an important driving force for the upgrading of information technology.
Network coding is an important cloud storage technique.
Network control strategies for energy-efficient operation of HetNets need to match the dynamics of spatial and temporal traffic loads and to stabilize the network.
 In this paper.
Network covert channels have become a sophisticated means for transferring hidden information over the network.
 Covert channel-internal control protocols.
Network intrusion attempts have been on the rise recently.
 Researchers have shown an increased interest in assessing the security situation for entire network instead of single asset.
 A considerable amount of assessment models have been designed.
Network Intrusion Detection Systems (NIDS) are deployed to protect computer networks from malicious attacks.
 Proper evaluation of NIDS requires more scrutiny than the evaluation for general network appliances.
 This evaluation is commonly performed by sending pregenerated traffic through the NIDS.
 Unfortunately.
Network Intrusion Detection Systems (NIDSs) have always been designed to enhance and improve the network security issue by detecting.
Network intrusion detection systems (NIDSs) have been developed for over twenty years and have been widely deployed in computer networks to detect a variety of network attacks.
 But one of the major limitations is that these systems would generate a large number of alarms.
Network intrusion detection systems are widely used in present-day public and private networks to successfully detect cyber intrusions.
 In recent times.
Network operators rely on security services to protect their IT infrastructures.
 Different kinds of network security policies are defined globally and distributed among multiple security middleboxes deployed in networks.
Network security in the traditional sense consists of the following four elements: secret.
Network security is becoming more and more important because there is a lot of anomaly traffic information in some websites such as online education.
 In order to detect the anomaly traffic effectively.
Network security is rapidly developing.
Network security management is a big challenge for network administrators due to increasing vulnerabilities.
 Vulnerabilities are the weakness of the network and allow malicious attackers access to resources controlled by an organization.
 To keep networks secure network administrators should be aware of all vulnerabilities through which an attacker can gain access.
 In this paper.
Network security risk forecast is a new research focus in the network security field.
 Though some research has been done on the forecast.
 Research of single intrusion attack incidents.
Network security situation awareness is vital important for network security supervision.
 In order to obtain the network security situation effectively.
Network security situation prediction is of great significance for the use of the Internet.
Network security systems inspect packet payloads for signatures of attacks.
 These systems use regular expression matching at their core.
 Many techniques for implementing regular expression matching at line rate have been proposed.
 Solutions differ in the type of automaton used (i.
Network security technologies have different issues that is important in next generation networks because of the real-time nature of its applications (e.
 VoIP and IPTV).
 The main requirements of these types of applications is to handle the attack situations without quality degradation.
 There are many references for implementation of intrusion detection systems in VoIP infrastructures but there is little effort on intrusion response systems.
 We concentrate on response systems for SIP-based entities and present a cost sensitive response system which considers environmental dynamic conditions.
 We categorize the deployable responses into different groups based on their severity level by considering their side effects.
 We also propose a new quantitative metric for damage cost to compare it with response cost.
 Our proposed decision making process is done based on the comparison of these costs (response and damage costs).
Network security with encryption and decryption technology to complete the application layer.
Network servers and applications commonly use static IP addresses and communication ports.
Network structures and human behaviors are considered as two important factors in virus defense currently.
Network Time Protocol based DDoS attacks saw a lot of popularity throughout 2014.
 This paper shows the characterization and analysis of two large datasets containing packets from NTP based DDoS attacks captured in South Africa.
 Using a series of Python based tools.
Network traffic classification is elementary to network security and management.
 Recent research tends to apply machine learning techniques to flow statistical feature based classification methods.
 The Gaussian Mixture Model (GMM) based on the correlation of flows has exhibited superior classification performance.
 It also has several important advantages.
Network traffic classification is the process of analyzing traffic flows and associating them to different categories of network applications.
 Network traffic classification represents an essential task in the whole chain of network security.
 Some of the most important and widely spread applications of traffic classification are the ability to classify encrypted traffic.
Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems.
 The impact of the loss layer of neural networks.
NeuroGUT is a EU-funded initial training network (ITN) of 14 research projects in neurogastroenterology that have employed an equal number of early-stage researchers.
 Neurogut trainees haveamong other activitiesattended an international conference on irritable bowel syndrome (IBS) in Bologna in 2016 and were asked to critically review and evaluate the current knowledge on IBS for their respective research activities.
Neuropathic pain (NP) is caused by damage to the nervous system.
New Horizons images of Pluto's companion Charon show a variety of terrains that display extensional tectonic features.
New imaging techniques enable visualizing and analyzing a multitude of unknown phenomena in many areas of science at high spatio-temporal resolution.
 The rapidly growing amount of image data.
New vision sensors.
Next-Generation Sequencing combined with bioinformatics is a powerful tool for analyzing the large number of DNA sequences present in the expressed antibody repertoire and these data sets can be used to advance a number of research areas including antibody discovery and engineering.
 The accurate measurement of the immune repertoire sequence composition.
Nightlight is a new desktop application designed to visualize astronomical data.
 A primary motivation for this program is to apply modern user interface design and advancements to the ubiquitous FITS file.
 In a more general sense.
NMR spectroscopy is an indispensably powerful technique for the analysis of biomolecules under ambient conditions.
NMRFx Processor is a new program for the processing of NMR data.
 Written in the Java programming language.
Noise addition is a data distortion technique widely used in data intensive applications.
 For example.
Nondestructive evaluation methods rely on prior knowledge of the expected interaction of ultrasonic waves with defects to inform detection and characterization decisions.
 Wavefield imaging.
Non-intrusive observations are fundamental to monitor river flows and understand water processes in natural systems.
Nonlinear evolution equations form the most fundamental theme in mathematical physics.
 The search for exact solutions of nonlinear equations has been of interest in recent years.
 In this paper.
Nonlinear least-squares regression is a valuable tool for gaining chemical insights into complex systems.
Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition.
 Although these methods perform impressively well.
Nonlinear similarity measures defined in kernel space.
Non-malleable codes.
Non-malleable coding.
Non-negative matrix factorization (NMF) is a very effective method for high dimensional data analysis.
Nonnegative matrix factorization has been widely used in co-clustering tasks which group data points and features simultaneously.
 In recent years.
Non-relational databases have recently been the preferred choice when it comes to dealing with BigData challenges.
Non-rigid video interpolation is a common computer vision task.
 In this paper we present an optical flow approach which adopts a Laplacian Cotangent Mesh constraint to enhance the local smoothness.
 Similar to Li et al.
Nonsense-mediated mRNA decay (NMD) plays an important role in eukaryotic gene expression.
Normal flow depth is an important parameter in design of open channels and analysis of gradually varied flow.
 In open channels with parabolic and rectangular cross-sections.
Norway experienced internet voting in 2011 and 2013 for municipal and parliamentary elections.
NoSQL and especially graph databases are constantly gaining popularity among developers as they promise to deliver superior performance when handling highly interconnected data compared to relational databases.
 Apache Shindig is the reference implementation for OpenSocial with a highly interconnected data model.
NoSQL data stores have emerged in last years as a solution to provide scalability and flexibility in data modelling for operational databases.
 These data stores have proven that they are better suited for some kinds of problems than relational databases.
 In order to scale.
NoSQL databases are designed to address performance and scalability requirements of web based application which cannot be addressed by traditional relational databases.
 Due to their contrast in priorities and architecture to conventional relational databases using SQL.
NoSQL databases are rapidly becoming the customary data platform for big data applications.
 These databases are emerging as a gateway for alternative approaches outside traditional relational databases and are characterized by efficient horizontal scalability.
NoSQL graph databases have been introduced in recent years for dealing with large collections of graph-based data.
 Scientific data and social networks are among the best examples of the dramatic increase of the use of such structures.
 NoSQL repositories allow the management of large amounts of data in order to store and query them.
 Such data are not structured with a predefined schema as relational databases could be.
 They are rather composed by nodes and relationships of a certain type.
 For instance.
NoSQL phenomenon has taken the database and IT application world by storm.
 Growth and penetration of NoSQL applications.
Novelty search is an evolutionary approach which promotes phenotypic diversity in a population.
 Novelty search has been successfully applied to a wide range of domains and a number of variants have been proposed.
 Here we introduce Multiple Assessment Directed Novelty Search (MADNS).
Novice programmers exhibit a repertoire of affective states over time when they are learning computer programming.
 The modeling of frustration is important as it informs on the need for pedagogical intervention of the student who may otherwise lose confidence and interest in the learning.
 In this paper.
Novice programmers have a misconception of what programming is in the early stages of learning programming.
 A Flowchart-based Programming Environment (FPE) is developed in this research with the aim of introducing the early stages of learning programming to clarify matters.
 An attempt is made to introduce the basic programming algorithms prior to surface structure using an automatic text-to-flowchart conversion approach in order to improve students' problem-solving skills.
Novice programmers often struggle with many concepts underlying computer programming.
 The concept of a program variable that is.
Novice students (N=99) participated in a lab study in which they learned the fundamentals of computer programming in Python using a self-paced computerized learning environment involving a 25-min scaffolded learning phase and a 10-min unscaffolded fadeout phase.
 Students provided affect judgments at approximately 100 points (every 15 s) over the course of viewing videos of their faces and computer screens recorded during the learning session.
 The results indicated that engagement.
Now a day with incredible change in social media network like mobile communication and computer.
Now days there is exponential growth in the volume of online users causes variety and volumes of data.
 So the rapid development of internet web technology the big data and big user adopting NoSQL.
 It is a large scale distributed computing and cloud storage services have brought great challenges to the traditional relational database.
 NoSQL databases are very scalable.
Nowadays databases have become the backbone of cloud-based applications.
 Cloud-based applications are used in about every industry today.
 Despite their popularity and wide adoption.
Nowadays environmental protection has become a valuable asset for the entire society.
 The information based society can provide more efficient solutions in its risks mitigation.
 There are already in place complex infrastructures used to monitor and control the natural environment.
Nowadays fully integrated enterprises are being replaced by business networks in which each participant provides others with specialized services.
 As a result.
Nowadays image processing and machine vision fields have become important research topics due to numerous applications in almost every field of science.
 Performance in these fields is critically dependent to the quality of input images.
 In most of the imaging devices.
Nowadays many applications must process events at a very high rate.
 These events are processed on the fly.
Nowadays securing data has become a prime concern for educational institutes.
Nowadays the IT industry is more and more present in several areas like: automotive.
Nowadays the manufacturing is facing the critical challenges from various aspects including the trend of moving towards the new era of Industrial 4.
0 [1]-an analytical and predictive driven production thinking.
Nowadays the most serious security problems are imperfection in the implementations of network protocols.
 This imperfection can bring a lot of vulnerabilities such as could allow malicious user to attack the systems remotely using the network protocols over the internet.
 That is why developers value software security phases involving review of code.
Nowadays there are many proposals that allow users to perform fuzzy queries on relational databases.
 Regardless of these proposals.
Now-related temporal data play an important role in many applications.
 Clifford et al.
's approach is a milestone to model the semantics of 'now' in temporal relational databases.
 Several relational representation models for now-related data have been presented; however.
Now-related temporal data play an important role in the medical context.
 Current relational temporal database (TDB) approaches are limited since (i) they (implicitly) assume that the span of time occurring between the time when facts change in the world and the time when the changes are recorded in the database is exactly known.
NRG-Loops are source-level abstractions that allow an application to dynamically manage its power and energy through adjustments to functionality.
Nucleic acids are responsible for the storage.
Number Plate Recognition identifies vehicle number without human intervention.
 It is a computer vision application and it has many important applications.
 The proposed system consists of two parts: number plate area extraction and character identification.
 In this paper.
Numerical control system is the core technology for flexible manufacturing system and computer integrated manufacturing system.
 In this paper.
Numerical Mathematics and knowledge of elementary computer algorithms are essential parts of any engineering education.
 While our university provided access to computer programming languages and computer algebra systems.
Numerical modeling is a well established tool in rock mechanics studies investigating a wide range of problems.
 Implicit methods for solving linear equations have the advantage of being unconditionally stable.
Numerous problems involving gradient-driven transport processes-e.
Numerous studies have explored the altered transcriptional landscape associated with skin diseases to understand the nature of these disorders.
Numerous tools automating various aspects of software engineering have been developed.
Numerous vulnerabilities threat the security of Mobile IP (MIP) networks.
 A variety of methods have been proposed to protect the MIP networks from the attacks.
NURBS patches have a serious restriction: they are constrained to a strict rectangular topology.
 This means that a request to insert a single new control point will cause a row of control points to appear across the NURBS patch.
Object class segmentation is a computer vision task which requires labeling each pixel of an image with the class of the object it belongs to.
 Deep convolutional neural networks (DNN) are able to learn and take advantage of local spatial correlations required for this task.
Object co-segmentation aims to simultaneously segment common regions of interest from multiple images.
 It is of great importance to image classification.
Object detection and classification have countless applications in human-robot interacting systems.
 It is a necessary skill for autonomous robots that perform tasks in household scenarios.
 Despite the great advances in deep learning and computer vision.
Object detection is one of the most important tasks of computer vision.
 It is usually performed by evaluating a subset of the possible locations of an image.
Object tracking is a well-studied problem in computer vision and has many practical applications.
 The problem and its difficulty depend upon several factors such as the knowledge about the target object.
Objective To establish a three-dimensional (3D) finite element (FE) model of ankylosing spondylitis (AS) kyphosis that is a digital platform for further studies.
 Methods A 30-year-old man with AS kyphosis underwent computed tomography transverse scanning from T1 to the sacrococcyx.
 The images were imported into Mimics (R) 17.
0 software to establish a 3D model of the posterior spine.
Objective visual quality assessment of 3D models is a fundamental issue in computer graphics.
 Quality assessment metrics may allow a wide range of processes to be guided and evaluated.
 Automated behavioral state classification can benefit next generation implantable epilepsy devices.
 In this study we explored the feasibility of automated awake (AW) and slow wave sleep (SWS) classification using wide bandwidth intracranial EEG (iEEG) in patients undergoing evaluation for epilepsy surgery.
 Data from seven patients (age $34\pm 12$ .
 Brain-computer interfaces (BCI) based on event-related potentials (ERP) incorporate a decoder to classify recorded brain signals and subsequently select a control signal that drives a computer application.
 Standard supervised BCI decoders require a tedious calibration procedure prior to every session.
 Several unsupervised classification methods have been proposed that tune the decoder during actual use and as such omit this calibration.
 Each of these methods has its own strengths and weaknesses.
 Our aim is to improve overall accuracy of ERP-based BCIs without calibration.
 We consider two approaches for unsupervised classification of ERP signals.
 Learning from label proportions (LLP) was recently shown to be guaranteed to converge to a supervised decoder when enough data is available.
 In contrast.
 Spike sorting is a fundamental preprocessing step for many neuroscience studies which rely on the analysis of spike trains.
 Most of the feature extraction and dimensionality reduction techniques that have been used for spike sorting give a projection subspace which is not necessarily the most discriminative one.
 The purpose of this study is to evaluate the performance of a natural language processing (NLP) system in classifying a database of free-text knee MRI reports at two separate academic radiology practices.
 MATERIALS AND METHODS.
 An NLP system that uses terms and patterns in manually classified narrative knee MRI reports was constructed.
 The NLP system was trained and tested on expert-classified knee MRI reports from two major health care organizations.
 Radiology reports were modeled in the training set as vectors.
 The purposes of this article are to describe concepts that radiologists should understand to evaluate machine learning projects.
 To compare the clinical outcome of patients presenting with symptoms of uncomplicated cystitis who were seen by a doctor.
Objective: Accurate outcome prediction models for patients with mild traumatic brain injury (MTBI) are key for prognostic assessment and clinical decision-making.
 Using multivariate machine learning.
Objective: An experiment was conducted to determine the relationship between the KAP11.
1 and the regulation wool fineness.
 Methods: In previous work.
Objective: Despite the huge efforts.
Objective: Manual evaluation of machine learning algorithms and selection of a suitable classifier from the list of available candidate classifiers.
OBJECTIVE: P27Kip1 is the one of the negative regulators of the cell cycle that plays an important role in regulating cell cycle and inhibiting cell proliferation by restraining cell in G1 phase.
 P27Kip1 downregulation maybe related to the occurrence of oral squamous cell carcinoma (OSCC).
 It was found that miR-155 significantly upregulated in OSCC tissue.
 Bioinformatics analysis revealed that miR155 may bind with the 3'-UTR of p27Kip1.
 This study investigated the role of miR-155 in regulating p27Kip1 and affecting Tca8113 cell proliferation.
Objective: Phenotyping algorithms are capable of accurately identifying patients with specific phenotypes from within electronic medical records systems.
Objective: The goal of this study was to develop a practical framework for recognizing and disambiguating clinical abbreviations.
Objective: The objective of this study was the identification of the stain HIF-alpha using the Image Cytometry.
Objective: This paper describes a new congestive heart failure (CHF) treatment performance measure information extraction system-CHIEF-developed as part of the Automated Data Acquisition for Heart Failure project.
Objective: To clarify clonality of distinct multisegmental main duct (MD)-intraductal papillary mucinous neoplasms (IPMNs) using microarray analysis.
 Background: IPMNs represent a pancreatic ductal cell field defect.
Objectives Monozygotic twins with near-identical genotypes and discordance for complex diseases represent an exceptional resource to ascertain disease etiology.
 This strategy has been particularly effective with the availability of high-resolution complete individual genome sequencing.
 The challenge is using effective approaches to identify relevant differences that may cause or contribute toward disease discordance.
 Participants and methods This study carried out a VarScan2 bioinformatic analysis and a pathway analysis on whole-genome sequences from two sets of monozygotic twins.
 Results Variants were identified that were present in the affected twin.
Objectives To determine the role of microRNA-15b (miR-15b) in interleukin-1 beta (IL-1 beta)-induced extracellular matrix (ECM) degradation in the nucleus pulposus (NP).
 Results MiR-15b was up-regulated in degenerative NP tissues and in IL-1 beta-stimulated NP cells.
 The purpose of this study was to determine the level of heterogeneity in high grade serous ovarian cancer (HGSOC) by analyzing RNA expression in single epithelial and cancer associated stromal cells.
 In addition.
Objectives/Hypothesis: Although the geometry of the vocal fold medial surface affects voice quality and is critical in the treatment of glottic insufficiency.
Objectives: Conventional material decomposition techniques for dual-energy computed tomography (CT) assume mass or volume conservation.
Objectives: Convolutional neural networks (CNNs) are a subtype of artificial neural network that have shown strong performance in computer vision tasks including image classification.
Objectives: The aim of this work was to construct InverPep.
Objectives: The clinical presentation of pediatric obsessive-compulsive disorder (OCD) is heterogeneous.
Objectives: The impact of wearing lenses on visual and musculoskeletal complaints in VDU workers is currently unknown.
 The goal of this study was 1) to evaluate the impact of wearing VDU lenses on visual fatigue and self-reported neck pain and disability.
Objectives: The proposal and implementation of a computational framework for the quantification of structural renal damage from Tc-99m-dimercaptosuccinic acid (DMSA) scans.
 The aim of this work is to propose.
Objectives: To develop and validate a noninvasive mobility sensor to automatically and continuously detect and measure patient mobility in the ICU.
 Design: Prospective.
Objectives: We describe haemagglutinin (HA) and neuraminidase (NA) sequencing in an apparent cross-site influenza A(H1N1) outbreak in renal transplant and haemodialysis patients.
ObjectiveTo evaluate whether and to which extent skin redness (erythema) affects investigator blinding in transcranial direct current stimulation (tDCS) trials.
 Material and MethodsTwenty-six volunteers received sham and active tDCS.
Objects with various types of mechanical joints are among the most commonly built.
 Joints implement a vocabulary of simple constrained motions (kinematic pairs) that can be used to build more complex behaviors.
 Defining physically correct joint geometry is crucial both for realistic appearance of models during motion.
Occlusion poses as a critical challenge in computer vision for a long time.
Off-highway vehicles have not received the same level of scrutiny that their on-highway counterparts did relative to safety.
Offshore software development outsourcing (OSDO) is a Global Software Engineering paradigm for developing high quality software at low cost in low-wage countries; where client organisation contracts out all or part of software development activities to vendor organisation.
 The vendor organisation provides the agreed software development work in return for remuneration.
 The objective of the research is to identify critical challenges faced by vendor organisations in management and execution of OSDO contract.
On one hand.
On one side the Cloud Computing offers scalable virtual computing resources in the form of web-services on pay-as-you-go basis; but on other side.
On one side.
On the Hilbert space (L) over tilde (2)(T) the singular integral operator with non-Carleman shift and conjugation K = P+ +(aI + AC)P- is considered.
On the Hilbert space the singular integral operator with non-Carleman shift and conjugation is considered.
One factor that seems to influence an individual's effectiveness in requirements engineering activities is her knowledge of the problem being solved.
One key aspect of synthetic biology is the development and characterization of modular biological building blocks that can be assembled to construct integrated cell-based circuits performing computational functions.
One key driver of the Linked Data paradigm is the ability to lift data graphs from legacy systems by employing various adapters and RDFizers (e.
One of major ideas to design a multivariate public key cryptosystem (MPKC) is to generate its quadratic forms by a polynomial map over an extension field.
One of most common aspect with traditional software development is managing requirements.
 As requirements emerge throughout the software development process and thus are needed to be addressed through proper communication and integration between stakeholders.
One of the adverse effects of shrinking transistor sizes is that processors have become increasingly prone to hardware faults.
 At the same time.
One of the backbones of the Indian economy is agriculture.
One of the barriers to entry of computer programming in schools is the lack of tools that support educators in the assessment of student projects.
 In order to amend this situation this paper presents Dr.
One of the biggest challenges to health data sharing is regulations that prohibit the transmission and distribution of Personal Health Information (PHI) even among collaborating organizations.
 This impedes research and reduces the utility of these datasets.
 Anonymization can address this issue by hiding PHI while maintaining the analytical utility of the data.
 Much research has focused on data that is static.
One of the challenges to promoting computer science in schools is to create and retain interest in programming.
 We describe a pilot project for primary and secondary students in Macau to introduce NET Gadgeteer.
One of the challenging issues in a distributed computing system is to reach on a decision with the presence of so many faulty nodes.
 These faulty nodes may update the wrong information.
One of the crucial issues regarding a storm sewer system is the ability to avoid sediment depositions on the pipe invert.
 In this study.
One of the current challenges of persimmon postharvest research is the development of non-destructive methods that allow determination of the internal properties of the fruit.
One of the existing problems of information management is an information security.
 In this aspects one of possible solution is divide information between a group of persons authorized to manage this information.
 Information sharing processes allow to protect the information from disclosure.
 In this paper.
One of the factors that limits the scale.
One of the first steps in numerous computer vision tasks is the extraction of keypoints in images.
 Despite the large number of works proposing image keypoint detectors.
One of the fundamental requirements of real time operating systems is the determinism of executing critical tasks and treating multiple periodic or aperiodic events.
 The present paper presents the hardware support of the nMPRA processor (Multi Pipeline Register Architecture) dedicated to treating time events.
One of the important tasks for most network security solutions is to track network flows in real-time.
 The universe of flow identifiers being huge.
One of the main challenges when working with modern climate model ensembles is the increasingly larger size of the data produced.
One of the main complications caused by diabetes mellitus is the development of diabetic foot.
One of the main features of nodes in mobile ad hoc networks (MANETs) is their cooperation with neighbors to propagate data.
 Misusing this feature.
One of the major challenges in digital forensics today is data encryption.
 Due to the leaked information about unlawful sniffing.
One of the major challenges related to teaching programming and algorithmics to amateur students is the time spent to explain a language's syntax.
One of the methods for obtaining the curve and the surface of complex shape in the engineering geometry.
One of the most challenging aspects in the accurate simulation of three-dimensional soft objects such as vesicles or biological cells is the computation of membrane bending forces.
 The origin of this difficulty stems from the need to numerically evaluate a fourth order derivative on the discretized surface geometry.
 Here we investigate six different algorithms to compute membrane bending forces.
One of the most challenging issues in computer graphics is generation of images reflected on a transparent object.
 The image is generated by light reflection on the surface and also by light refraction through the object.
 In order to generate the image.
One of the most common distortions in images is blur which makes performing lot of image processing and computer vision tasks like detection and recognition a very difficult task.
 Assessment of quality images corrupted blur is very important and active area of research.
 Nonsubsampled contourlet transform is one of the transformation which is shift invariant and used in lot of tasks of image processing like denoising.
One of the most important controversial issues in the design and implementation of software is the functionality of the designed system.
 With impressive efforts of different software teams in the field of the system.
One of the most important parts in developing any kind of software is to check the sustainability of the software which is done by the software testing by the software engineers.
 A varied number of test cases have been taken into account and the test is done by using the Unified Modelling language (UML).
 By using the combination of the hybrid revised genetic algorithm along with the Bee Colony Optimization.
One of the most intriguing objectives when teaching computer science in mid-adolescence high school students is attracting and mainly maintaining their concentration within the limits of the class.
 A number of theories have been proposed and numerous methodologies have been applied.
One of the most significant challenges involved in efforts to understand the effects of repeated earthquake cycle activity is the computational costs of large-scale viscoelastic earthquake cycle models.
 Computationally intensive viscoelastic codes must be evaluated at thousands of times and locations.
One of the most widely used techniques to improve the quality of existing software systems is refactoring-the process of improving the design of existing code by changing its internal structure without altering its external behavior.
 While it is important to suggest refactorings that improve the quality and structure of the system.
One of the promising approaches to the efficient management in a heterogeneous distributed computing environment is the use of multi-agent systems.
 When researchers want to solve tasks in such an environment.
One of the serious challenges in computer vision and image classification is learning an accurate classifier for a new unlabeled image dataset.
One of today's motivating medical image processing problem is registration.
 Medical images acquired from different modalities give rise to a practical problem in image registration.
 For many years.
One serious problem that all the developed nations are facing today is death and injuries due to road accidents.
 The collision of an animal with the vehicle on the highway is one such big issue.
One-class classification belongs to the one of the novel and very promising topics in contemporary machine learning.
 In recent years ensemble approaches have gained significant attention due to increasing robustness to unknown outliers and reducing the complexity of the learning process.
 In our previous works.
One-class classification is a basic problem in machine learning.
 Unlike the existing typical one-class classifiers designed from the angle of probability or geometric.
One-on-one tutoring from a human expert is an effective way for novices to overcome learning barriers in complex domains such as computer programming.
 But there are usually far fewer experts than learners.
 To enable a single expert to help more learners at once.
Online bookstores have highly thrived and changed consumer behaviors in these years.
Online discussion forums are one of the most ubiquitous kinds of resources for people who are learning computer programming.
Online Genetic Improvement embeds the ability to evolve and adapt inside a target software system enabling it to improve at runtime without any external dependencies or human intervention.
 We recently developed a general purpose tool enabling Online Genetic Improvement in software systems running on the java virtual machine.
Online hosts and networks are easy targets of network attacks due to their static nature.
Online Multimedia Social Networks(OSNs) are popular and efficient medium for millions of users.
 Unfortunately.
Online photo sharing is an increasingly popular activity for Internet users.
 More and more users are now constantly sharing their images in various social media.
Online question and answer (Q&A) forums are emerging as excellent learning platforms for learners with varied interests.
 In this paper.
Online Question Answering Systems are very popular and helpful for programming community.
 In these systems.
Online social networks (OSNs) gradually integrate financial capabilities by enabling the usage of real and virtual currency.
 They serve as new platforms to host a variety of business activities.
Online social networks (OSNs) have become more and more popular and have attracted a great many users.
 Friend recommendation.
Online social networks have become so pervasive in people's lives that they can play a crucial role in design and development processes of applications.
Ontology-based data access (OBDA) generalizes query answering in relational databases.
 It allows to query a database by using the language of an ontology.
Open innovation (OI) means that innovation is fostered by using both external and internal influences in the innovation process.
 In software engineering (SE).
Open Learning movement opens up opportunities for the collaboration between institutions.
OpenCL is a portable interface that can be used to program cluster nodes with heterogeneous compute devices.
 The OpenCL specification tightly binds its workflow abstraction.
OpenMOLE is a scientific workflow engine with a strong emphasis on workload distribution.
 Workflows are designed using a high level Domain Specific Language (DSL) built on top of Scala.
 It exposes natural parallelism constructs to easily delegate the workload resulting from a workflow to a wide range of distributed computing environments.
 OpenMOLE hides the complexity of designing complex experiments thanks to its DSL.
 Users can embed their own applications and scale their pipelines from a small prototype running on their desktop computer to a large-scale study harnessing distributed computing infrastructures.
Operating system is essential to operate computers.
Operating system jitter is one of the causes of runtime overhead in high-performance computing applications.
 Many high-performance computing applications perform burst accesses to I/O.
Operating systems interface between hardware and the user.
Optical Character Recognition (OCR) that has been a main research topic of computer vision and artificial intelligence now extend its applications to detection of text area from video or image contents taken by camera devices and retrieval of text information from the area.
 This paper aims to implement a binarization algorithm that removes user intervention and provides robust performance to outdoor lights by using TopHat algorithm and channel transformation technique.
 In this study.
Optical soliton shaping is the process of changing the waveform of transmitted pulses.
Optical time-stretch microscopy has recently attracted intensive attention for its capability of acquiring images at an ultrahigh frame rate.
 Unfortunately.
Optimal estimation of signal amplitude.
Optimal supply of trace elements (TE) is a prerequisite for microbial growth and activity in anaerobic digestion (AD) bioprocesses.
Optimization can be defined as the operation of finding the best solution for a problem.
 This operation is performed by changing the initial parameters using existing data.
 There are various optimization algorithms to solve these kinds of problems; however.
Optimization of power system restoration path is a key issue to the system restoration following a significant disruption.
Optimization processes are an essential element in many practical applications.
Order dependencies (ODs) describe a relationship of order between lists of attributes in a relational table.
 ODs can help to understand the semantics of datasets and the applications producing them.
 They have applications in the field of query optimization by suggesting query rewrites.
Ordinary differential equations (ODEs) are widespread in many natural sciences including chemistry.
Organic-rich mudrock systems play an important role in the world's volatile petroleum industry.
 Due to their complex pore structures with a large portion of nano pores.
Orientation and wayfinding in architectural Immersive Virtual Environments (IVEs) are non-trivial.
Original codes and combinatorial-geometrical computational schemes are presented.
Orthogonal modulation.
Oscillometric measurement is widely used to estimate systolic blood pressure (SBP) and diastolic blood pressure (DBP).
 In this paper.
OS-level virtualization is often used for server consolidation in data centers because of its high efficiency.
Osteoprotegerin (OPG) is implicated in the pathogenesis of postmenopausal osteoporosis.
Our approach permits to capitalize the expert's knowledge as business rules by using an agent-based platform.
 The objective of our approach is to allow experts to manage the daily evolutions of business domains without having to use a technician.
Our goal in this article is to reflect on the role LEGO robotics has played in college engineering education over the last 15 years.
Our keynote article Coactivation in bilingual grammars: A computational account of code mixing (Goldrick.
Our motivation for this study is to provide new information to a researcher based on our field of study: Networking Systems.
 This is because Cyber crime is becoming very widespread and it is imperative to improve measures used to counteract such threats.
 There are various research gaps based on awareness of cyber security.
Our multi-year national research study examines knowledge and perceptions of computer science (CS).
Our new algorithm.
Our paper describes the algorithmic construction of Kovanics basic expectedness distribution formulas and the technique of computation of the gnostical scale parameter with the help of the Matlab parallel computing toolbox.
 Properties of distributions (and their derivatives) are demonstrated also by graphical figuration.
 Some references on applications on data from health and environment are introduced.
Our previous studies have demonstrated that KLF4 is a critical transcription factor that promotes the odontoblastic differentiation of dental papilla cells.
 Klf4 mRNA was found to be regulated by multiple microRNAs (miRNAs).
 Competitive endogenous RNAs (ceRNAs) are a group of transcripts post transcriptionally regulating each other by competing for their common miRNAs.
Our previous works presented experimental investigations of the brittle material removal fraction (BRF) on the machined surface of optical glass.
 It is related to the area fraction of brittle material removed on the machined surface and is used to assess the surface roughness of optical glass during precision grinding.
 In general.
Our process scheduling algorithm was created with the help of circular linked list and skip ring data structures and algorithms.
 Skip ring data structure consists of circular link lists formed in layers which are linked in a canonical way.
 Time complexity of search.
Our research works are in the context of verifying the integrity of access control policies in relational database management systems.
 This paper addresses the following question: In terms of security and particularly access control.
Our society will enter a comprehensive network era.
Our work in this paper stems from our insight that recent research efforts on open vehicle routing (OVR) problems.
Our work is at the crossroads of educational adaptive hypermedia and the competency based approach.
 Adaptive Hypermedia Systems (AHS) are based on a domain model and a learner model to provide a virtual hypermedia to the learner.
 To achieve the interconnection between the domain representation and a virtual hypermedia.
Over a distinguished career.
Over the Internet.
Over the last 50 years a wide variety of automatic network layout algorithms have been developed.
 Some are fast heuristic techniques suitable for networks with hundreds of thousands of nodes while others are multi-stage frameworks for higher-quality layout of smaller networks.
Over the last decade.
Over the last three decades.
Over the past decade.
Over the past decade.
Over the past decades.
Over the past few years.
Over the past ten years.
Over the past years software architecture has become an important sub-field of software engineering.
 There has been substantial advancement in developing new technical approaches to start handling architectural design as an engineering discipline.
 Measurement is an essential part of any engineering discipline.
 Quantifying the quality attributes of the software architecture will reveal good insights about the architecture.
 It will also help architects and practioners to choose the best fit of alternative architectures that meets their needs.
 This work paves the way for researchers to start investigating ways to measure software architecture quality attributes.
 Measurement of these qualities is essential for this sub-field of software engineering.
 This work explores Stability and Understandability of software architecture.
Over the years.
Overhead high-voltage transmission lines (HVTLs) extend over diverse geographic regions under uneven weather conditions.
Overlapping community detection algorithm research is one of hot topics in current social network analysis.
 In this paper.
Owing to the rapid development of earth observation technology.
Owing to the ubiquity of web applications in modern computing.
Owing to their low scalability.
Ownership of a smartphone has never been easier nowadays.
Packet classification is a core function in network and security systems; hence.
Pair-programming is a software development technique that was introduced as part of Extreme Programming.
 In pair-programming.
Papers [1] and [2] focused on governor fundamental speed control basics; the focus of this paper is a fundamental understanding of Automatic Voltage Regulator [3] basics for voltage control and MVAR load sharing.
 Discussions focus on non-complex.
Parallel and distributed computing has led to a proliferation in solving computationally intensive mathematical.
Parallel computing is a useful technology for scientific and engineering algorithms/applications.
 LU-SGS (lower-upper Symmetric-Gauss-Seidel method) is an efficient and robust scheme for CFD (Computational fluid dynamics) and has strong data dependence in its computation.
 In this paper.
Parallel computing is widely utilized to speed up automatic test pattern generation (ATPG); however.
Parallel computing systems promise higher performance for computationally intensive applications.
 Since programmes for parallel systems consist of tasks that can be executed simultaneously.
Parentage data from beef calves has shown that in multiple-sire pastures a disproportionate number of calves are born from a single bull.
 Investigating and accurately quantifying bull behavior within multiple sire pastures will begin to determine reason(s) for the variability in the number of calves sired.
 The study objective was to assess accelerometer data and various classification algorithms to accurately predict bull behavior events in a multiple-sire pasture.
 Behavior events of interest in this study included lying.
Parliamentary websites have become one of the most important windows for citizens and media to follow the activities of their legislatures and to hold parliaments to account.
Partial label learning aims to learn from training examples each associated with a set of candidate labels.
Partial match queries constitute the most basic type of associative queries in multidimensional data structures such as -d trees or quadtrees.
 Given a query where s of the coordinates are specified and are left unspecified ().
Partial reconfiguration (PR) of FPGAs can be used to dynamically extend and adapt the functionality of computing systems by swapping in and out HW tasks.
 To coordinate the on-demand task execution.
Partial wetting surfaces and its influence on the droplet movement of micro and nano scale being contemplated for many useful applications.
 The dynamics of the droplet usually analyzed with a multiphase lattice Boltzmann method (LBM).
 In this paper.
Particle filters (PFs) are widely used for nonlinear signal processing in wireless sensor networks (WSNs).
Particle image velocimetry (PIV) has matured as a flow measurement technique.
 It enables the description of the instantaneous velocity field of the flow by analyzing the particle motion obtained from digitally recorded images.
 Correlation based PIV evaluation technique is widely used because of its good accuracy and robustness.
 Although very successful.
Particle swarm optimization (PSO) uses a social topology for particles to share information among neighbors during optimization.
 A large number of existing literatures have shown that the topology affects the performance of PSO and an optimal topology is problem dependent.
PartitionFinder 2 is a program for automatically selecting best-fit partitioning schemes and models of evolution for phylogenetic analyses.
 PartitionFinder 2 is substantially faster and more efficient than version 1.
Partitioning geometric data into two sets.
Partitioning operating systems (POSs) have been widely applied in safety-critical domains from aerospace to automotive.
 In order to improve the safety and the certification process of POSs.
Past research has proven the significant effects of game-based learning on learning motivation and academic performance.
Patarin proposed a crytographic trapdoor called Hidden Field Equation (HFE).
Path polymorphism is the ability to define functions that can operate uniformly over arbitrary recursively specified data structures.
 Its essence is captured by patterns of the form xy which decompose a compound data structure into its parts.
 Typing these kinds of patterns is challenging since the type of a compound should determine the type of its components.
 We propose a static type system (i.
 no run-time analysis) for a pattern calculus that captures this feature.
 Our solution combines type application.
Pathogenicity islands (PAIs) are mobile integrated genetic elements that contain a diverse range of virulence factors.
 PAIs integrate into the host chromosome at a tRNA locus that contains their specific bacterial attachment site.
PathWhiz is a web server built to facilitate the creation of colorful.
Pattern matching is a fundamental approach to detect malicious behaviors and information over Internet.
PAX3 functions at the nodal point in neural stem cell maintenance and differentiation.
 Using bioinformatics methods.
Pay-as-you-consume.
Pectin degrading enzymes are essential for quality of product from cocoa fermentation.
 Previously.
Pedagogical agents are animated avatars that stimulate cognitive and socio-emotive aspects of instructional activity in virtual learning environment.
 In the context of visual design.
Pedagogues of art face serious problems when it comes to teaching the composition.
 The analysis of pedagogical practice and a survey of schoolchildren and teachers of art schools shows that the skills of composition after graduating the institutions of fine arts are not sufficient for a successful use of the composition of a creative work.
 This problem is partially solved.
Peer learning or.
Pentaprism scanning system has been widely used in the measurement of large flat and wavefront.
People express their opinions about things like products.
Percutaneous image-guided tumor ablation is a minimally invasive surgical procedure for the treatment of malignant tumors using a needle-shaped ablation probe.
 Automating the insertion of a needle by using a robot could increase the accuracy and decrease the execution time of the procedure.
 Extracting the needle tip position from the ultrasound (US) images is of paramount importance for verifying that the needle is not approaching any forbidden regions (e.
Perfect matchings and maximum weight matchings are two fundamental combinatorial structures.
 We consider the ratio between the maximum weight of a perfect matching and the maximum weight of a general matching.
 Motivated by the computer graphics application in triangle meshes.
Performance and determinism are two critical metrics in most embedded systems with real-time requirements.
 Owing to the complexity of current embedded systems.
Performance enhancement is one of the most important issues in high performance distributed computing systems.
 In such computing systems.
Performance modeling for MapReduce applications with large-scale data is a very important issue in the study of optimization.
Performing the Systematic Literature Review (SLR) in the turbulent field of Software Engineering (SE) brings different obstacles and uncertainties.
 The commonly used guidelines for performing the SLR in this field are adapted from health sciences and presented by Kitchenham and Charters in 2007.
 This paper follows the Kitchenham's three-phases-review-process and fulfils it with the findings.
Periploca sepium Bunge is a traditional medicinal plant.
Permanent deformation is known as one of the most critical distresses observed in asphalt pavements.
Permutation polynomials over finite fields play important roles in finite fields theory.
 They also have wide applications in many areas of science and engineering such as coding theory.
Person re-identification receives increasing attentions in computer vision due to its potential applications in video surveillance.
 In order to alleviate wrong matches caused by misalignment or missing features among cameras.
Personalized anticancer therapy requires continuous consolidation of emerging bioinformatics data into meaningful and accurate information streams.
 The use of novel mathematical and physical approaches.
Personalized e-learning environment is desirable in computer programming education.
 An important issue on personalized e-learning environment is to know the learning status of each student.
 This article proposes a method.
Perturbation-iteration method is generalized for systems of first order differential equations.
 Approximate solutions of Lotka-Volterra systems are obtained using the method.
 Comparisons of our results with each other and with numerical solutions are given.
 The method is implemented in Mathematica.
Petri nets with name creation and management (-PNs) have been recently introduced as an expressive model for dynamic (distributed) systems.
Phase analysis techniques of fringe patterns have been widely used for noncontact three-dimensional shape and deformation measurement by the fringe projection method.
Phase measuring profilometry (PMP) has been widely used in many fields.
Phase-shift control can effectively avoid soliton interactions.
 With symbolic computation and Hirota's bilinear method.
PHAT is an open-source C++ library for the computation of persistent homology by matrix reduction.
Phenotyping is important to understand plant biology.
Photometric redshifts (photo-z) are fundamental in galaxy surveys to address different topics.
Photorealistic modeling and rendering of materials with complex internal mesostructure is a hard challenge in Computer Graphics.
 In particular.
Phychography is an important technique in the quantitative phase imaging research domain.
Phylogenetic analysis has achieved extraordinary results in domains like species delimitation and evolutionary biology.
 An essential element behind this success has been the introduction of high performance computing techniques in the step of estimating the phylogenetic likelihoods.
 This paper describes the design and implementation of a distributed and CPU-GPU based heterogeneous computing system on parallelizing the analysis.
 The parallelization has been implemented in the state-of-the-art version of MrBayes.
Physical layer security (PHY-security) takes the advantages of channel randomness nature of transmission media to achieve communication confidentiality and authentication.
 Wiretap coding and signal processing technologies are expected to play vital roles in this new security mechanism.
 PHY-security has attracted a lot of attention due to its unique features and the fact that our daily life relies heavily on wireless communications for sensitive and private information transmissions.
 Compared to conventional cryptography that works to ensure all involved entities to load proper and authenticated cryptographic information.
Physics simulation is an active research topic in games.
Physiological modeling of retina plays a vital role in the development of high-performance image processing methods to produce better visual perception.
 People with normal vision have an ability to discern different colors.
 The situation is different in the case of people with color blindness.
 The aim of this work is to develop a human visual system model for detecting the level of perception of people with red.
Pitched-roof buildings make up a considerable proportion Of architectural roof styles.
 Precise estimation of solar energy potential on pitched roofs is thus crucial to the sustainable development and renewable energy consumption of human habitats.
 Conventional solar radiation measurements usually adopt Light Detection and Ranging (LiDAR) data.
Planar graph navigation is an important problem with significant implications to both point location in geometric data structures and routing in networks.
Planar shape interpolation is a classic problem in computer graphics.
 We present a novel shape interpolation method that blends C-infinity planar harmonic mappings represented in closed-form.
 The intermediate mappings in the blending are guaranteed to be locally injective C-infinity harmonic mappings.
Plant diseases is one of the major bottlenecks in agricultural production that have bad effects on the economic of any country.
 Automatic detection of such disease could minimize these effects.
 Features selection is a usual pre-processing step used for automatic disease detection systems.
 It is an important process for detecting and eliminating noisy.
Plant leaves.
Plantar warts are caused by human papillomaviruses (HPVs) and have been associated with several HPV genotypes.
PLASMA is a numerical library intended as a successor to LAPACK for solving problems in dense linear algebra on multicore processors.
 PLASMA relies on the QUARK scheduler for efficient multithreading of algorithms expressed in a serial fashion.
 QUARK is a superscalar scheduler and implements automatic parallelization by tracking data dependencies and resolving data hazards at runtime.
Plug-in electric vehicles are one of the clean technologies that have many advantages to overcome the problems of power systems for instance voltage unbalance in distribution networks.
 Considering coordinated or uncoordinated charging method.
5 is a major public health concern and some severe diseases have been attributed to exposure to PM2.
Pneumonia is a lower respiratory tract infection that causes dramatic mortality worldwide.
 The present study aimed to investigate the pathogenesis of pneumonia and identify microRNA (miRNA) biomarkers as candidates for targeted therapy.
 RNA from the peripheral blood plasma of participants with pneumonia (severe.
Point alignment is an important topic in computer vision.
 In order to implement cross-stitch embroidery in an automatic way.
Point cloud data are an important source for 3D geoinformation.
 Modern day 3D data acquisition and processing techniques such as airborne laser scanning and multi-beam echosounding generate billions of 3D points for simply an area of few square kilometers.
 With the size of the point clouds exceeding the billion mark for even a small area.
Point cloud registration is a fundamental task in computer graphics.
Point compression is an essential technique to save bandwidth and memory when deploying elliptic curve based security solutions in wireless communication systems.
 In this contribution.
Point vortex models are frequently encountered in conceptual studies in geophysical fluid dynamics.
Pointwise intensity-based algorithms are the most popular algorithms in dynamic laser speckle measurement of physical or biological activity.
 The output of this measurement is a two-dimensional map which qualitatively separates regions of higher or lower activity.
 In the paper.
Policy and path updates are common causes of network instability.
Political redistricting.
Polylogarithms appear in many diverse fields of mathematics.
Polynomials over finite fields play a central role in algorithms for cryptography.
Polytechnic education is a type of higher education in Singapore that focuses on technical and industrial-oriented education.
 It has been observed in the recent student enrolment that there is a significant increase in the number of overseas students enrolling into the polytechnic.
 A rising concern among educators is the potential differences in their approach to learning.
 These differences lead to difficulties in student engagement.
Pool tag scanning is a process commonly used in memory analysis in order to locate kernel object allocations.
Pooling designs are fundamental tools in many applications such as biotechnology and network security.
 Many famous pooling designs have been constructed from mathematical structures by containing relations.
Poor locality as a natural property of graph data structures causes enormous amount of network traffic in large-scale distributed graph processing systems.
Popularization of mobile and personalized services motivates the adoption of learning strategies supported by the principles of ubiquitous computing.
Population growth has made the probability of incidents at large-scale crowd events higher than ever.
 In the past decades.
Pore networks considering variable connectivity and geometrical restrictions among voids of assorted sizes are simulated using an 8-multicore computing system.
 The topology of the resulting networks is visualized in terms of the sizes and connectivity of the pores through color graphics.
 Results allow the calculation of percolation thresholds.
Port scan detection is one of the important topics in network security and has received lots of attention by researchers; however a slow port scan attack can deceive most of the existing IDS.
Port scans are typically at the begin of a chain of events that will lead to the attack and exploitation of a host over a network.
 Since building an effective defense relies on information what kind of threat an organization is facing.
Portable devices are today used in all areas of life thanks to their ease of use as well as their applications with unique features.
 The increase in the number of users.
Portfolio optimization problem is a multi-objective optimization problem.
Position information plays a pivotal role in wireless sensor network (WSN) applications and protocol/algorithm design.
 In recent years.
Positron emission tomography (PET) images are degraded by a phenomenon known as the partial volume effect (PVE).
 Approaches have been developed to reduce PVEs.
Posttraumatic stress disorder (PTSD) is a mental disorder developing after exposure to traumatic events.
 Although psychotherapy reveals some therapeutic effectiveness.
Power electronic circuit (PEC) design and optimization is a significant problem in both scientific and engineering communities.
 Due to the complex search space of the PEC optimization problem.
Power load forecasting is an important part of power grid scheduling.
Power systems are generating masses of data.
Power systems need to have sufficient generation capacity to support the demand at all times.
 In addition.
Precision global health is an approach similar to precision medicine.
Predicting performance of an application running on parallel computing platforms is increasingly becoming important due to the long development time of an application and the high resource management cost of parallel computing platforms.
Predicting protein submitochondrial locations has been studied for about ten years.
 A dozen of methods were developed in this regard.
 Although a mitochondrion has four submitochondrial compartments.
Predicting the grasping function during reach-to-grasp motions is essential for controlling a prosthetic hand or a robotic assistive device.
 An early accurate prediction increases the usability and the comfort of a prosthetic device.
 This work proposes an electromyographic-based learning approach that decodes the grasping intention at an early stage of reach-to-grasp motion.
Preeclampsia presents serious risk of both maternal and fetal morbidity and mortality.
 Biomarkers for the detection of preeclampsia are critical for risk assessment and targeted intervention.
 The goal of this study is to screen potential biomarkers for the diagnosis of preeclampsia and to illuminate the pathogenesis of preeclampsia development based on the differential expression network.
 Two groups of subjects.
Preference learning is the branch of machine learning in charge of inducing preference models from data.
 In this paper we focus on the task known as label ranking problem.
Preferences in the scope of relational databases allow modeling user wishes by queries with soft constraints.
 There are different frameworks for database preferences including commercially available systems.
 They slightly vary in semantics and expressiveness but have in common that preferences induce strict partial orders on a given data set.
 In the present paper we study the expressiveness of preference operators in the available implementations.
 Particularly.
Premise of the study: DNA metabarcoding has broad-ranging applications in ecology.
Presbyacusis.
Presence of joints and fractures in rocks strongly influences the behavior of the rock mass by dividing the media into smaller units.
 These structures intensify the potential instability besides the development of sliding and rotational movements.
 The assumption of discontinuum media changes the whole analysis conditions in relation to the continuum analysis.
 Acquisition of geometrical and structural discontinuity data alongside their mechanical properties is of paramount importance in a rock mass analysis.
 Orientation.
Present article deals with some exact solutions of (2 + 1)-dimensional system of coupled Konopelchenko Dubrovsky equations.
 Similarity transformations method is proposed to seek the solution of the system using Lie group theory.
 The Lie group theory is a very strong tool by which complicated.
Preventing data exfiltration by insiders is a challenging process since insiders are users that have access permissions to the data.
 Existing mechanisms focus on tracking users' activities while they are connected to the database.
Previous studies have demonstrated that matrix factorization techniques.
Previous study has demonstrated that erythrocyte-rich thrombi contain more inflammatory cells and reflect high thrombus burden.
Principles of Computer Programming (PCP) is a compulsory subject for all degree courses offered during first year of study in UniKL MIIT.
 There were seven courses taking the subject as a compulsory subject.
Prior to having available the High Dynamic Range (HDR) techniques.
Privacy is a software quality that is closely related to security.
 The main difference is that security properties aim at the protection of assets that are crucial for the considered system.
Privacy-preserving distributed machine learning becomes increasingly important due to the recent rapid growth of data.
 This paper focuses on a class of regularized empirical risk minimization machine learning problems.
Pro-active message's delay tolerant networks (DTNs) are based on the usage of mobile code to obtain messages that contain their own routing code.
 This architecture allows applications to use the same network in different ways.
 The keystone of this type of heterogeneous network is a collection of contextual and application-related information that it is stored in every node and accessed by the messages' routing code.
 Access to that information must be protected in order to make the whole architecture feasible; the operation of the network has to be secure.
Probabilistic inference over large data sets is a challenging data management problem since exact inference is generally #P-hard and is most often solved approximately with sampling-based methods today.
 This paper proposes an alternative approach for approximate evaluation of conjunctive queries with standard relational databases: In our approach.
Probabilistic relational databases play an important role on uncertain data management.
 Informally.
Problem-Based Learning (PBL) has often been seen as an all-or-nothing approach.
Problems of sustainable development today are identified and tried to solve in all fields of life.
 In this respect information technologies play a specific role.
 Many studies have been devoted to analysis of information technology (ICT) as tools for sustainable development.
 Different contributions and threats posed by ICT to sustainable development are discussed.
Process mining is an emerging discipline whose aim is to discover.
Process model guidance is an important feature by which the software process is orchestrated.
 Without complying with this guidance.
Process-Aware Information Systems (PAIS) support business processes and generate large amounts of event logs from the execution of business processes.
 An event log is represented as a tuple of CaseID.
Processing time has become increasingly a major factor in computed tomography.
Producing software systems that achieve acceptable tradeoffs among multiple non-functional properties remains a significant engineering problem.
 We propose an approach to solving this problem that combines synthesis of spaces of design alternatives from logical specifications and dynamic analysis of each point in the resulting spaces.
 We hypothesize that this approach has potential to help engineers understand important tradeoffs among dynamically measurable properties of system components at meaningful scales within reach of existing synthesis tools.
 To test this hypothesis.
Production decline analysis has been considered as a robust method to obtain the flow parameters.
Production planning models are achieving more interest for being used in the primary sector of the economy.
 The proposed model relies on the formulation of a location model representing a set of farms susceptible of being selected by a grocery shop brand to supply local fresh products under seasonal contracts.
 The main aim is to minimize overall procurement costs and meet future demand.
 This kind of problem is rather common in fresh vegetable supply chains where producers are located in proximity either to processing plants or retailers.
 The proposed two-stage stochastic model determines which suppliers should be selected for production contracts to ensure high quality products and minimal time from farm-to-table.
Programmable Network like SDN allows administrators to program network infrastructure according to service demand and custom-defined policies.
 Network policies are interpreted by the centralized controller to define actions and rules to process the network traffic on devices that belong to a single domain.
Programming for beginners is becoming increasingly more difficult to understand since most innovative tools immediately throw students into an environment where they must build everything in a program from nothing with inconsequential programming skills and little knowledge of programming concepts.
 As programming based careers are rising in demand.
Programming is one of the most important areas in computer science.
 Based on many researches.
Programming-specific Q&A sites (e.
Programs are.
Progress in biomechanical modelling of human soft tissue is the basis for the development of new clinical applications capable of improving the diagnosis and treatment of some diseases (e.
Projection mapping is a projection technology used to turn objects such as buildings into a display surface for video projection.
 In combination with sensing technology and projection mapping technology.
Projection of computer graphics images on walls or architecture.
Proliferation of information is a major confront faced by e-commerce industry.
 To ease the customers from this information proliferation.
Proper protein localization is essential for critical cellular processes.
Proper recovery of electromechanical products can prevent pollution.
Proper scene inference provides the basis for a seamless integration of virtual objects into the real environment.
 While widely neglected in many AR/MR environments.
Properties of granular materials or molecular structures are often studied on a simple geometric model - a set of 3D balls.
 If the balls simultaneously change in size by a constant speed.
Properties of lithium-ion battery electrodes relate to the complex microstructure that develops during solvent removal.
 We use cryogenic scanning electron microscopy in combination with broad ion beam slope-cutting (Cryo-BIB-SEM) for the ex-situ imaging of film formation in battery electrodes.
 Drying of anode films is quenched by cryo-preservation in slushy nitrogen at systematically increasing drying times.
Proposed is a smart single viewing axis optical laser line illumination-based 3-D shape sensor that uses an electronically controlled variable focus lens (ECVFL) for 3-D optical beamforming.
 Specifically.
Protecting control planes in networking hardware from high rate packets is a critical issue for networks under operation.
 One common approach for conventional networking hardware is to offload expensive functions onto hard-wired offload engines as ASICs.
 This approach is inadequate for OpenFlow networks because it restricts a certain amount of flexibility for network control that OpenFlow tries to provide.
Protein phosphorylation has important regulatory functions in cell homeostasis and is tightly regulated by kinases and phosphatases.
 The tegument of human cytomegalovirus (CMV) contains not only several proteins reported to be extensively phosphorylated but also cellular protein phosphatases (PP1 and PP2A).
 To investigate this apparent inconsistency.
Protein secondary structure describe protein construction in terms of regular spatial shapes.
Protein secondary structure prediction is an important problem in bioinformatics and transforming biomedical big data into valuable knowledge is also a quite interesting and challenging task.
 Various machine learning algorithms have been widely applied in bioinformatics to extract knowledge from protein data.
 In recent years.
Protein-Protein Interactions (PPIs) are essential to most biological processes and play a critical role in most cellular functions.
 With the development of high-throughput biological techniques and in si/ico methods.
Proteins are one of the most versatile modular assembling systems in nature.
 Experimentally.
Proteins are the building blocks of cells of living creatures of all kingdoms.
 Due to pivotal role of protein structures in understanding evolutionary relationships and inferring their functions.
Proteins harbor domains or short linear motifs.
Proteomic changes have been described in many neurodegenerative diseases.
Proteomics have passed through a tremendous development in the recent years by the development of ever more sensitive.
Proteomics is the large-scale analysis of proteins.
 Proteomic techniques.
Prototyping plays various roles in software engineering: it can function in an exploratory way in order to gather requirements.
Provide opportunities for educational and scientific development.
Providing adaptive support to users engaged in learning tasks is the central focus of intelligent tutoring systems.
 There is evidence that female and male users may benefit differently from adaptive support.
Providing optimal mechanical ventilation to critically-ill children remains a challenge.
 Patient-ventilator dyssynchrony results frequently with numerous deleterious consequences on patient outcome including increased requirement for sedation.
Providing students opportunities to appreciate interdisciplinary systems remains a challenge for educators at all levels.
 Sensing and data-logging in the environment and in engineered systems offer a unique opportunity for students to explore the connections between engineering processes.
Provision of an uniform query interface facade to access the health-care data present in multiple data-stores being used autonomously by various departments of an hospital.
Psana (Photon Science Analysis) is a software package that is used to analyze data produced by the Linac Coherent Light Source X-ray free-electron laser at the SLAC National Accelerator Laboratory.
 The project began in 2011.
Pseudorandom binary sequences play a crucial role in cryptography.
 The classical approach to pseudorandomness of binary sequences is based on computational complexity.
 This approach has certain weak points thus in the last two decades a new.
Psychiatry research has long experienced a stagnation stemming from a lack of understanding of the neurobiological underpinnings of phenomenologically defined mental disorders.
Public auditing protocol is very significant for implementing secure cloud storage since it can be used to check the integrity of the data stored in the cloud without downloading them.
Public engagement is essential to the procedural and substantive sustainability of environmental assessment.
 Public hearings present the lowest barrier to entry for public participation.
Public key infrastructure (PKI) is the most widely used security mechanism for securing communications over the network.
Public-Key Cryptography (PKC) based on multivariate quadratic equations is one of the most promising alternatives for classical PKC after the eventual coming of quantum computers.
Public-key infrastructure (PKI) is based on public-key certificates and is the most widely used mechanism for trust and key management.
Pulsed thermography.
Purpose - Evacuation drills should be more realistic and interactive.
 Focusing on situational and audio-visual realities and scenario-based interactivity.
Purpose - In recent times.
Purpose - Software product management (SPM) unites disciplines related to product strategy.
Purpose - The current work shows an approach to solve the quality of service (QoS) multicast routing problem by using particle swarm optimization (PSO).
 The problem of finding a route from a source node to multiple destination nodes (multicast) at a minimum cost is an NP-complete problem (Steiner tree problem) and is even greater if QoS constraints are taken into account.
Purpose - The following paper is a Q&A interview conducted by Joanne Pransky of Industrial Robot journal as a method to impart the combined technological.
Purpose - The mathematical complexity of the B-J (x) Brillouin function makes it unsuitable for most calculations and its application difficult for computer programming in magnetism.
Purpose - The purpose of this article is to review and discuss the varied ways computer programming is introduced to schools and families as a new form of learning.
 The paper examines the rhetoric around coding within academic journals and popular media articles over the past three decades.
 This article argues that despite the best intentions of media researchers and enthusiasts.
Purpose - The purpose of this paper is to establish an adaptive assembly.
Purpose - The purpose of this paper is to evaluate the accuracy and functionality of a selection of basic Android and iOS apps for mobile devices designed to generate bibliographic citations.
 Design/methodology/approach - A number of inexpensive or free apps were installed on several different tablets and phones.
 Book citations in MLA and APA format were generated and evaluated for accuracy.
 Findings - Results show that the majority of the apps tested produced unacceptably inaccurate citations.
Purpose - The purpose of this paper is to present a four-level architecture that aims at integrating.
Purpose - The purpose of this paper is to reveal dynamical behavior of nonlinear wave by searching for the new breather soliton and cross two-soliton solutions of the fifth-order Caudrey-Dodd-Gibbon (CDG) equation.
 Design/methodology/approach - The authors apply bilinear form and extended homoclinic test approach to the fifth-order CDG equation.
 Findings - In this paper.
Purpose - The purpose of this paper is to reveal the dynamical behavior of higher dimensional nonlinear wave by searching for the multi-wave solutions to the (3+1)-D Jimbo-Miwa equation.
 Design/methodology/approach - The authors apply bilinear form and extended homoclinic test approach to the (3+1)-D Jimbo-Miwa equation.
 Findings - In this paper.
Purpose - The use of social media and in particular community Question Answering (Q&A) websites by learners has increased significantly in recent years.
 The vast amounts of data posted on these sites provide an opportunity to investigate the topics under discussion and those receiving most attention.
 The purpose of this paper is to automatically analyse the content of a popular computer programming Q&A website.
Purpose - This paper aims to report on the information security behaviors of smartphone users in an affluent economy of the Middle East.
 Design/methodology/approach - A model based on prior research.
Purpose Despite advances that have been made in systemic chemotherapy.
Purpose In this proof-of-concept pilot study.
Purpose of Review Echocardiography is the mainstay in the diagnostic evaluation of constrictive pericarditis (CP) and restrictive cardiomyopathy (RCM).
Purpose of review Seizure prediction has made important advances over the last decade.
 Applying CNGA3 gene augmentation therapy to cure a novel causative mutation underlying achromatopsia (ACHM) in sheep.
 Impaired vision that spontaneously appeared in newborn lambs was characterized by behavioral.
Purpose: Automated segmentation of breast and fibroglandular tissue (FGT) is required for various computer-aided applications of breast MRI.
 Traditional image analysis and computer vision techniques.
Purpose: Ketamine-induced cystitis (KC) among chronic ketamine young abusers has increased dramatically and it has brought attention for Urologists.
 The underlying pathophysiological mechanism(s) of KC is still unclear.
Purpose: Neurovascular interventional procedures using biplane fluoroscopic imaging systems can lead to increased risk of radiation-induced skin injuries.
 The authors developed a biplane dose tracking system (Biplane-DTS) to calculate the cumulative skin dose distribution from the frontal and lateral x-ray tubes and display it in real-time as a color-coded map on a 3D graphic of the patient for immediate feedback to the physician.
 The agreement of the calculated values with the dose measured on phantoms was evaluated.
 Methods: The Biplane-DTS consists of multiple components including 3D graphic models of the imaging system and patient.
Purpose: Sleep disorders affect a great percentage of the population.
 The diagnosis of these disorders is usually made by polysomnography.
 This paper details the development of new software to carry out feature extraction in order to perform robust analysis and classification of sleep events using polysomnographic data.
 The software.
Purpose: Sparsity-promoting regularizers can enable stable recovery of highly undersampled magnetic resonance imaging (MRI).
Purpose: The authors are currently developing a dual-resolution multiple-pinhole microSPECT imaging system based on three large NaI(Tl) gamma cameras.
 Two multiple-pinhole tungsten collimator tubes will be used sequentially for whole-body scout imaging of a mouse.
Purpose: The pretreatment physics plan review is a standard tool for ensuring treatment quality.
 Studies have shown that the majority of errors in radiation oncology originate in treatment planning.
Purpose: This article aims to the evaluation of a prototypal assistive technology for Alzheimer's disease (AD) patients that helps them to remember personal details of familiar people they meet in their daily lives.
 Method: An architecture is proposed for a personal information system powered by face recognition.
Purpose: This paper studies the influence of information and communication technologies on human reasoning and decision making.
 It investigates the potential impact of ambient intelligence on change in pedestrian mobility behavior.
Purpose: To assess the clinical validity of visual field (VF) archetypal analysis.
Purpose: To evaluate image quality and anatomical detail depiction in dose-reduced digital plain chest radiograms using a new needle screen storage phosphor (NIP) in comparison to full dose conventional powder screen storage phosphor (PIP) images.
 Materials and Methods: 24 supine chest radiograms were obtained with PIP at standard dose and compared to follow-up studies of the same patients obtained with NIP with dose reduced to 50% of the PIP dose (all imaging systems: AGFA-Gevaert.
Purpose: We sought to evaluate the low-contrast performance of a newly developed needle image plate/line scanner (NIP) computed radiography system in comparison with a standard powder image plate/flying-spot scanner (PIP) system.
 Materials and Methods: A total of 36 images of a CDRAD phantom.
Purpose-Additive manufacturing (AM) processes are the integration of many different science and engineering-related disciplines.
Quadtrees are a well-known data structure for representing geometric data in the plane.
Quantification of interfacial interaction with randomly rough surface is the prerequisite to quantitatively understand and control the interface behaviors such as adhesion.
Quantitative interpretation of large-scale controlled-source electromagnetic (CSEM) data in the frequency domain requires efficient and stable three-dimensional (3D) forward modeling and inversion codes.
 In this paper.
Quantum computing promises to improve the speed and scalability of computations over that of classical computing hardware.
 At this early stage of quantum computer hardware development.
Quantum correlations (QCs) in some separable states have been proposed as a key resource for certain quantum communication tasks and quantum computational models without entanglement.
 In this paper.
Quantum key distribution (QKD) enables provably secure communication between two parties over an optical fiber that arguably withstands any form of attack.
 Besides the need for a suitable physical signalling scheme and the corresponding devices.
Quantum key distribution (QKD) promises unconditionally secure communications.
Quantum key distribution (QKD) schemes rely on the randomness to exchange secret keys between two parties.
 A control key to generate the same (pseudo)-randomness for the key exchanging parties increases the key exchange rate.
Quantum Key Distribution (QKD) systems exploit the laws of quantum mechanics to generate secure keying material for cryptographic purposes.
Quantum private comparison (QPC) aims to accomplish the equality comparison of secret inputs from two users on the basis of not leaking their contents out.
Quantum secure direct communication (QSDC) is an important branch of quantum cryptography.
 It can transmit secret information directly without establishing a key first.
Quantum secure direct communication can transmit a secret message directly through quantum channels without first generating a shared secret key.
 In the most of the existing protocols.
Quantum-proof randomness extractors are an important building block for classical and quantum cryptography as well as device independent randomness amplification and expansion.
 Furthermore.
Quorum sensing (QS) is a process by which bacteria alter gene expression in response to cell density changes.
 In Vibrio species.
Quorum sensing molecules (QSMs) are involved in the regulation of complicated processes helping bacterial populations respond to changes in their cell-density.
 Although the QS gene cluster (comQXPA) has been identified in the genome sequence of some bacilli.
R2MLwiN is a new package designed to run the multilevel modeling software program MLwiN from within the R environment.
 It allows for a large range of models to be specified which take account of a multilevel structure.
Rabies virus (RABV) is a neurotropic virus that causes serious disease in humans and animals worldwide.
 It has been reported that different RABV strains can result in divergent prognoses in animal model.
 To identify host factors that affect different infection processes.
Railroad tracks need to be periodically inspected and monitored to ensure safe transportation.
 Automated track inspection using computer vision and pattern recognition methods has recently shown the potential to improve safety by allowing for more frequent inspections while reducing human errors.
 Achieving full automation is still very challenging due to the number of different possible failure modes.
Railway similar to the other branches of global economy commonly uses information technologies in its business.
 This includes.
Random number generators are essential in modern cryptography.
 The security of a cryptographic scheme can be achieved under the assumption that the system uses ideal random numbers to produce sensitive security parameters such as encryption keys and initial vectors.
 The weakness of the random number generator makes the entire cryptographic system insecure.
 In particular.
Random numbers are highly used in applications like computer simulation.
Random sequences are widely used in many cryptographic applications and hence their generation is one of the main research areas in cryptography.
 Statistical randomness tests are introduced to detect the weaknesses or nonrandom characteristics that a sequence under consideration may have.
 In the literature.
Ranking items is an essential problem in recommendation systems.
 Since comparing two items is the simplest type of queries in order to measure the relevance of items.
Rapid development of new technologies for information management and knowledge extraction.
Rapid developments have been made in synthetic biology within the past two decades.
Rapid gravity filters.
Rapid transport of water and solutes through desiccation soil cracks can lead to crop water and nutrient stress.
 The challenge of irrigation management of cracking soils is to take advantage of the rapid water intake rate of a dry.
Rapidly developing Next Generation Sequencing technologies produce huge amounts of short reads that consisting randomly fragmented DNA base pair strings.
 Assembling of those short reads poses a challenge on the mapping of reads to a reference genome in terms of both sensitivity and execution time.
 In this paper.
Rare events.
Rationale and Objectives: The process of education involves a variety of repetitious tasks.
 We believe that appropriate computer tools can automate many of these chores.
Rationale: Hypertrophic cardiomyopathy (HCM) is a prototypic single-gene disease caused mainly by mutations in genes encoding sarcomere proteins.
 Despite the remarkable advances.
Raw signal simulation is a useful tool for synthetic aperture radar (SAR) system design.
Ray tracing is a computationally intensive task required by movie-makers to create the highly realistic images they require for motion pictures.
 GPUs currently dominate as hardware accelerators in the multi-billion dollar movie industry.
Rbio is a free software for data processing.
 It is compatible and integrated with the free R software.
Reaction Mechanism Generator (RMG) constructs kinetic models composed of elementary chemical reaction steps using a general understanding of how molecules react.
 Species thermochemistry is estimated through Benson group additivity and reaction rate coefficients are estimated using a database of known rate rules and reaction templates.
 At its core.
Realistic 3D human model reconstruction is an important component in computer graphics and computer vision.
 In particular.
Realistic animation of an expressive human face has been a great challenge in computer graphics due to the human perception of human faces.
 It is a complex.
Realistic rendering of natural scenes captured by digital cameras is the ultimate goal of image processing.
 In recent years.
Realization of Millennium Development Goals should be also reflected in construction industry.
 Sustainable consumption and production patterns can be associated with implementing and developing reverse logistics system on construction site to close the supply chain of construction materials.
 Initiating activities are undertaken already at the construction site.
Realizing long-distance quantum key distribution (QKD) in fiber channel where classical optical communications and quantum signals are multiplexed by their different wavelengths has attracted considerable attentions.
 The achievable secure distance of commonly-used Bennet-Brassard 1984 (BB84) protocol is lowered severely due to inevitable crosstalk from classical optical pulses.
 Unlike conventional quantum key distribution (QKD) protocols.
Real-life utility networks such as smart grids.
Real-time adaptive optics is a technology for enhancing the resolution of ground-based optical telescopes and overcoming the disturbance of atmospheric turbulence.
 The performance of the system is limited by delay errors induced by the servo system and photoelectrons noise of wavefront sensor.
 In order to cut these delay errors.
Real-time and accurate background modeling is an important researching topic in the fields of remote monitoring and video surveillance.
Real-time identification and tracking of the joint positions of people can be achieved with off-the-shelf sensing technologies such as the Microsoft Kinect.
Real-time infrared simulation technology can provide a large number of infrared images under different conditions to support the design.
Real-time rendering of high precision shadows using digital terrain models as input data is a challenging task.
 Especially when interactivity is targeted and level of detail data structures are utilized to tackle huge amount of data.
 In this paper.
Real-time visualization of 3D scenes is a very important feature of many computer graphics solutions.
 In applications such as computer-aided design.
Receiver Strength Signal Indication based Wireless Sensor Networks offer a cheap solution for location-aware applications.
 For a final breakthrough these systems need fast deployment and easy auto-configuration.
 In this study.
Recent advance in technology enables researchers to gather and store enormous data sets with ultra high dimensionality.
 In bioinformatics.
Recent advancements in machine learning algorithms have transformed the data analytics domain and provided innovative solutions to inherently difficult problems.
Recent advances in medical treatment and emergency applications.
Recent approaches to network functions virtualization (NFV) have shown that commodity network stacks and drivers struggle to keep up with increasing hardware speed.
 Despite this.
Recent computing devices execute massive parallel data requiring huge computing hardware.
 To satisfy increasing computing need.
Recent evolution in computer science and web technology result in continually expanding resources on the web that pose unprecedented challenges in Big data to access it with the existing framework.
 In order to access the vastly increasing resource more effectively.
Recent experiments demonstrated that atherosclerosis is a Thl dominant autoimmune condition.
Recent experiments have demonstrated that a fundamental mechanism of the electrospinning process is the rapid whipping of an electrically charged liquid jet.
 This article focuses on the mathematical modeling of such a whipping.
Recent graphic processing units (GPUs) have remarkable raw computing power.
Recent growth in the processing and memory resources of mobile devices has fueled research within the field of mobile virtualization.
 Mobile virtualization enables multiple persona on a single mobile device by hosting heterogeneous operating systems (OSs) concurrently.
Recent increased interest in computational thinking poses an important question to researchers: What are the best ways to teach fundamental computing concepts to students? Visualization is suggested as one way of supporting student learning.
 This mixed-method study aimed to (i) examine the effect of instruction in which students constructed visualizations on students' programming achievement and students' attitudes toward computer programming.
Recent literature has explored automated pornographic detection a bold move to replace humans in the tedious task of moderating online content.
 Unfortunately.
Recent papers highlight the presence of large numbers of compressed angles in metal ion coordination geometries for metalloprotein entries in the worldwide Protein Data Bank.
Recent reports show that there is a 40-200 millisecond delay for sound playback in Android devices.
 At the same time there is no such reports and identified problem for other platforms.
 Research focuses on unresolved audio playback latency problems in devices with Android operating systems.
 Examination of possible problems.
Recent research has investigated the possibility of predicting epileptic seizures.
 Intervention before the onset of seizure manifestations could be envisioned with accurate seizure forecasting.
 Although efforts for better prediction have been made.
Recent research has proposed that GIT2 (G protein-coupled receptor kinase interacting protein 2) acts as an integrator of the aging process through regulation of 'neurometabolic' integrity.
 One of the commonly accepted hallmarks of the aging process is thymic involution.
 At a relatively young age.
Recent research has suggested that improving fine-grained datalocality is one of the main approaches to improving energy efficiency and performance.
Recent research on interactive electronic systems.
Recent smartphone platforms based on new operating systems.
Recent software engineering paradigms such as software product lines.
Recent studies experienced the use of advanced tools for smart aircraft maintenance and inspection.
 These tools often require the use of computer-vision based technologies to recognize and track a given aircraft mechanical part in order to make it possible to show additional information to a technician on a suitable display.
 In this paper we propose a visual recognition module of aircraft mechanical parts that has been included in a prototype system designed for the smart maintenance of the Alenia-Aermacchi M346.
 The evaluation.
Recent studies indicate that transient memory errors (soft errors) have become a relevant source of system failures.
 This paper presents a generic software-based fault-tolerance mechanism that transparently recovers from memory errors in object-oriented program data structures.
 The main benefits are the flexibility to choose from an extensible toolbox of easily pluggable error detection and correction schemes.
Recent studies on brain imaging analysis witnessed the core roles of machine learning techniques in computer-assisted intervention for brain disease diagnosis.
 Of various machine-learning techniques.
Recent studies on remote detection methods were mostly for improving variables like sensing distance.
Recent technological advances have enabled DNA methylation to be assayed at single-cell resolution.
Recent variants of Distributed Denial-of-Service (DDoS) attacks leverage the flexibility of application-layer protocols to disguise malicious activities as normal traffic patterns.
Recent years have seen an increase in the popularity of multivariate pattern (MVP) analysis of functional magnetic resonance (fMRI) data.
Recent years have seen an increasing number of scientists employing data parallel computing frameworks.
Recent years have witnessed a processor development trend that integrates central processing unit (CPU) and graphic processing unit (GPU) into a single chip.
 The integration helps to save some host-device data copying that a discrete GPU usually requires.
Recent years have witnessed a remarkable growth in the way mathematics.
Recent years have witnessed rapid advances in the use of contextual information in ubiquitous and ambient computing.
 Such information improves situated cognition and awareness as well as stakeholders' usage experience.
 While domains such as Web 3.
0 - the next generation of the web - have made context awareness a main requirement of their solution space.
Recent years have witnessed the development of large knowledge bases (KBs).
 Due to the lack of information about the content and schema semantics of KBs.
Recent years have witnessed the increasing emphasis on human aspects in software engineering research and practices.
 Our survey of existing studies on human aspects in software engineering shows that screen-captured videos have been widely used to record developers' behavior and study software engineering practices.
 The screen-captured videos provide direct information about which software tools the developers interact with and which content they access or generate during the task.
 Such Human-Computer Interaction (HCI) data can help researchers and practitioners understand and improve software engineering practices from human perspective.
Recent years.
Recently group-oriented applications over unsecure open networks such as Internet or wireless networks have become very popular.
Recently hashing with multiple tables has become attractive in many real life applications.
Recently scientific communities produce a growing number of computation-intensive applications.
Recently Service Function Chaining (SFC) is promising to innovate the network service mode in modern networks.
Recently sparse representation (SR) based clustering has attracted a growing interests in the field of image processing and pattern recognition.
 Since the SR technology has favorable category distinguishing ability.
Recently the hardware performance of mobile devices have been extremely increased and advanced mobile devices provide multi-cores and high clock speed.
 In addition.
Recently with the rapid development of smart phones and mobile Internet.
Recessive mutations of F13A gene are reported to be responsible of FXIIIA subunit deficiency (FXIIIA).
Recognition of texts in scenes is one of the most important tasks in many computer vision applications.
 Though different scene text recognition techniques have been developed.
Recognizing the need of information-sharing and its implications.
Recombined fingerprints have been suggested as a convenient approach to improve the efficiency of anonymous fingerprinting for the legal distribution of copyrighted multimedia contents in P2P systems.
 The recombination idea is inspired by the principles of mating.
Reconfigurability.
Reconstruction of the point-spread function (PSF) is a critical process in weak lensing measurement.
 We develop a real-data based and galaxy-oriented pipeline to compare the performances of various PSF reconstruction schemes.
 Making use of a large amount of the CFHTLenS data.
Reconstruction of the tridimensional geometry of a visual scene using the binocular disparity information is an important issue in computer vision and mobile robotics.
Recording growth stage information is an important aspect of precision agriculture.
Rectangular dissections are commonly used for tabular forms such as spreadsheets.
Recursions are quite important mathematical tools since many systems are mathematically modelled to ultimately take us to these equations because of their rather easy algebraic natures.
 They fit computer programming needs quite well in many circumstances to produce solutions.
Recursive branch and bound algorithms are often used.
Redirected walking techniques have been introduced to overcome physical space limitations for natural locomotion in virtual reality.
 These techniques decouple real and virtual user trajectories by subtly steering the user away from the boundaries of the physical space while maintaining the illusion that the user follows the intended virtual path.
 Effectiveness of redirection algorithms can significantly improve when a reliable prediction of the users future virtual path is available.
 In current solutions.
Redirection techniques allow users to explore large virtual environments on foot while remaining within a limited physical space.
Reducing latency in distributed computing and data storage systems is gaining increasing importance.
 Several empirical works have reported on the efficacy of scheduling redundant requests in such systems.
Redundancy checking (RC) is a key knowledge reduction technology.
 Extension rule (ER) is a new reasoning method.
Refactoring large systems involves several sources of uncertainty related to the severity levels of code smells to be corrected and the importance of the classes in which the smells are located.
 Both severity and importance of identified refactoring opportunities (e.
 code smells) are difficult to estimate.
Referential integrity is one of the three inherent integrity rules and can be enforced in databases using foreign keys.
Regarding the issue of role-playing games (RPG) and the experiential learning cycle (ELC).
ReGenesees is a new software system for design-based and model-assisted analysis of complex sample surveys.
Regional seismic damage simulation of buildings can potentially reveal possible consequences that are important for disaster mitigation and decision making.
Region-based hierarchical image representation is crucial in many computer vision applications.
Registrations of multi-angle point cloud involve enormous point cloud data.
Regression analysis is a machine learning approach that aims to accurately predict the value of continuous output variables from certain independent input variables.
Regression testing is performed to see if any changes introduced in software will not affect the rest of functional software parts.
 It is inefficient to re-execute all test cases every time the changes are made.
 In this regard test cases are prioritized by following some criteria to perform efficient testing while meeting limited testing resources.
 In our research we have proposed value based particle swarm intelligence algorithm for test case prioritization.
 The aim of our research is to detect maximum faults earlier in testing life cycle.
 We have introduced the combination of six prioritization factors for prioritization.
 These factors are customer priority.
Regression testing is the process of retesting a system after it or its environment has changed.
 Many techniques aim to find the cheapest subset of the regression test suite that achieves full coverage.
 More recently.
Regulatory and technological aspects of cloud technology are showing both opportunities and gaps in the rules on security and accessibility.
 Our proposal aims at addressing a problem that has not yet manifested using a protocol and discussing the normative aspects regarding the possibility of rendering a document completely immaterial.
 Our article proposes a protocol that uses the network in an unconventional way to make a document fully immaterial.
 By immaterial we mean that is not localisable anywhere in its entirety.
 If we continue the analogy to climate.
Reinforcement learning is an appealing approach for allowing robots to learn new tasks.
 Relevant literature reveals a plethora of methods.
Relational Databases (DB) with linguistic data based on hedge algebras (HA) were introduced.
Relational databases (RDB) are the main sources of structured data for government institutions and businesses.
 Since these databases are dependent on autonomous hardware and software they create problems of data integration and interoperability.
 Solutions have been proposed to convert RDB into ontology to enable their sharing.
Relational databases (RDBs) have been widely used as back end for information systems.
 Considering that RDBs have valuable knowledge interwoven in between stored data.
Relational databases are created for the purpose of handling and organizing sensitive data for organizations as well as for individuals.
 Although database security mechanisms and network intrusion detection systems (IDSs) are present.
Relational databases queries are simple and the structured language SQL facilitates this kind of processing.
 SQL is very useful but it does not realize queries if qualitative words are used: expressions as high.
Relational databases remain the leading data storage technology.
 Nevertheless.
Relational learning algorithms mine complex databases for interesting patterns.
Reliably producing software architectures in selected architectural styles requires significant expertise yet remains difficult and error-prone.
 Our research goals are to better understand the nature of style-specific architectures.
Remote data integrity checking (RDIC) enables a data storage server.
Remote sensing algorithms often invert multiple measurements simultaneously to retrieve a group of geophysical parameters.
 In order to create a robust retrieval algorithm.
Remote Sensing of resource in a geographic space at regular temporal intervals has paved way for the evolution of geo-spatial information processing.
 Knowledge engineering of facts acquired through this technology primarily aims at qualitative results to support human in solving complex tasks that cannot be solved through quantitative relational query processing methods with Database Management Systems (DBMS).
 This necessitates the need for automated inference mechanism to be built over relational databases.
 Automated reasoning.
Rendering performance is an everlasting goal of computer graphics and significant driver for advances in both.
Rendering textures in real-time environments is a key task in computer graphics.
 This paper presents a new parallel patch-based method which allows repeatable sampling without cache.
Reparameterization of surfaces is a widely used tool in computer graphics known mostly from the remeshing algorithms.
Reports on the association of TP53 polymorphisms with oral cancer are not only limited but also not specific to site and/or gender.
Representation choice and the development of search operators are crucial aspects of the efficiency of Evolutionary Algorithms (EAs) in combinatorial problems.
 Several researchers have proposed representations and operators for EAs that manipulate spanning trees.
 This paper proposes a new encoding called Node-depth Phylogenetic-based Encoding (NPE).
 NPE represents spanning trees by the relation between nodes and their depths using a relatively simple codification/decodification process.
 The proposed NPE operators are based on methods used for tree rearrangement in phylogenetic tree reconstruction: subtree prune and regraft; and tree bisection and reconstruction.
 NPE and its operators are designed to have high locality.
Representational State Transfer (REST) web services has gained popular acceptance over the world-wide-web as a straightforward choice to the traditional or SOAP-based services.
Representing digital objects with structured meshes that embed a coarse block decomposition is a relevant problem in applications like computer animation.
Requirement prioritization are considered crucial towards the development of successful and high quality software.
Requirements analysis is the software engineering stage that is closest to the users' world.
 It also involves tasks that are knowledge intensive.
Requirements Engineering (RE) includes processes intended to elicit.
Requirements engineering produces specifications of the needs or conditions to meet for a software product.
 These specifications may be vague and ungrounded.
Requirements prioritization is one of the important parts of managing requirements in software development process which plays its role in the success or failure of a software product.
 A software product can go wrong or fail if right requirements are not prioritized at right time.
Research and treatment in the nervous system is challenged by many physiological barriers posing a major hurdle for neurologists.
 The CNS is protected by a formidable blood brain barrier (BBB) which limits surgical.
Research reported in this paper aims to improve the extraction of cucumber leaf spot disease under complex backgrounds.
 An improved fuzzy C-means (FCM) algorithm is proposed in this paper.
Research suggests that worry precludes emotional processing as well as biases attentional processes.
 Although there is burgeoning evidence for the relationship between executive functioning and worry.
Researchers have identified problems with the validity of software engineering research findings.
 In particular.
Researchers in software engineering have attempted to improve software development by mining and analyzing software repositories.
 Since the majority of the software engineering data is unstructured.
Researchers in the field of child neurology are increasingly looking to supplement clinical trials of motor rehabilitation with neuroimaging in order to better understand the relationship between behavioural training.
Reservoir simulations always involve a large number of parameters to characterize the properties of formation and fluid.
Resource allocation and the associated deadlock prevention problem originated in the design and the implementation of the operating systems.
Resource-constrained scheduling problems appear at different levels of decisions in logistics.
Resources in large-scale distributed systems are distributed among several autonomous domains.
 These domains collaborate to produce significantly higher processing capacity through load balancing.
Respondents cannot always explicitly state which numeric value or linguistic term is the most suitable to express their opinions.
Responding to the Association of Southeast Asian Nations (ASEAN) Community Educational Action to teach common ASEAN values in curricula and to develop materials using the United Nations Education for Sustainable Development (UN-ESD).
Responding to the difficulties of implementing meta-heuristics hybridization.
Results of CCD observations of 154 double or multiple stars.
Return-oriented programming (ROP) has been crucial for attackers to evade the security mechanisms of operating systems.
 It is currently used in malicious documents that exploit viewer applications and cause malware infection.
 For inspecting a large number of commonly handled documents.
Reuse in programming language development is an open research problem.
 Many authors have proposed frameworks for modular language development.
 These frameworks focus on maximizing code reuse.
Reuse of software components.
Reverse unknown protocol's hidden behavior has played an important role in the field of network security.
 The proposed work takes the captured messages and the binary code that implement the protocol both as the studied object.
 Dynamic Taint Analysis combined with Static Analysis is used for protocol analyzing.
Reversible circuit synthesis is an important field in quantum computing.
Reversible data hiding (RDH) can extract secret messages and restore the original image without distortion.
 The reversibility benefits many practical applications such as medical image processing and multimedia archive management.
 Because of high image quality.
Reversible logic has gained its importance in the field of low power digital design.
 In any digital system.
Reversible logic has various applications in fields of computer graphics.
Revocation and key evolving paradigms are central issues in cryptography.
RGB-D data has turned out to be a very useful representation of an indoor scene for solving fundamental computer vision problems.
 It takes the advantages of the color image that provides appearance information of an object and also the depth image that is immune to the variations in color.
RGB-D human action recognition is a very active research topic in computer vision and robotics.
 In this paper.
RGB-D sensors have been widely used in various areas of computer vision and graphics.
 A good descriptor will effectively improve the performance of operation.
 This article further analyzes the recognition performance of shape features extracted from multi-modality source data using RGB-D sensors.
 A hybrid shape descriptor is proposed as a representation of objects for recognition.
 We first extracted five 2D shape features from contour-based images and five 3D shape features over point cloud data to capture the global and local shape characteristics of an object.
 The recognition performance was tested for category recognition and instance recognition.
 Experimental results show that the proposed shape descriptor outperforms several common global-to-global shape descriptors and is comparable to some partial-to-global shape descriptors that achieved the best accuracies in category and instance recognition.
 Contribution of partial features and computational complexity were also analyzed.
 The results indicate that the proposed shape features are strong cues for object recognition and can be combined with other features to boost accuracy.
Rheumatoid Arthritis (RA) is a chronic autoimmurie disease that affect joints and muscles.
Rice (Oryza sativa) is one of the most important staple foods for more than half of the global population.
 Many rice traits are quantitative.
Risk analysis of security threats in computer networks is one of the most challenging fields in network management.
 Security risk analysis is usually done by security experts.
 Although they utilize analysis tools such as scanners and analyzers.
RNA binding proteins (RBPs) have been implicated in cancer development.
 An integrated bioinformatics analysis of RBPs (n = 1756) in various datasets (n = 11) revealed several genetic and epigenetically altered events among RBPs in glioblastoma (GBM).
 We identified 13 mutated and 472 differentially regulated RBPs in GBM samples.
 Mutations in AHNAK predicted poor prognosis.
 Copy number variation (CNV).
RNA-Puzzles is a collective experiment in blind 3D RNA structure prediction.
 We report here a third round of RNA-Puzzles.
 Five puzzles.
Road designers assume that drivers will follow the road alignment with trajectories centred in the lane.
Robot recognition tasks usually require multiple homogeneous or heterogeneous sensors which intrinsically generate sequential.
Robot software combines the challenges of general purpose and real-time software.
Robotic writing is a very challenging task and involves complicated kinematic control algorithms and image processing work.
 This paper.
Robust and efficient target-tracking algorithms embedded on moving platforms.
Role of knowledge management and knowledge reuse has been investigated analytically in higher educational environment using Nonaka & Takeuchi and Harsh models.
 It has been observed that in three dimensional environment knowledge management and reuse together play key role for students and faculty both if these could be appropriately exploited.
 A comprehensive system can be built which can benefit both students and faculty in wider areas of their respective knowledge management.
 Special benefits of knowledge reuse may be seen if we could treat knowledge reusability as an independent quantity along with explicit and tacit knowledge.
 Current model reveals analytically that knowledge reusability may boost the operative knowledge in an educational organization and may have its sovereign reality.
 Present work may be also helpful to manage knowledge during software reuse and associated actions.
Rolling stock examination is performed to identify the defects during train movements at speeds < 30 kmph.
 In this study.
Root system analysis is a complex task.
Rotation symmetric Boolean functions have been extensively studied in the recent years because of their applications in cryptography.
 In this study.
Rough set theory provides a powerful tool for dealing with uncertainty in data.
 Application of variety of rough set models to mining data stored in a single table has been widely studied.
Round the globe mobile devices like Smartphone.
Router discovery in IPv6 is vulnerable to rogue Router Advertisements (RAs).
Routing algorithm design for on-chip networks (OCNs) has become increasingly challenging due to high levels of integration and complexity of modern systems-on-chip (SoCs).
 The inherent unreliability of components.
Routing in underwater wireless sensor networks (UWSN) is an important and a challenging activity due to the nature of acoustic channels and to the harsh environment.
 This paper extends our previous work [Al-Salti et al.
 in Proceedings of cyber-enabled distributed computing and knowledge discovery (CyberC).
RSSalg software is a tool for experimenting with Semi-Supervised Learning (SSL).
Runtime analysis of black-box search algorithms provides rigorous performance guarantees.
S(3)Computing requires distributed computing performed by social network platform has high scalability and security.
 Protocol models meeting the requirements of S(3)Computing not only ensure the correctness and robustness of distributed computing.
SAFER is a family of block ciphers.
Safety is a key concern in the design.
Saliency detection has been widely studied to predict human fixations.
Saliency detection is the task of locating informative regions in an image.
Saliency detection is widely used in the fields of computer graphics and multimedia processing.
 Many computer graphics tasks.
Saliency modeling has played an important part in computer vision studies over the past 30 years.
 Many state-of-the-art models adopted complex mathematical and machine learning theories.
 In this paper.
Salient object detection is an important issue in computer vision and image procession in that it can facilitate humans to locate conspicuous visual regions in complex scenes rapidly and improve the performance of object detection and video tracking.
 In recent years.
Salient object detection.
Salmon gelatin and boldine as a natural antioxidant were used to prepare edible films by a cold casting method.
 The concentration of each component was optimised by applying a Box-Behnken experimental design (BBD) with the goal of maximising radical scavenging capacity of film forming suspensions (FFS) measured by the 2.
Sample Adaptive Offset (SAO) has been adopted as a new in-loop filtering block in High Efficiency Video Coding (HEVC).
 It can significantly increase compression efficiency especially for sequences that contain computer graphics content up to 23%.
 To get the optimum SAO parameters.
Satsuma myomphala is critically endangered through loss of natural habitats.
Saving power becomes one of the main objectives in information technology industry and research.
 Companies consume a lot of money in the shape of power consuming.
 Virtual Desktop Infrastructure (VDI) is a new shape of delivering operating systems remotely.
 Operating systems are executing in a cloud data center.
 Users desktops and applications can be accessed by using thin client devices.
 Thin client device is consisting of screen attached with small CPU.
 VDI has benefits in terms of cost reduction and energy saving.
 In this paper.
S-box plays an imperative role in designing a cryptographically strong block cipher.
 Designing S-box based on chaos has attracted lots of attentions because of its distinct characteristics relevant to cryptography.
 In this paper.
Scene text information extraction plays an important role in many computer vision applications.
 Most features in existing text extraction algorithms are only applicable to one text extraction stage (text detection or recognition).
Scheduling for generation units in modern power markets is one of the most important tasks of independent system operators (ISO).
 To ensure secure and economical operation of power systems.
Scheduling multiple parallel workflows.
Schizophrenia research is plagued by enormous challenges in integrating and analyzing large datasets and difficulties developing formal theories related to the etiology.
Schlieren and shadowgraph techniques are used around the world for imaging and measuring phenomena in transparent media.
 These optical methods originated long ago in parallel with telescopes and microscopes.
Schwann cells (SCs) proliferation is crucial for nerve regeneration following nerve injury.
 This study aims to investigate effects of interleukin-22 (IL-22) on SCs proliferation in vitro.
Science computing platforms are changing from traditional on-premises computing platforms to clouds.
 The style of research publications is also changing with the movement of open science.
 For example.
Science Gateways provide scientists with tools for creating.
Scientific software engineering is a distinct discipline from both computational chemistry project support and research informatics.
 A scientific software engineer not only has a deep understanding of the science of drug discovery but also the desire.
Scientific workflows orchestrate the execution of complex experiments frequently using distributed computing platforms.
 Meta-workflows represent an emerging type of such workflows which aim to reuse existing workflows from potentially different workflow systems to achieve more complex and experimentation minimizing workflow design and testing efforts.
 Workflow interoperability plays a profound role in achieving this objective.
 This paper is focused at fostering interoperability across meta-workflows that combine workflows of different workflow systems from diverse scientific domains.
 This is achieved by formalizing definitions of meta-workflow and its different types to standardize their data structures used to describe workflows to be published and shared via public repositories.
 The paper also includes thorough formalization of two workflow interoperability approaches based on this formal description: the coarse-grained and fine-grained workflow interoperability approach.
 The paper presents a case study from Astrophysics which successfully demonstrates the use of the concepts of meta-workflows and workflow interoperability within a scientific simulation platform.
Scissor structure is used to generate deployable objects for space-saving in a variety of applications.
Screening alcohol use disorder (AUD) patients has been challenging due to the subjectivity involved in the process.
SDN as a network architecture emerged on top of existing technologies and knowledge.
 Through defining the controller as a software program.
Search-based software engineering (SBSE) solutions are still not scalable enough to handle high-dimensional objectives space.
 The majority of existing work treats software engineering problems from a single or bi-objective point of view.
Search-based structured prediction methods have shown promising successes in both computer vision and natural language processing recently.
Search-based techniques have been applied successfully to the task of generating unit tests for object-oriented software.
Searching for new efficient algorithms to solve complex optimization problems in big data scenarios is a priority.
Searching is a necessary tool for managing and navigating the massive amounts of data available in today's information age.
 While new searching methods have become increasingly popular and reliable in recent years.
Secret sharing schemes divide a secret among multiple participants so that only authorized subsets of parties can reconstruct it.
 We show that secretly embedded trapdoor with universal protection attack can be embedded in secret sharing schemes that employ enough randomness to give the attacker an overwhelming advantage to access the secret.
 In case of ideal schemes.
Secreted polysaccharides are important functional and structural components of bacterial biofilms.
 The opportunistic pathogen Pseudomonas aeruginosa produces the cationic exopolysaccharide Pel.
Secure communication protocols are becoming increasingly important.
Secure data aggregation (SDA) schemes are widely used in distributed applications.
Secure summation is one of the most applicable functions of secure multiparty computation (MPC) in which a group of users securely computes the summation value of their private inputs.
 The current solutions to this problem are basically on adding a random number to private inputs or splitting the inputs among users which need secure channel among members.
Securing the networks of large organizations is technically challenging due to the complex configurations and constraints.
 Managing these networks requires rigorous and comprehensive analysis tools.
 A network administrator needs to identify vulnerable configurations.
Security analysis and attack-defense modeling are effective method to identify the vulnerabilities of information systems for proactive defense.
 The attack graph model reflects only attack actions and system state changes.
Security and privacy issues are magnified by velocity.
Security and privacy of data are one of the prime concerns in today's Internet of Things (IoT).
 Conventional security techniques like signature-based detection of malware and regular updates of a signature database are not feasible solutions as they cannot secure such systems effectively.
Security features such as privacy and device authentication are required in wireless sensor networks.
Security in terms of Networks have turn out to be more significant to Organizations.
Security is a major concern in all computing environments.
 One way to achieve security is to deploy a secure operating system (OS).
 A trusted OS can actually secure all the resources and can resist the vulnerabilities and attacks effectively.
 In this paper.
Security is an important aspect of an information system in terms of its confidentiality.
Security is an important problem in wireless sensor networks.
 Intrusion detection system is one of the most common methods of network security.
Security is essential for wide wireless sensor network (WSN) deployments.
Security is essential to enable the Internet of Things (IoT).
 Key security measures that work well on the traditional Internet.
Security is nowadays an indispensable requirement in software systems.
 Traditional software engineering processes focus primarily on business requirements.
Security is the primary requirement in this era of informationization.
 Threats and worms are rising with a high degree and penetrating into the system.
Security protocols are the key to ensure network security.
 In the context of the state of the art.
Seeking product's quality is essential nowadays.
 One of the many quality aspects in software development is the source code complexity.
 Not taking care for the complexity during the development can result in unexpected cost.
Segmentation and classification of objects in images is one of the most important and yet one of the most complex problems in computer vision.
 In this work we propose a new model for natural image object classification using contextual information at the level of image segments.
 Context modeling is largely independent of appearance-based classification and proposed model enables simple upgrade of existing systems with information from global and/or local context.
 Context modeling is based on non-parametric use of appearance-based classification results which is a novel approach compared to previous systems that model context on a limited number of rules expressed with a fixed set of parameters.
 Model implementation resulted in a system that.
Segmentation based image analysis techniques are routinely employed for quantitative analysis of complex microstructures containing two or more phases.
 The primary advantage of these approaches is that spatial information on the distribution of phases is retained.
Segmentation is considered the central part of an image processing system due to its high influence on the posterior image analysis.
 In recent years.
Seismic images are composed of positive and negative seismic wave traces with different amplitudes (Robein 2010 Seismic Imaging: A Review of the Techniques.
Selecting appropriate mid-model is critical for high efficiency of model transition and data migration between relational databases and NoSQL databases.
 We explore the problem of uniform mid-model design used for model transition and data migration between relational databases and NoSQL databases.
 Our suggested approach proposes a complete process.
 We use data features and query features to generate a mid-model.
 And through predefined strategy.
Selecting the relevant factors in a particular domain is of utmost interest in the machine learning community.
 This paper concerns the feature selection process for twin support vector machine (TWSVM).
Self-report studies have found evidence that cultures differ in the display rules they have for facial expressions (i.
Semantic web facilitates the effective sharing and reuse of existing information.
 Institutional repositories (IRs) are built to organize and manage the intellectual output of an institute.
 They generally use relational databases for maintaining metadata of digital documents.
 The focus of this research is to share the information of an existing IR with other information systems for discovering common interests.
 To process the data in semantic context.
Semantic Web services (SWs) has become the most dominant paradigm of the service-oriented computing and one of the hot issues in the area of distributed computing technology to perform business services composition more efficiently and effectively for a number of years now.
 The distributed composition of SWs according to their functionality increases the capability of an application to fulfill the user's requirements.
 In this paper.
Sendai virus (SeV) is an enveloped nonsegmented negative-strand RNA virus that belongs to the genus Respirovirus of the Paramyxoviridae family.
 As a model pathogen.
Sensor networks are widely used in various domains like the intelligent transportation systems.
 Users issue queries to sensors and collect sensing data.
 Due to the low quality sensing devices or random link failures.
Sensory evaluation of garment silhouettes is comprehensive.
Sentiment analysis is an active research area in today's era due to the abundance of opinionated data present on online social networks.
 Semantic detection is a sub-category of sentiment analysis which deals with the identification of sentiment orientation in any text.
 Many sentiment applications rely on lexicons to supply features to a model.
 Various machine learning algorithms and sentiment lexicons have been proposed in research in order to improve sentiment categorization.
 Supervised machine learning algorithms and domain specific sentiment lexicons generally perform better as compared to the unsupervised or semi-supervised domain independent lexicon based approaches.
 The core hindrance in the application of supervised algorithms or domain specific sentiment lexicons is the unavailability of sentiment labeled training datasets for every domain.
 On the other hand.
Sentiments from online word-of-mouth (WOM) are often controversial.
Sentinel-1 is the first of a family of satellites designed to provide a data stream for the European environmental monitoring program known as Copernicus.
 Sentinel-1 constellation has been specifically designed to perform.
Sequences similarity analysis is one of the major topics in bioinformatics.
 It helps researchers to reveal evolution relationships of different species.
 In this paper.
Sequential pattern mining (SPM) is an important data mining problem with broad applications.
 SPM is a hard problem due to the huge number of intermediate subsequences to be considered.
 State of the art approaches for SPM (e.
Serial multi-class contour preserving classification can improve the representation of the contour of the data to improve the levels of classification accuracy for feed-forward neural network (FFNN).
 The algorithm synthesizes fundamental multi-class outpost vector (FMCOV) and additional multi-class outpost vector (AMCOV) at the decision boundary between consecutive classes of data to narrow the space of data.
 Both FMCOVs and AMCOVs will assist the FFNN to place the hyper-planes in such a way that can classify the data more accurately.
Serine acetyltransferase (CysE) belongs to the hexapeptide acetyltransferase family and is involved in the biosynthesis of L-cysteine in microorganisms.
 Mycobacterium tuberculosis CysE is regarded as a potential target for anti-tuberculosis (TB) drugs; however.
Serious games are generally considered a good alternative to improve motivation to learn.
 A game should have meaningful mechanics and elements to involve the players and to keep their attention.
 Each person has his own motivation to play games.
 This leads to different types of players and consequently to the need of different types of elements to keep them engaged.
 Understanding the player's behaviour is an important issue to improve support in the game.
 While the designer of a serious game focus mainly on its educational value.
Server-side socialbot detection approaches can identify malicious accounts and spams in online social networks.
Service quality management has become a recent phenomenon.
Service recommender systems provide same recommendations to different users based on ratings and rankings only.
Service-based systems have become popular in the software industry.
 In software engineering.
Session Initiation Protocol (SIP) has proved to be the integral part and parcel of any multimedia based application or IP-based telephony service that requires signaling.
 SIP supports HTTP digest based authentication.
Session initiation protocol (SIP) is a widely used authentication protocol for the Voice over IP communications.
 Over the years.
Session initiation protocol (SIP) is an application layer protocol used for signaling purposes to manage voice over IP connections.
 SIP being a text-based protocol is vulnerable to a range of denial of service (DoS) attacks.
 These DoS attacks can render the SIP servers/SIP proxy servers unusable by depleting memory and CPU time.
 In this paper.
Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology.
 In Requirements Reuse (RR).
Several applications in shape modeling and exploration require identification and extraction of a 3D shape part matching a 2D sketch.
 We present CustomCut.
Several approaches have been proposed in the literature for offering RDF views over databases.
 In addition to these.
Several approaches have been proposed to anonymize relational databases using the criterion of k-anonymity.
Several challenges to the field of ontology matching have been outlined in recent research.
 The selection of the appropriate similarity measures as well as the configuration tuning of their combination are known as fundamental issues the community should deal with.
 Verifying the semantic coherence of the discovered alignment is also known as a crucial task.
 As the challenging issues are both in basic matching techniques and in their combination.
Several DNA representations are used to study radio-induced complex DNA damages depending on the approach and the required level of granularity.
 Among all approaches.
Several methods based on port.
Several researchers around the world have studied gesture recognition.
Several techniques are used by requirements engineering practitioners to address difficult problems such as specifying precise requirements while using inherently ambiguous natural language text and ensuring the consistency of requirements.
Sex-determining region Y-box 2 (SOX2) is an oncogene known to be amplified and overexpressed in various human malignancies.
SFLASH is an instance of the famous C* multivariate public key cryptographic schemes and it was chosen by the NESSIE cryptographic project of the European Consortium in 2003 as a candidate signature algorithm used for digital signatures on limited-resource devices.
SGML standardized in ISO 8879 [International Organization for Standardization (1986)] has been proliferated because it can provide various styles and transform documents on different platforms.
 The SGML document has logical structure information in addition to the contents.
 As SGML documents are widely used.
Shallow water wave equation has increasing use in many applications for its success in eliminating spurious oscillation.
Shape alignment or estimation under occlusion is one of the most challenging tasks in computer vision field.
 Most previous works treat occlusion as noises or part models.
Shape correspondence is a fundamental problem in computer graphics and vision.
Shape description and feature detection are fundamental problems in computer graphics and geometric modeling.
 Among many existing techniques.
Sharing and reusing the big data in relational databases in a semantic way have become a big challenge.
 In this paper.
Shepperd and MacDonell Evaluating prediction systems in software project estimation.
 Information and Software Technology 54 (8).
ShK toxin is a cysteine-rich 35-residue protein ion-channel ligand isolated from the sea anemone Stichodactyla helianthus.
 In this work.
Shodan has been acknowledged as one of the most popular search engines available today.
Side-channel attacks have emerged as the nondestructive threats of security vulnerability in cryptographic hardware.
 This paper provides an overview of the protection techniques with counter ways of utilizing sidechannel information leakage for combatting side-channel attacks as well as securing the authenticity of devices against counterfeits or even falsification.
Signaling pathways driven by protein and lipid kinases are altered in most human diseases.
Silent data corruptions (SDCs) are errors that corrupt the system or falsify results while remaining unnoticed by firmware or operating systems.
 In numerical integration solvers.
Similarity join on XML documents which are usually modeled as rooted ordered labeled trees is widely applied.
Similarity search is an important and fundamental problem.
Similarly to computer operating systems which guarantee safe access to memory resources.
Similarly to the correspondence between radical ideals of a polynomial ring and varieties in algebraic geometry.
Simple database storage configuration includes block device and filesystem.
 In advanced multi-disk database storage configuration also disk manager is required.
 Article presents multi-disk storage configuration impact on relational database performance.
 Research results include various local multi-disk storage space configuration scenarios for database cluster in modern database management systems with popular disk managers like software RAID and standard or thin provisioned logical volume equipped with disk allocation policy.
 The research conclusions facilitate the local storage space configuration for efficient transaction processing in relational databases.
Simple linear regression in the functional errors-in-variables (EIV) model is revisited from a different perspective.
Simple Object Access Protocol (SOAP) among other techniques implements Web Services (WS).
 SOAP offers a lightweight and simple mechanism for exchange of structured and typed information among computing devices in a decentralized.
Simulation models are widely used in risk analysis to study the effects of uncertainties on outcomes of interest in complex problems.
Simulation of blood flow in a stenosed artery using Smoothed Particle Hydrodynamics (SPH) is a new research field.
Simulation-based studies (SBS) have become an interesting investigation approach for Software Engineering (SE).
Simulations are a practical and reliable approach to power calculations.
Simultaneous wireless information and power transfer (SWIPT) has recently drawn significant attention.
Since April 2014 the Artemis/ECSEL project EMC2 is running and provides significant results.
 EMC2 stands for Embedded Multi-Core Systems for Mixed Criticality Applications in Dynamic and Changeable Real-Time Environments.
 In this paper we report recent progress on technical work in the different workpackages and use cases.
 We highlight progress in the research on system architecture.
Since conventional evacuation drills do not adequately simulate disaster situations.
Since data and resources have massive feature and feature of data are increasingly complex.
Since decades the sensors and computer programming became part of automation industry.
 A number of sensors and programming languages are evolving for which the ancient technologies are being not repairable and are being kept aside.
 Indeed the use of electricity as the major source makes the system work only with the available electricity.
 This makes the system costlier and applications in connection with the availability of electricity.
 The present research work is to evolve an automatic flusher that work on the mechanical linkage.
Since frequent communication between applications takes place in high speed networks.
Since pair programming appeared in the literature as an effective method of teaching computer programming.
Since substitution box (S-box) is the only nonlinear component related to confusion properties for many block encryption algorithms.
Since the emergence of more sophisticated interaction devices.
Since the initial visions proposed in the SmartDust project fifteen years ago.
Since the introduction of SPICE non-linear controlled voltage and current sources.
Since the proposal of a fast learning algorithm for deep belief networks in 2006.
Since the proposal of NTRU cryptosystem.
Since the shale gas production data predicted by traditional simulators is far below that of field test.
Since XML could benefit greatly from database support and more specifically from relational database systems.
Single image super-resolution is of great importance in computer vision.
 Various methods (e.
 learning methods) have been successfully developed in recent years.
 Despite the demonstrated success in the natural images.
Single system image is a computing paradigm where a number of distributed computing resources are aggregated and presented via an interface that maintains the illusion of interaction with a single system.
 This approach encompasses decades of research using a broad variety of techniques at varying levels of abstraction.
Single-ISA asymmetric multicore processors (AMPs).
Single-Root I/O Virtualization (SR-IOV) is a specification that allows a single PCI Express (PCIe) device (physical function or PF) to be used as multiple PCIe devices (virtual functions or VF).
 In a virtualization system.
Singular value decomposition (SVD) is a tool widely used in data denoising.
Singular value decomposition (SVD)-based ultrasound blood flow clutter filters have recently demonstrated substantial improvement in clutter rejection for ultrafast plane wave microvessel imaging.
Situation prediction is an increasingly important focus in network security.
 The information of incoming security situation in the network is important and helps the network administrator to make good decisions before taking some defense remedies towards the attack exploitation.
 Although Grey Verhulst prediction model has demonstrated satisfactory results in other fields but some further investigations are still required to improve its performance in predicting incoming network security situation.
 In order to attain higher predictive accuracy of the existing Grey Verhulst prediction models.
Sketching as a natural mode for human communication and creative processes presents opportunities for improving human-computer interaction in geospatial information systems.
Skilled human full-body movements are often planned in a highly predictive manner.
 For example.
SLC22A2 facilitates the transport of endogenous and exogenous cationic compounds.
 Many pharmacologically significant compounds are transported by SLC22A2.
Sleep impairment significantly alters human brain structure and cognitive function.
Sliding Window Operations (SWOs) are widely used in image processing applications.
 They often have to be performed repeatedly across the target image.
Slope shaping is important for the prevention of and recovery from sediment disasters.
 Because of the danger of direct operation at disaster scenes.
Small noncoding RNAs (sncRNAs) play important roles in RNA interference (RNAi).
 In addition to microRNA (miRNA) and Piwi-interacting RNA (piRNA).
Small RNAs are a class of short non-coding endogenous RNAs that play essential roles in many biological processes.
 Recent studies have reported that microRNAs (miRNAs) are also involved in ethylene signaling in plants.
 LeERF1 is one of the ethylene response factors (ERFs) in tomato that locates in the downstream of ethylene signal transduction pathway.
 To elucidate the intricate regulatory roles of small RNAs in ethylene signaling pathway in tomato.
Small satellites have limited payload and their attitudes are sometimes difficult to determine from the limited onboard sensors alone.
 Wrong attitudes lead to inaccurate map projections and measurements that require post-processing correction.
 In this study.
Small urbanized areas (UAs) as defined by the U.
 Census and the Environmental Protection Agency (EPA) have a population between 50.
Smart cards are well-known tamper-resistant devices.
Smart devices from smartphones to wearable computers today have been used in many purposes.
 These devices run various mobile operating systems like Android.
Smart farming is a management style that includes smart monitoring.
Smart grid introduces new communication demands.
Smart grid is an electrical grid that uses digital information and communication technology to gather information.
 Like other digital systems.
Smart grids are considered as one of the most revolutionary technologies in the field of power and communication.
Smart meter.
Smart mobile devices have fostered new interaction scenarios that demand sophisticated interfaces.
 The main developers of operating systems for such devices provide APIs for developers to implement their own applications.
Smart mobile devices including smart phones and tablets have become one of the most popular devices in the personal computing environment.
 One of the major characteristics of mobile applications is that the applications in the field of entertainment like games and augmented reality require a great deal of computations.
 In order to deal with this.
Smartphones have become an integral part of our daily life.
 Businesses now offer services through smartphones.
 Users also store sensitive personal information on their smartphones and perform financial transactions.
 Consequently.
Smartphones have become ubiquitous in our society.
 With a large number of users spending more time and sharing more personal data with these devices.
Social online communities and platforms play a significant role in the activities of software developers either as an integral part of the main activities or through complimentary knowledge and information sharing.
 As such techniques become more prevalent resulting in a wealth of shared information.
Socially assistive robotics is an important emerging research area.
 Socially assistive robotics is challenging as it is required to move robots out of laboratories and industrial settings to interact with ordinary human beings as peers.
Software and software deliverables have high impact on all fields.
 Once software is deployed.
Software architecting activities are not discussed in most agile software development methods.
 That is why.
Software architectural style is one of the best concepts to define a family of related architectures and their common properties.
 Despite the essential role of software architectures in the software engineering practice.
Software cybernetics aims at improving the reliability of software by introducing the control theory into software engineering domain systematically.
 A key issue in software verification is to improve the reliability of software by inspecting whether the software can achieve its expected behaviors.
 In this paper.
Software cybernetics research is to apply a variety of techniques from cybernetics research to software engineering research.
 For more than fifteen years since 2001.
Software defect prediction can automatically predict defect-prone software modules for efficient software test in software engineering.
 When the previous defect labels of modules are limited.
Software defect prediction predicts fault-prone modules which will be tested thoroughly.
Software Defined Networking (SDN) has recently emerged to become one of the promising solutions for the future Internet.
 With the logical centralization of controllers and a global network overview.
Software Defined Networking is a paradigm still in its emergent stages in the realm of production-scale networks.
 Centralisation of network control introduces a new level of flexibility for network administrators and programmers.
 Security is a huge factor contributing to consumer resistance to implementation of SDN architecture.
 Without addressing the issues inherent from SDNs centralised nature.
Software developers use many different communication tools and channels in their work.
 The diversity of these tools has dramatically increased over the past decade and developers now have access to a wide range of socially enabled communication channels and social media to support their activities.
 The availability of such social tools is leading to a participatory culture of software development.
Software development (SD) companies employ.
Software development has become a fundamental process on any business or organization.
 As a consequence.
Software development is highly dependent on human efforts and collaborations.
Software development life cycle is a structured process.
Software development organizations that rely on Capability Maturity Model Integration (CMMI) to assess and improve their processes have realized that agile approaches can provide improvements as well.
 CMMI and agile methods can work well together and exploit synergies that have the potential to improve dramatically business performance.
 The major question is: How to realize the integration of these two seemingly different approaches? In an earlier work.
Software Engineering activities are information intensive.
 Research proposes Information Retrieval (IR) techniques to support engineers in their daily tasks.
Software engineering and mission command are two separate but similar fields.
Software engineering is a knowledge-intensive task.
 Software development organisations depend on the knowledge they have at their disposal to develop high-quality software and become competitive in the software industry.
 Knowledge held by employees and in the organisation's repositories.
Software engineering is concerned with the development and advancement of huge and multiple software intensive systems.
 It shelters theories.
Software engineering is the study and an application of engineering to the design.
Software engineering means applying engineering principles to software development.
 In the same way.
Software engineering predictive modeling involves construction of models.
Software engineering standards often utilize different underpinning metamodels and ontologies.
Software fault prediction is important in software engineering field.
 Fault prediction helps engineers manage their efforts by identifying the most complex parts of the software where errors concentrate.
 Researchers usually study the fault-proneness in modules because most modules have zero faults.
Software fault tolerance is an important issue when using software systems in safety-critical applications.
 In such systems.
Software inspection models have been remarkable development in over the past four decades.
Software integration testing plays an increasingly important role as the software industry has experienced a major change from isolated applications to highly distributed computing environments.
 Conducting integration testing is a challenging task because it is often very difficult to replicate a real enterprise environment.
 Emulating testing environment is one of the key solutions to this problem.
Software intelligent development has become one of the most important research trends in software engineering.
 In this paper.
Software models.
Software product line (SPL) development is a new approach to software engineering which aims at the development of a whole range of products.
Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive.
Software Product Lines allow creating a set of applications that share a set of common features.
 This makes software product lines appropriate for implementing a family of software products when each stakeholder has different needs and requirements evolve constantly.
 In the case of emergency management.
Software project scheduling in dynamic and uncertain environments is of significant importance to real-world software development.
 Yet most studies schedule software projects by considering static and deterministic scenarios only.
Software refactoring has been recognized as a valuable process during software development and is often aimed at repaying technical debt.
 Technical debt arises when a software product has been built or amended without full care for structure and extensibility.
 Refactoring is useful to keep technical debt low and if it can be automated there are obvious efficiency benefits.
 Using a combination of automated refactoring techniques.
Software reliability is one of the most important software quality indicators.
 It is concerned with the probability that the software can execute without any unintended behavior in a given environment.
 In previous research we developed the Reliability Prediction System (RePS) methodology to predict the reliability of safety critical software such as those used in the nuclear industry.
 A RePS methodology relates the software engineering measures to software reliability using various models.
Software startups are newly created companies with no operating history and oriented towards producing cutting-edge products.
Software systems tend to become larger and more complicated as the functional requirements increase gradually.
 Well-modularized software systems are widely believed to be understood.
Software-based.
Software-defined network structure of the network fundamental changes.
Software-defined networking (SDN) is an emerging paradigm.
Software-Defined Networking (SDN) relies on open programmability of network devices.
Software-defined networking (SDN) supports flexibly routing and switching by splitting the control plane and data plane in network devices.
Soil moisture is one of the essential climate variables for the Global Climate Observing System (GCOS) that has been prioritized by the ESA's Climate Change Initiative to construct its homogeneous long-term climate record.
 This requires a consistent characterization of the error structures in the individual data sets.
Soil scientists in the USA have created a large national database of written soil profile descriptions that follow a well-defined set of rules for describing soil morphological properties.
 Interpreting these soil descriptions is a skill that requires considerable practice and experience.
 While writing a soil description is straightforward.
Solar power has become an attractive alternative source of energy.
 The multi-crystalline solar cell has been widely accepted in the market because it has a relatively low manufacturing cost.
 Multi-crystalline solar wafers with larger grain sizes and fewer grain boundaries are higher quality and convert energy more efficiently than mono-crystalline solar cells.
 In this article.
Solid State Drives (SSDs) have been extensively deployed as the cache of hard disk-based storage systems.
 The SSD-based cache generally supplies ultra-large capacity.
Soliton control and management using generalized external potentials in an inhomogeneous fiber to the design of high speed optical devices and ultrahigh capacity transmission systems are investigated based on solving the variable-coefficient generalized nonautonomous nonlinear Schrodinger equation with the help of symbolic computation.
 We construct Lax pair for GNLS equation by means of AKNS method and two soliton solutions are obtained by virtue of the Darboux transformation.
 With symbolic computation.
Soliton interactions for the coupled nonlinear Schrodinger equations.
Solitude verification is arguably one of the simplest fundamental problems in distributed computing.
Solving a complex problem often requires a way to break it down into smaller.
Solving large-scale all-to-all comparison problems using distributed computing is increasingly significant for various applications.
 Previous efforts to implement distributed all-to-all comparison frameworks have treated the two phases of data distribution and comparison task scheduling separately.
 This leads to high storage demands as well as poor data locality for the comparison tasks.
Some recent results (Bauer et al.
 in Algorithms in bioinformatics.
Sorting is one of the important algorithms in computer programming.
 The ordinary 6 sorting algorithms are analyzed and compare from the algorithmic time complexity and stability.
 The executive efficiency of 6 sorting algorithms is verified by Java program.
 The costing time and stability of 6 sorting algorithms are compared to provide certain reference for sorting algorithm.
SPA (Speaking Plant Approach) is a comparatively modern cultivation method which grows plant according to their biological information.
 The SPA plant factory system has suitable elements for use in engineering education.
 In this paper.
Space debris is a special kind of fast-moving.
Space moving object recognition and tracking is an important research topic in computer vision.
 It has broad application prospects in space exploration.
Sparse matrix-vector multiplication (SpMV) is an important issue in scientific computing and engineering applications.
 The performance of SpMV can be improved using parallel computing.
 The implementation and optimization of SpMV on GPU are research hotspots.
 Due to some irregularities of sparse matrices.
Sparse matrix-vector multiplication (spMVM) is the most time-consuming kernel in many numerical algorithms and has been studied extensively on all modern processor and accelerator architectures.
Sparse modeling has been widely and successfully used in many applications.
Sparse representation-based classifier (SRC) and kernel sparse representation-based classifier (KSRC) are founded on combining pattern recognition and compressive sensing methods and provide acceptable results in many machine learning problems.
 Nevertheless.
Sparse representations have proven their efficiency in solving a wide class of inverse problems encountered in signal and image processing.
 Conversely.
Spatial errors (e.
Spatial patterns of land use change due to urbanization and its impact on the landscape are the subject of ongoing research.
 Urban growth scenario simulation is a powerful tool for exploring these impacts and empowering planners to make informed decisions.
 We present FUTURES (FUTure Urban - Regional Environment Simulation) - a patch-based.
Spatiotemporal co-occurrence patterns (STCOPs) represent the subsets of feature types whose instances are frequently co-occurring both in space and time.
 Spatiotemporal co-occurrences reflect the spatiotemporal overlap relationships among two or more spatiotemporal instances both in spatial and temporal dimensions.
 STCOPs can be potentially used to predict and understand the generation and evolution of different types of interacting phenomena in various scientific fields such as astronomy.
Speckle imaging method is useful for monitoring of blood flow in living bodies.
 We have proposed so far the method for simultaneous imaging of blood flow and blood concentration change using laser speckle patterns at two wavelengths.
Speckle reduction is a prerequisite for many image processing tasks in synthetic aperture radar images.
Spectral clustering makes use of spectral-graph structure of an affinity matrix to partition data into disjoint meaningful groups.
 It requires robust and appropriate affinity graphs as input in order to form clusters with desired structures.
 Constructing such affinity graphs is a nontrivial task due to the ambiguity and uncertainty inherent in the raw data.
 Most existing spectral clustering methods typically adopt Gaussian kernel as the similarity measure.
Spectral theory has many applications in several main scientific research areas (structural mechanics.
Speed and accuracy are important factors when dealing with time-constraint events for disaster.
Speed binning of system-on-chips (SoCs) using conventional F-max test requires application of complex functional test patterns.
 Functional workload-based speed binning techniques incur high test-cost in terms of long test-time and complexity in functional test generation.
Speed limits are a common traffic regulation for balancing traffic mobility and safety on roadways.
 Speed transition zones bear complicated driver behaviours.
Spin-optoelectronics is an emerging technology in which novel and advanced functionalities are enabled by the synergetic integration of magnetic.
Spreadsheet applications are widely adopted by millions of end users from several application domains and provide strategic support to many business.
Spreadsheets are among the most commonly used applications for data management and analysis.
 They combine data processing with very diverse supplementary features: statistics.
Stacks of digital astronomical images are combined in order to increase image depth.
 The variable seeing conditions.
Staff removal is an image processing task that aims to facilitate further analysis of music score images.
 Even when restricted to images in specific domains such as music score recognition.
Starting from a link with the discrete polynomial decomposition.
Starting from the classical differential cryptography.
State recognition in disconnecting switches is important during substation automation.
Static diagrams are the most prevalent artifact used in visualizing component-and-connector architectures and supporting software architecture learning.
 The use of such artifacts exhibits a fundamental disconnect from the dynamic nature of software systems.
Static slicing is a popular program analysis used in software engineering to find which parts of a program affect other parts.
 Unfortunately.
Steganography is a technique of dissimulating information in digital media.
 In contrast to cryptography.
Stem cell factor (SCF) is a critical protein with key roles in the cell such as hematopoiesis.
Step length estimation is an important issue in areas such as gait analysis.
Stock-separation of highly mobile Clupeids (sprat - Sprattus sprattus and herring - Clupea harengus) using otolith morphometrics was explored.
 Analysis focused on three stock discrimination problems with the aim of reassigning individual otoliths to source populations using experiments undertaken using a machine learning environment known as WEKA (Waikato Environment for Knowledge Analysis).
 Six feature sets encoding combinations of size and shape together with nine learning algorithms were explored.
 To assess saliency of size/shape features half of the feature sets included size indices.
Storage class memory (SCM) has the potential to revolutionize the memory landscape by its non-volatile and byte-addressable properties.
Storing and retrieving data in cloud are important in today's environment.
 It also adds insecurity as data sharing in cloud would be affected by hacking or modifying the original content of the data.
 For secure data transmission.
Strawberry is one of the most economically important fruit crops in the world.
 Cytokinins (CKs) play a critical role in plant growth and development.
Streams in urban areas are prone to degradation.
 While urbanization-induced poor water quality is a widely observed and well documented phenomenon.
Streptococcus pyogenes group A Streptococcus (GAS) is the most common cause of bacterial throat infections and can cause mild to severe skin and soft tissue infections including impetigo.
Streptococcus uberis is one of the most prevalent pathogens causing clinical and subclinical mastitis worldwide.
 Among bacterial factors involved in intramammary infections caused by this organism.
Stress Corrosion Cracking (SCC) is a very common failure mechanism characterized by a slow.
Stress is one of the key factor that impacts the quality of our daily life: Fromthe productivity and efficiency in the production processes to the ability of (civilian and military) individuals in making rational decisions.
Structural monitoring systems are an objective and quantitative-based management tool that have been developed to assist structure owners with their diagnostic and prognostic decision making processes.
 As sensing technologies mature.
Structured Query Language (SQL) is widely used to operate Relational Databases (RDBs).
Student cheating is a well-known topic with a large number of references in literature.
 This paper presents and discusses the role of assessment rules as a vehicle for cheating minimization.
Student course planning and advising at most at all universities is an important process to ensure students to fulfill the degree requirements of the university in a structured way and without encountering unnecessary and lengthy delays.
 United Arab Emirates University (UAEU) is one such institution where student register courses with one of the most advanced and adequate registration systems however; students have faced problems because of a need of a proper advising system as well as a need of seeking proper advising.
 A Knowledge Based Course Planning System (KBCPS) software package is devised to suit the course planning needs of the Electrical Engineering Department.
Students learn about the existence of the linking process.
Students who begin university studies generally have some characteristics ( lack of habits and skills) which can often be an obstacle for their development in a different field and with other teaching and learning patterns.
Students who have studied in their native language face difficulty in acquiring programming skills through English medium instruction.
 One solution for this is to create screencasts of live coding along with instructor explanations in the native language.
 In this paper we examine the impact of screencast based self-paced learning for Hindi medium (native language) students.
 Our study had three groups - One group had Hindi as the medium of prior instruction.
Studied in this paper are the bright-dark vector soliton solutions for a generalized coupled Hirota system which describes the propagation for the high-intensity ultrashort pulses in the optical glass fiber.
 Beyond the existing bilinear forms.
Studied in this paper is a ()-dimensional nonlinear Schrodinger equation with the group velocity dispersion.
Studies have reported that intraplaque neovascularisation (IPN) is closely correlated with plaque vulnerability.
 In this study.
Studies have shown that deaf and hearing-impaired students have many difficulties in learning applied disciplines such as Medicine.
Study of cavities and channels in molecular structure is a crucial step in understanding the function of biomolecules.
 Current tools and techniques for extracting these structural features are sensitive to uncertainties in atomic position and radii.
 In this paper.
Studying the patterns hidden in gene expression data helps to understand the functionality of genes.
 But due to the large volume of genes and the complexity of biological networks it is difficult to study the resulting mass of data which often consists of millions of measurements.
 In order to reveal natural structures and to identify interesting patterns from the given gene expression data set.
Subcutaneous vein network plays important roles to maintain microcirculation that is related to some diagnostic aspects.
 Despite developments of optical imaging technologies.
Subdivision surfaces are a common tool in geometric modelling.
Subgroup discovery is a key data mining method that aims at identifying descriptions of subsets of the data that show an interesting distribution with respect to a pre-defined target concept.
 For practical applications the integration of numerical data is crucial.
Submodular function minimization is a key problem in a wide variety of applications in machine learning.
Subset Construction (SC) is the classical algorithm for the determinization of a nondeterministic finite automaton (NFA) into an equivalent deterministic one (DFA).
 Although SC works fine in most application domains.
Substitution Box (S-Box) is one of the most significant structures used to create an encryption which is strong and resistant against attacks in block encryption algorithms.
 S-Box plays an important role in data encryption.
 This paper presents a novel S-Box generation algorithm design based on scaled Zhongtang chaotic system.
 In this study.
Substitution boxes (S-boxes) are the fundamental mechanisms in symmetric key cryptosystems.
 These S-boxes guarantee that the cryptosystem is cryptographically secure and make them nonlinear.
 The S-boxes used in conventional and modern cryptography are mostly constructed over finite Galois field extensions of binary Field F-2 .
 We have presented a novel construction scheme of S-boxes which is based on the elements of subgroups of multiplicative groups of units of the commutative finite chain rings of type F-2[u]/.
Substitution Boxes (S-Boxes) play an important role in many modern-day cryptographic algorithms.
Substitution-boxes.
Suffix arrays are fundamental full-text index data structures of importance to a broad spectrum of applications in such fields as bioinformatics.
Suffix trees are highly regarded data structures for text indexing and string algorithms [MCreight 76.
Supermarkets nowadays are equipped with barcode scanners to speed up the checkout process.
 Nevertheless.
Super-simple designs can be used to provide samples with maximum intersection as small as possible in statistical planning of experiments and can be also applied to cryptography and codes.
 In this paper.
Supervisory control and data acquisition (SCADA) systems have become a salient part in controlling critical infrastructures.
Supervisory Control and Data Acquisition (SCADA) systems monitor and control industrial systems of national importance.
Supply Chain Management is a critical domain for Fast Moving Consumer Goods (FMCGs).
 This domain is known for its complexity.
 New standards and regulations regarding Energy Efficiency and Environmental Aspects in general.
Supporting large amounts of spatial data is a significant characteristic of modern databases.
Suppose a client stores elements in a hash table that is outsourced to an untrusted server.
 We address the problem of authenticating the hash table operations.
Suppose x is any exactly k-sparse vector in R-n.
 We present a class of sparse matrices A.
Surface bughole of concrete is one of surface imperfections of concrete appearance.
Surface development origins from the cloth-making and computer graphics without consideration of the thickness.
Surface meshes have recently attracted great interest since they are frequently used in many computer graphics applications.
 These meshes are often generated by isosurfacing representations or also by scanning devices.
 Unfortunately.
Surrogate assisted global optimization is gaining popularity.
Surveying endangered species is necessary to evaluate conservation effectiveness.
 Camera trapping and biometric computer vision are recent technological advances.
 They have impacted on the methods applicable to field surveys and these methods have gained significant momentum over the last decade.
Sustainability research faces many challenges as respective environmental.
Sustainable living.
Suzaku is a pattern programming framework that enables programmers to create pattern-based parallel MPI programs without writing the MPI message-passing code implicit in the patterns.
 The purpose of this framework is to simplify message-passing programming and create better structured programs based upon established parallel design patterns.
 The focus for developing Suzaku is on teaching parallel programming.
 This paper covers the main features of Suzaku and describes our experiences using it in parallel programming classes.
Switched Affine Systems (SAS's) is a class of Hybrid Systems composed of a collection of Affine Systems (AS's) and a switching signal that determines.
Switched systems theory is used to analyze the stability of image-based observers for three-dimensional localization of objects in a scene in the presence of intermittent measurements due to occlusions.
Switching cost is an important factor for policy makers to consider because it sets a higher price for locked-in consumers by making the market less competitive.
 Though there has been some empirical research analyzing switching costs in the mobile telecommunications market.
Symbolic Computation and Satisfiability Checking are two research areas.
Symbolic computation has underpinned a number of key advances in Mathematics and Computer Science.
 Applications are typically large and potentially highly parallel.
Symbolic computation is an important area of both Mathematics and Computer Science.
Symbolic execution is a technique enabling the automatic generation of test inputs that exercise a set of execution paths within a code unit to be tested.
 If the paths cover a sufficient part of the code under test.
Symmetric positive definite (SPD) matrices in the form of covariance matrices.
Symmetric-key cryptography can resist the potential post-quantum attacks expected with the not-so-faraway advent of quantum computing power.
 Hash-based.
Symmetry-breaking problems are among the most well studied in the field of distributed computing and yet the most fundamental questions about their complexity remain open.
 In this article we work in the LOCAL model (where the input graph and underlying distributed network are identical) and study the randomized complexity of four fundamental symmetry-breaking problems on graphs: computing MISs (maximal independent sets).
Synthetic aperture radar (SAR) image segmentation is investigated from feature extraction to algorithm design.
System monitoring can help to detect anomalies.
Systems of polynomial equations arise throughout mathematics.
Systems-on-a-Chip are among the best-performing and complete solutions for complex electronic systems.
 This is also true in the field of network security.
T cells recognize antigen using a large and diverse set of antigen-specific receptors created by a complex process of imprecise somatic cell gene rearrangements.
 In response to antigen-/receptor-binding-specific T cells then divide to form memory and effector populations.
 We apply high-throughput sequencing to investigate the global changes in T cell receptor sequences following immunization with ovalbumin ( OVA) and adjuvant.
Tabulation method is usually adopted to describe the mapping relation of the analytic function of the local rules of Cellular Automaton.
 Although tabulation method is quite suitable to describe the mapping relation among discrete variables.
Tactical Edge Networks provide one of the most challenging communication environments.
 In order to cope with node mobility.
Taking advantage of the Hirota bilinear form.
Taking the inhomogeneities of media and nonuniform boundaries into account.
Target classification algorithms have generally kept pace with developments in the academic and commercial sectors since the 1970s.
Target detection is a hard real-time task for video and image processing.
 This task has recently been accomplished through the feedforward process of convolutional neural networks (CNN).
Targeting non-native-ligand binding sites for potential investigative and therapeutic applications is an attractive strategy in proteins that share common native ligands.
Task allocation problems have traditionally focused on cost optimization.
Task scheduling is one of the major issues to achieve high performance in distributed systems such as Grid.
Taylor series expansions are widely used in engineering approximations.
Teacher-centered has been traditionally playing the role of basic teaching method for long time.
 In this way.
Teachers and students often consider learning programming a difficult pursuit.
 Face-to-face learning alone cannot provide effective teaching or efficient solutions for learning.
 A case teaching model can make students active in programming courses.
Teaching students how to code in K-12 classrooms is considered as one of the major keys to promoting engineering education in the future.
Teams working on the development of software systems use certain tools and workflows of product and knowledge management.
 These tools and workflows help them plan.
Technological advances of embedded computing exposed humans to an increasing intrusion of computing in their day-to-day life (e.
 smart devices).
 Cooperation.
Technologies for scalable analysis of very large datasets have emerged in the domain of internet computing.
Technologies Intermittent Cutting Problem and Endpoint Cutting Problem for cutting the sheet material belong to so called resource-saving technologies.
 These technologies allow to overlap the contours of cut-off details.
 This paper is devoted to review of mathematical models for cut-off (lie details by these technologies and algorithms for defining the cutter route according to some technological restrictions.
Technology - based learning could help students achieve fundamental abilities and skills like computational thinking and creativity.
 Engaging young students in computer science concepts and programming with a creative and enjoyable way is a challenging issue.
 We are aiming to apply and evaluate constructionist or apprenticeship techniques and visual programming tools that could assist and improve the learning activity.
Telecare Medicine Information Systems (TMIS) improve the medical and healthcare services by information and communication technology.
 The doctor can monitor the patient's physiological condition by TMIS.
Telecare medicine information systems (TMIS) provide flexible and convenient e-health care.
Temporal graphs represent vertices and binary relations that change along time.
 The work in this paper proposes to represent temporal graphs as cells in a 4D binary matrix: two dimensions to represent extreme vertices of an edge and two dimensions to represent the temporal interval when the edge exists.
 This strategy generalizes the idea of the adjacency matrix for storing static graphs.
 The proposed structure called Compressed () is capable of dealing with unclustered data with a good use of space.
 The uses asymptotically the same space than the (worst case) lower bound for storing cells in a 4D binary matrix.
Temporal Logic Model Checking is a verification method in which we describe a system.
Temporal networks are data structures for representing and reasoning about temporal constraints on activities.
 Many kinds of temporal networks have been defined in the literature.
Tensor analysis has reached a celebrity status in the areas of machine learning.
Ternary content-addressable memory (TCAM)-based search engines generally need a priority encoder (PE) to select the highest priority match entry for resolving the multiple match problem due to the don't care (X) features of TCAM.
 In contemporary network security.
Terrain data can be processed from the double perspective of computer graphics and graph theory.
 We propose a hybrid method that uses geometrical and vertex attribute information to construct a weighted graph reflecting the variability of the vertex data.
 As a planar graph.
Terrestrial laser scanners are frequently used in most of measurement application.
Testes-biased genes evolve rapidly and are important in the establishment.
Tetris provides a difficult.
Text detection in a natural environment plays an important role in many computer vision applications.
 While existing text detection methods are focused on English characters.
Text input is an important part of the data annotation process.
Texture analysis is a major task in many areas of computer vision and pattern recognition.
Texture characterisation for freeform non-Euclidean surfaces is becoming increasingly important due to the widespread of the use of such surfaces in different applications.
Texture feature extraction methods have been improved greatly in recent years.
 It is widely known that the local texture feature descriptor can achieve desired performance under the change of image geometric size.
Texture synthesis is a well-established area.
Textures are widely used in modern computer graphics.
 Their size.
The AAC - Austrian Academy Corpus is a diachronic German language digital text corpus of more than 500 million tokens.
 The text corpus has collected several thousands of texts representing a wide range of different text types.
 The primary research aim is to develop text language resources for the study of texts.
 For corpus linguistics and corpus based language research large text corpora need to be structured in a systematic way.
 For this structural purpose the AAC is making use of the notion of container.
 By container in the context of corpus research we understand a flexible system of pragmatic representation.
The kernel model has been part of operating system architecture for decades.
The (1+1)-dimensional higher-order Broer-Kaup (HBK) system is investigated in this paper.
 Painleve test shows that there are two solution branches.
The (2 + 1)-dimensional Zakharov equations arising from the propagation of a laser beam in a plasma are studied in this paper.
 Analytic soliton solutions are obtained by means of the symbolic computation.
The 2011 release of the first version of the ISO 26262 standard for automotive systems demand the elicitation of safety goals following a rigorous method for hazard and risk analysis.
 Companies are struggling with the adoption of the standard due to ambiguities.
The 3-hydroxy-3-methylglutaryl-CoA reductase (HMGR) is considered the first rate-limiting enzyme in plant isoprenoid biosynthesis.
 We discovered the MdHMGR4 gene in Malus domestica which showed 97.
70% amino acid sequence identity with MdHMGR1.
 MdHMGR4 showed a root-specific expression pattern in apple and was strongly enhanced by ETH.
The ability of capturing unknown attacks is an attractive feature of anomaly-based intrusion detection and it is not surprising that research on such a topic represents one of the most promising directions in the field of network security.
 In this work we consider two different traffic descriptors and evaluate their ability in capturing different kinds of anomalies.
The ability to automatically recognize human faces based on dynamic facial images is important in security.
The ability to derive new insights from data using advanced machine learning or analytics techniques can enhance the decision-making process in companies.
 Nevertheless.
The ability to design effective solutions using parallel processing should be a required competency for every computing student.
The ability to distinguish peatland types at the landscape scale has implications for inventory.
The ability to exploit emerging exascale computational systems will require a careful review and redesign of core numerical algorithms and their implementations to fully exploit multiple levels of concurrency.
The ability to present effectively is essential for professionals; therefore.
The ability to process and manage large data volumes has been proven to be not enough to tackle the current challenges presented by Big Data.
 Deep insight is required for understanding interactions among connected systems.
The ability to retrieve accurate information from relational databases requires proficiency in structured query language (SQL).
 In spite of its declarative nature.
The ability to save the state of a running virtual machine (VM) for later restoration is an important tool for home.
The ability to timely process significant amounts of continuously updated spatial data is mandatory for an increasing number of applications.
 Parallelism enables such applications to face this data-intensive challenge and allows the devised systems to feature low latency and high scalability.
 In this paper.
The Abstract interpretation has been widely applied to approximate data structures and (usually numerical) value information.
The Abstraction in computer graphics defines a procedure that discriminates the essential information that is worth keeping.
 Usually details.
The abundant aspects of big data and it's technology are increasing due to new methods of fetching data and diverse needs.
 Meteorological data is also the source of big data in terms of volume.
The accelerating progress and availability of low cost computers.
The accuracy and reliability of an anomaly-based network intrusion detection system are dependent on the quality of data used to build a normal behavior profile.
The accuracy of imaging for bulk volume estimation of apple slices in the process of drying at temperatures from 40 to 80 degrees C was investigated and compared with physical caliper measurements.
 The initial hypothesis was to estimate bulk volume of cylindrically shaped apple slices from diameter and thickness imaging.
 Imaging of diameter showed strong agreement with caliper measurements throughout the entire drying process.
The accurate location of eyes in a facial image is important to many human facial recognition-related applications.
The achievement of the objectives of precision agriculture requires not only the development of new technologies.
The actuator saturation problem is very common in the real system.
The adoption of computational thinking (CT) has been increasing in K-12 classrooms and curricula.
 One population that could benefit from early instruction in CT is students with Autism Spectrum Disorders (ASD).
 Unfortunately.
The adoption of quality assurance methods based on software process improvement models has been regarded as an important source of variability in software productivity.
 Some companies perceive that their implementation has prohibitive costs.
The Adriatic-Ionian macro region is one of the four Mediterranean sub-regions.
The advance of massively parallel computing in the nineteen nineties and beyond encouraged finer grid intervals in numerical weather-prediction models.
 This has improved resolution of weather systems and enhanced the accuracy of forecasts.
The advance reservation (AR) of connections is becoming an increasingly important requirement for many emerging commercial bandwidth and scientific computing users.
The advancement of software defined networking (SDN) is redefining traditional computer networking architecture.
 The role of the control plane of SDN is of such importance that SDNs are referred to as network operating systems (OSs).
The advancement of World Wide Web has revolutionized the way the manufacturers can do business.
 The manufacturers can collect customer preferences for products and product features from their sales and other product-related Web sites to enter and sustain in the global market.
 For example.
The advances in information technology have witnessed great progress on healthcare technologies in various domains nowadays.
The advantage of the ICCG method for solving large sparse matrix is taken in the CN-FDTD equation solving.
 The CN-ICCG-FDTD can accelerate iteration for numerical calculation.
The advent and prosperity of the GPS equipped devices and reliable location technologies has resulted in a wide growth of location based service.
 As a certain type of geo-spatial application.
The advent of both Cloud computing and Internet of Things (IoT) is changing the way of conceiving information and communication systems.
The advent of intelligent vehicle technologies holds significant potential to alter the dynamics of traffic flow.
 Prior work on the effects of such technologies on the formation of self-organized traffic jams has led to analytical solutions and numerical simulations at the mesoscopic scale.
The advent of Linked Data is spurring the deployment of applications that use the RDF data model at the information tier.
 In addition to querying RDF data.
The advent of native mobile technologies particularly advances in the Mobile Operating Systems (OS).
The advent of network functions virtualization (NFV) has revolutionized numerous network-based applications due to its several benefits such as flexibility.
The Aegilops kotschyi thermo-sensitive cytoplasmic male sterility (K-TCMS) system may facilitate hybrid wheat (Triticum aestivum L.
) seed multiplication and production.
 The K-TCMS line is completely male sterile during the normal wheat-growing season.
The agricultural research community offers languages and approaches to model farmers' decision-making processes but does not often clearly detail the steps necessary to build an agent model underlying farmers' decision-making processes.
 We propose an original and readily applicable methodology for modelers to guide data acquisition and analysis.
The aim is an experimental research on the flat objects recognition by linear discriminant analysis.
The aim is the development of a new multi-level model of fuzzy semi-structured information data storage.
 A distinction of the proposed model from known is the use of extended polybasic intuitionistic sets for description of fuzzy data and representation of fuzzy attributes on three typing levels.
 The main result is formal description of fuzzy semi-structured data storage.
 To verify the result an example of application the developed model of data storage in the intellectual diagnostic decision-making system for railway transport is showed.
The aim of computational molecular design is the identification of promising hypothetical molecules with a predefined set of desired properties.
 We address the issue of accelerating the material discovery with state-of-the-art machine learning techniques.
 The method involves two different types of prediction; the forward and backward predictions.
 The objective of the forward prediction is to create a set of machine learning models on various properties of a given molecule.
 Inverting the trained forward models through Bayes' law.
The aim of the chemotherapeutic regimens (CHR) digitalization project is the proposal of a universal structure and creation of a publicly accessible database of contemporary CHR as a universal utility for the communication and evaluation of contemporary and newly defined clinical schedules in anti-tumor chemotherapy.
 After analysis of contemporary anti tumor CHR a standard XML structure was proposed.
The aim of the current study is to map landslide susceptibility over the Ziarat watershed in the Golestan Province.
The aim of the present study is to calculate the process of detonation combustion of gas mixtures in engines.
 Development and verification of 3D transient mathematical model of chemically reacting gas mixture flows incorporating hydrogen was performed.
 Development of a computational model based on the mathematical one for parallel computing on supercomputers incorporating CPU and GPU units was carried out.
 Investigation of the influence of computational grid size on simulation precision and computational speed was performed.
 Investigation of calculation runtime acceleration was carried out subject to variable number of parallel threads on different architectures and implying different strategies of parallel computation.
The aim of the present study is to extend the (G'/G)-expansion method to fractional differential-difference equations of rational type.
 Particular time-fractional models are considered to show the strength of the method.
 Three types of exact solutions are observed: hyperbolic.
The aim of the present study was to investigate the possible pathogenesis of osteosarcoma using bioinformatics analysis to examine gene-gene interactions.
 A total of three datasets associated with osteosarcoma were downloaded from the Gene Expression Omnibus.
 The differentially expressed genes (DEGs) were identified using the significance analysis of microarrays method.
The aim of the presented study is to assess the fractal dimension (D) and the geometrical characteristics (length and width) of the landslides identified in North of Tehran.
The aim of the publication is to brief on a safe and cost-effective security solution cost and time estimation tool for small to medium size enterprises since most of these either do not have a budget for cyber security or are miniscule in general.
 Evaluation tool will be used to determine approximate time and costs a small company might face implementing a comprehensive security solution.
 Analyzing ISO/IEC 27002:2013 standards.
The aim of the study was to present the effect of Lego Mindstorms Ev3 based design activities on students' attitudes towards learning computer programming.
The aim of the study was to show that the presence of a small quantity of konjac glucomannan (KGM) in potato starch suspension increased the stability of carvacrol trapping.
 For that purpose.
The aim of this article is the design and implementation of intelligent irrigation devices using the Arduino microcontroller.
 The control interface of the irrigation system was created as a mobile application.
 The currently existing various similar solutions are all from the perspective of money unprofitable.
 The present solution is very cheap and effective especially for home usage.
 Following the extension it can also be applied on a larger scale e.
 for buildings greenhouse.
The aim of this paper is to demonstrate the applicability and the effectiveness of a computationally demanding stereo-matching algorithm in different low-cost and low-complexity embedded devices.
The aim of this paper is to develop software for designing of steel reinforced elastomeric isolator (SREI) according to American Association for State Highway and Transportation Officials Load and Resistance Factor Design (AASHTO LRFD) Specifications.
 SREI is used for almost all bridge types and special structures.
 SREI-structures interface defines support boundary conditions and may affect the seismic performance of bridges.
 Seismic performance of the bridge is also affected by geometrical and materials properties of SREI.
 The selection of SREI is complicated process includes satisfying all the design constraints arising from code provisions and maximizing performance at the lowest possible cost.
 In this paper.
The aim of this paper is to present a development and an implementation of a tracking algorithm for Over-The-Horizon Radar (OTHR) with surface wave propagation.
 Proposed implementation utilizes standard tracking algorithm design for OTHR and introduces modifications of Joint Probability Data Association (JPDA) algorithm in order to simplify processing.
 To justify chosen approach.
The aim of this paper is to present a mobile agents model for distributed classification of Big Data.
 The great challenge is to optimize the communication costs between the processing elements (PEs) in the parallel and distributed computational models by the way to ensure the scalability and the efficiency of this method.
 Additionally.
The aim of this paper is to show that well known SPARSKIT SpMV routines for Ellpack-Itpack and Jagged Diagonal formats can be easily and successfully adapted to a hybrid GPU-accelerated computer environment using OpenACC.
 We formulate general guidelines for simple steps that should be done to transform source codes with irregular data access into efficient OpenACC programs.
 We also advise how to improve the performance of such programs by tuning data structures to utilize hardware properties of GPUs.
 Numerical experiments show that our accelerated versions of SPARSKIT SpMV routines achieve the performance comparable with the performance of the corresponding CUSPARSE routines optimized by NVIDIA.
The aim of this study is to assess the performance of a computer-aided semi-automated algorithm we have adapted for the purpose of segmenting malignant pleural mesothelioma (MPM) on CT.
 Forty-five CT scans were collected from 15 patients (M:F 10:5.
The aim of this study was to design a multiepitope universal vaccine for major human papillomavirus (HPV) structural proteins.
The aim of this work is to develop a software tool that provides a user-friendly graphical interface of generic and configurable design in order to ease the researcher's tasks in data manipulation and analysis.
 Two primary targets have influenced the design: data preparation for analysis in animal breeding programs and usability increase for those applications that perform important analysis methods.
The aim of this work is to propose a methodology that seeks to discover how.
The aim of this work was to check the possibilities of usage the beacons to find out the relations between the members of cattle herd.
 Relations between animals are important because of animals welfare and can influence on animal health and productivity.
 In the early stage of the research.
The algebraic and algorithmic study of integro-differential algebras and operators has only started in the past decade.
 Integro-differential operators allow us in particular to study initial value and boundary problems for linear ODEs from an algebraic point of view.
 Differential operators already provide a rich algebraic structure with a wealth of results and algorithmic methods.
 Adding integral operators and evaluations.
The algorithm for the symbolic computation of the Aluthge transform of a polynomial matrix is derived in this paper.
 For this purpose.
The algorithms of exposure of signals of unknown form are offered in multidimensional time series with subsequent presentation of separate time series from every mining hole as points multidimensional space of parameters of these mining holes.
 By means of facilities of cognitive machine graphic arts these mining holes form kappa cognitive characters in that evidently as aberrant behavior of parameters of these mining holes can be educed in case of multidimensional terabyte of time series.
The all area of security can be hot issue of computer and technology.
 In the past years.
The all-optical linear array with a reconfigurable pipelined bus system (OLARPBS) optical conduit parallel computing model consists of pipelined optical conduits (buses) that interconnect all-optical processing elements.
 Previous work on the OLARPBS.
The alternative splicing pattern of transcription units can be influenced by the genotype of a neighboring locus.
The amount of data collected from different real-world applications is increasing rapidly.
 When the volume of data is too large to be loaded to memory.
The amount of remote sensing (RS) data available for applications is constantly growing due to the rise of very high resolution sensors and short-repeat-cycle satellites.
 Consequently.
The analysis of retina blood vessels in clinics indices is one of the most efficient methods employed for diagnosing diseases such as diabetes.
The analysis of several algorithms and data structures can be framed as a peeling process on a random graph: vertices with degree less than k and their adjacent edges are removed until no vertices of degree less than k are left.
 Often the question is whether the remaining graph.
The analysis of software architecture plays an important role in understanding the system structures and facilitate proper implementation of user requirements.
 Despite its importance in the software engineering practice.
The analysis of travel mode choice is an important task in transportation planning and policy making in order to understand and predict travel demands.
 While advances in machine learning have led to numerous powerful classifiers.
The anonymity of Bitcoin prevents analysis of its users.
 We collect Google Trends data to examine determinants of interest in Bitcoin.
 Based on anecdotal evidence regarding Bitcoin users.
The antibiotics resistance phenomena of Aeromonas hydrophila has become serious economic and public health problems for the world aquaculture industry and human health care.
 In this study.
The apicomplexan parasite Sarcocystis neurona causes equine protozoal myeloencephalitis (EPM).
The application and development of computer technology have driven and promoted the change of information technology.
 The computer and information technology have a wide range of penetration and affinity.
The application of synchronization theory to build new cryptosystems has been a hot topic during the last two decades.
 In this paper.
The architecture and implemented a software system parsing schemes of algebraic algebra and their interpretation.
 The software system allows to generate automated in such schemes create a program and debug the relevant schemes.
 A clear division of algebraic algebra into separate modules is defined.
The architecture of access control system for user jobs access to computational resources of grid distributed computing networks.
The army's combat training is very important now.
The arrival of the information age has gradually expanded the practical application of computer networks.
The art of quantum algorithm design is highly nontrivial.
 Grover's search algorithm constitutes a masterpiece of quantum computational software.
 In this article.
The article concerns the problem of detecting masqueraders in computer systems.
 A masquerader in a computer system is an intruder who pretends to be a legitimate user in order to gain access to protected resources.
 The article presents an intrusion detection method based on a fuzzy approach.
 Two types of user's activity profiles are proposed along with the corresponding data structures.
 The solution analyzes the activity of the computer user in a relatively short period of time.
The article deals with semantic data model and its application in data integration.
 It analyses the issue of mapping database schemes (particularly relational models) for common data models expressed in the form of ontology.
 Substantial part is dedicated to methods of acquiring ontology from relational databases.
The article describes efficient methods to visualize the results from finite element analysis and implementation of these methods in post-processing results.
 The work is based on premise that computer memory and performance are limited and amount of data processed by complex finite element analysis is enormous.
The article presents a generalization of the group mutual exclusion (GME) problem.
The article presents an innovative concept of applying graph databases in transport information systems.
 The model of a graph database has been presented together with implementation of data structures and search operations in a graph.
 The transformation concept of relational model to a graph data model has been developed.
 The schema of graph database has been proposed for public transport information system purposes.
 The realization methods have been illustrated by the use of search function based on the Cypher query language.
The article presents the description of a cloud based eLearning platform capable to support the implementation of the so called body syntonic reasoning.
 It summarizes a research proposal that has been put together by a number of players in the field of e-learning.
 The architecture is the result of a research initiative.
 started recently at the Bucharest University of Economic Studies.
 The aim of the initiative is to create a state-of-the-art eLearning platform that can be used for teaching computer programming to children and students.
 By using elements of body-syntonic reasoning.
The ASDEX Upgrade Discharge Control System (DCS) is a distributed real-time control system executing complex control and monitoring tasks.
The aspect ratio of a plot can strongly influence the perception of trends in the data.
 Arc length based aspect ratio selection (AL) has demonstrated many empirical advantages over previous methods.
The asymptotic property of the maximum likelihood estimator (MLE) has been utilized to reconstruct threedimensional (3D) sectional images in the photon counting imaging (PCI) regime.
The ATLAS experiment at the LHC collects billions of events each data-taking year.
The ATLAS experiment used for many years a large database infrastructure based on Oracle to store several different types of non-event data: time-dependent detector configuration and conditions data.
The attenuation effect can influence the precision of inversion of Earth's physical properties when solving geophysical inverse problems utilizing seismic data.
 Therefore using the viscoelastic acoustic wave equation is more suitable for the actual situation.
 In this paper.
The audio-to-score framework consists of two separate stages: preprocessing and alignment.
 The alignment is commonly solved through offline dynamic time warping (DTW).
The author analyzes the process of formation and development of research in the field of cybernetics.
 The 1950-1960s became a flourishing era for computer technology and the highest achievements in the scientific field of cybernetics in the USSR.
 Siberian Physical-Technical Institute (SPhTI) at Tomsk State University (TSU) became the Center of the formation and development of research in the field of cybernetics in the 1950s.
 in the east of the USSR.
 In the early 1950s at the department of radiophysics of TSU the associate professors F.
P Tarasenko.
The authors describe a method for producing Boolean functions of degree d3 in n=2dk-1 (k=1.
The authors present a comprehensive overview of image processing and analysis work done to support research into the model flowering plant Arabidopsis thaliana.
 Beside the plant's importance in biological research.
The authors present a protocol and security architecture for the smart grid in an advanced metering infrastructure (AMI).
 Various levels of the hierarchical grid are isolated in protocol planes for preventing propagation of attacks.
 The utility meters are interconnected via the wireless technologies in order for metering functions.
The authors propose a generalised multi-layer architecture for the phase shifter network (PSN) in hybrid analogue/digital beamforming (BF).
The authors proposed a new method to automatically mesh computed tomography (CT)-based threedimensional human airway geometry for computational fluid dynamics (CFD)-based simulations of pulmonary gas-flow and aerosol delivery.
 Traditional methods to construct and mesh realistic geometry were time-consuming.
The authors show that two well-known and widely employed public-key encryption schemes - RSA optimal asymmetric encryption padding (RSA-OAEP) and Diffie-Hellman integrated encryption scheme (DHIES).
The automated identification of markers in medical imaging is one of the issues proposed in recent years for many tasks with wide application in digital image analysis.
The automatic generation of procedures for combinatorial optimization problems is emerging as a new alternative to address the hardest problems of this class.
 One of these problems still offering great computational difficulty is the traveling salesman problem.
 Its simple presentation masks the great difficulty that exists when solving it numerically.
 The results obtained so far for this problem are based on the hybridization of known heuristics.
The automatic training of agent-based simulators can be a complex task because of (a) their common nondeterministic behavior and (b) their complex relationships between their input parameters and the outputs.
 This work presents a technique called ATABS for automatically training agent-based simulators.
 This technique is based on a novel mechanism for generating random numbers that reduces the variability of the global results.
 This work provides a framework that automates this training by considering the relationships between the simulation parameters and the output features.
 This technique and framework have been applied to automatically train two different simulators.
 The current approach has been empirically compared with the most similar alternative.
 The results show that ATABS outperforms this alternative considering (1) the similarity between simulated and real data and (2) the execution time in the training process.
 The ATABS framework is publicly available.
 In this way.
The automation of agricultural mapping using satellite-derived remotely sensed data remains a challenge in Africa because of the heterogeneous and fragmental landscape.
The auxiliary equation method presents wide applicability to handling nonlinear wave equations.
 In this article.
The availability is an important issue of software-defined networking (SDN).
 In this paper.
The availability of continental and global-scale spatio-temporal geographical data sets and the requirement to efficiently process.
The availability of heterogeneous CPU+GPU systems has opened the door to new opportunities for the development of parallel solutions to tackle complex biological problems.
 The reconstruction of evolutionary histories among species represents a grand computational challenge.
The availability of medical imaging data from clinical archives.
The background for the proposed vibration isolation method is based on the principle of active pressure control.
The backpressure scheduling algorithm for multihop wireless networks is known to be throughput optimal.
The basic model for processing packets and routing them from one hardware system to another can be extended to describe virtual packet transmission from one module to another within a single program image.
 Representing virtual and physical ports interchangeably and embodying their routing relations in runtime data structures makes it possible to produce object modules that can be deployed in a variety of roles without recompilation.
 We describe a fielded system that combines (a) application virtual ports.
The behaviors of the keyhole and the weld pool are dynamically-coupled in controlled-pulse plasma arc welding and can be used to indicate weld quality.
 The vision system was improved to detect the geometries of both the keyhole and weld pool at the backside simultaneously during a whole controlled-pulse keyholing period by one single CCD camera without auxiliary illumination.
 With the assistance of an appropriate optical filter system.
The Belle II experiment will start physics data taking in 2016 at the SuperKEKB factory to search for new physics beyond the Standard Model from flavor physics channels.
 The Belle II data acquisition (DAQ) system consists of a large number of subsystems to handle the high trigger rate and huge data bandwidth.
The benefits of Cloud computing include reduced costs.
The benefits of using e-Infrastructure environments.
The Bernoulli and Poisson processes are two popular discrete count processes; however.
The BES-III experiment at the Institute of High Energy Physics (Beijing.
The bidirectional reflectance distribution function (BRDF) is one of the fundamental concepts in such diverse fields as multidimensional reflectometry.
The bilateral filter is an edge-preserving smoother that has diverse applications in image processing.
The binary Euclidean algorithm is a modification of the classical Euclidean algorithm for computation of greatest common divisors which avoids ordinary integer division in favour of division by powers of two only.
 The expectation of the number of steps taken by the binary Euclidean algorithm when applied to pairs of integers of bounded size was first investigated by R.
 Brent in 1976 via a heuristic model of the algorithm as a random dynamical system.
 Based on numerical investigations of the expectation of the associated Ruelle transfer operator.
The blending effect of design pattern of five-surface plane developed drawing is important to the design pattern.
 The blending of three-dimensional pattern design can realize the automation of pattern design.
 The target pattern design of air brushing is based on the square design picture.
The bootstrap is a popular and powerful method for assessing precision of estimators and inferential methods.
The Brazilian Public Software Portal (SPB) provides a social and scientific service through a software repository.
 Given the importance of this service improvement opportunities are observed through proposals to assess the quality of the software residing on the portal.
The burgeoning volume of torrential data continues to grow exponentially in this very age of the Internet of Things.
 As this torrent of digital datasets continue to outgrow in datacenters.
The business of manufacturing companies is evolving towards knowledge-intensive industrial services.
 It is important to stay as near to the customer process as possible in this evolution to enable the growth of efficiency of customer business over the life cycle.
 In this type of business.
The campus website is a comprehensive information platform of campus informatization.
 When using the algorithm design of campus information system.
The carbide drill is an important hole-machining tool.
 Modern processing solutions set higher requirements for the accuracy and efficiency of carbide drill manufacturing.
 This paper presents a detailed study of mathematical models of the spiral groove.
The Cave Automatic Virtual Environment (CAVE) is a fully immersive Virtual Reality (VR) system.
 CAVE systems have been widely used in many applications.
The cell surface hydrophobicity (CSH) is an assessable physicochemical property used to evaluate the microbial adhesion to the surface of biomaterials.
The challenge in detecting explosive hazards is that there are multiple types of targets buried at different depths in a highly-cluttered environment.
 A wide array of target and clutter signatures exist.
The challenge raised by the introduction of Internet of Things (IoT) concept will permanently shape the networking and communications landscape and will therefore have a significant social impact.
 The ongoing IoT research activities are directed towards the definition and design of open architectures and standards.
The chaotic map has complex dynamics under ideal conditions however it suffers from the problem of performance degradation in the case of finite computing precision.
 In order to prevent the dynamics degradation.
The characterisation of fatigue damage evolution in constrained glass fibre reinforced plastic off-axis laminates is presented.
 A newly developed imaging technique known as Automatic Crack Counting (ACC) is used to quantify the off -axis crack state in constant amplitude (CA) and variable amplitude (VA) block loading tension-tension fatigue tests and constant amplitude compression-tension tests.
 The quantified crack states are analysed by combining the newly developed ACC method with a data mining approach and applying these to large data sets obtained during fatigue tests.
 It is shown that for a constant stress level.
The characterization of nonlinear dynamical systems and their attractors in terms of invariant measures.
The Circle Hough Transform (CHT) is one of the popular circle detection algorithm in image processing and machine vision application.
The circularly orthogonal moments have been widely used in many computer vision applications.
 Unfortunately.
The classic way of building a software today simplistically consists in connecting a piece of code calling a method with the piece of code implementing that method.
 We consider these piece of code (software systems) not calling anything.
The classical objective of obfuscation considers indistinguishability of the obtained code in relation to original programs of equal functionality.
 The present paper reviews another objective of obfuscation.
The cloud can be used to outsource data storage or data computation.
 Data computation outsourcing enables to move computationally expensive operations outside a mobile device.
 Many pairing-based cryptographic schemes are designed to enable documents' encryption while fulfilling some defined security requirements.
 In practice.
The cloud computing environment has expanded considerably with the rapid advancement of related technologies.
 Although cloud computing is convenient for users.
The cloud radio access network (Cloud-RAN) has recently been proposed as one of the cost-effective and energy efficient techniques for 56 wireless networks.
 By moving the signal processing functionality to a single baseband unit (BBU) pool.
The coalition structure generation problem is well known in the area of multi-agent systems.
 Its goal is to establish coalitions between agents while maximizing the global welfare.
 Among the existing different algorithms designed to solve the coalition structure generation problem.
The combination of Petri net (PN) modeling with Al-based heuristic search (HS) algorithms (PNHS) has been successfully applied as an integrated approach to deal with scheduling problems that can be transformed into a search problem in the reachability graph.
 While several efficient HS algorithms have been proposed albeit using timed PN.
The commonly used inhalation anesthetic.
The comparability of grain sizes emerging from different methods are discussed.
The comparison of sets of genome intervals (e.
The complete binary tree as an important network structure has long been investigated for parallel and distributed computing.
The complete binary tree is an important network structure for parallel and distributed computing.
The Complex Span paradigm is one of the most influential and widely used instruments for measuring working memory capacity (WMC).
 We report the results of four experiments designed to explore the feasibility of obtaining valid estimates of WMC online.
 We explored the relationships between the Complex Span tasks and fluid intelligence (gF) in the lab and on the web using a new platform called the Online Working Memory Lab (the OWL).
 The OWL is universally accessible across all computer operating systems and functions in both local and remote contexts.
The complex structure of relational data makes the process of knowledge discovery from data a more challenging task compared with the single table data structure.
 The usefulness of granular computing based approaches to mining data stored in a single table is a driving force for adapting this method to relational data.
 This paper proposes relation-based granules that are defined in a granular computing based approach to mining relational data.
 The relations are used to represent relational data and patterns to be discovered.
 Thanks to this representation.
The complexity increase in the software and hardware necessary to support more and more advanced applications for Wireless Sensor Networks conspicuously contribute to render them susceptible to security attacks.
 The nodes of most complex WSN applications sport desktop-level operating systems and this reliance on software make them ideal prey for traditional threats.
The complexity of data and methods in industrial ecology (IE) keeps growing.
The complexity of humanoid robots is increasing with the availability of new sensors.
The computational demand of exact-search procedures has pressed the exploitation of parallel processing accelerators to reduce the execution time of many applications.
The computer graphics and computer vision communities have been working closely together in recent years.
The computer input/output (I/O) subsystem and its functioning are very abstract concepts that are difficult for undergraduate freshmen to understand.
The computer technology has advanced profoundly that the application seems to have no limit.
 Equipped with programming.
The concatenated Greenberger-Horne-Zeilinger (C-GHZ) state is a new type of multipartite entangled state.
The concept of a mapping.
The concept of autonomous farming concerns automatic agricultural machines operating safely and efficiently without human intervention.
 In order to ensure safe autonomous operation.
The concept of homogeneous integrity constraints in database systems is introduced.
 The metric space for the state of database schemes with homogeneous integrity constraints is established.
 It is shown that many common integrity constraints that are used in practice satisfy the condition of homogeneity of integrity constraints.
 Such constraints include key.
The concept of the data structure is part of the accepted and relatively unexplored background of the information disciplines.
The concept of uncertain pattern mining was recently proposed to fulfill the demand for processing databases with uncertain data.
The conception and deployment of cost effective Picture Archiving and Communication Systems (PACS) is a concern for small to medium medical imaging facilities.
The connected dominating set (CDS) problem is a well studied NP-hard problem with many important applications.
 Dorn et al.
 (Algorithmica 58:790-810 2010) introduce a branch-decomposition based algorithm design technique for NP-hard problems in planar graphs and give an algorithm (DPBF algorithm) which solves the planar CDS problem in time and time.
The construction of large software systems is always achieved through assembly of independently written components - program modules.
 For these software components to work together.
The contact structure of asphalt mixtures has been considered as an important micromechanical mixture property related to the rutting performance.
 In this study.
The contemporary powerful mathematical software enables a new approach to handling and manipulating complex mathematical expressions and other mathematical objects.
 Particularly.
The continually increasing size of geospatial data sets poses a computational challenge when conducting interactive visual analytics using conventional desktop-based visualization tools.
 In recent decades.
The continuous auxiliary inputs leakage is more strong side-channel attacks.
 In this article.
The controller area network with flexible data rate (CAN-FD) is attracting attention as the next generation of in-vehicle network technology.
The conventional wisdom is that aggressive networking requirements.
The convertible authentication encryption (CAE) scheme.
The core aspect of this work is a review for player detection and tracing.
 Coaches and players prepare widely by studying the opponents attacking and self-protecting formations.
The correlation process in a GNSS receiver tracking module can be computationally prohibitive if it is executed on a central processing unit (CPU) using single-instruction single-data algorithms.
 An efficient replacement for a CPU is a graphics processing unit (GPU).
 A GPU is composed of massive parallel processors with high floating point performance and memory bandwidth.
 It can be used to accelerate the burdensome correlation process in GNSS software receivers.
 We propose a novel GPU-based correlator architecture for GNSS software receivers.
The coupled cubic-quintic nonlinear schrodinger equations in fiber optics.
The coupled equations for the incoherent optical spatial solitons in a nonlocal nonlinear medium is studied analytically.
 With the soliton solutions hereby obtained via the symbolic computation.
The courses of computer programming language are important basic specialty courses for majors of science and technology in universities.
 These courses are often both highly abstract and practical.
The creation of a network for an organization or a firm can be a multifarious task especially if there a large number of nodes.
The crowdsourcing.
The crystallographic properties of fluorapatite (FAp) and polytetrafluoroethylene (PTFE) as biomedical materials were compared.
 Both materials contain fluorine and casually belong to the hexagonal crystal system.
 It is interesting that FAp is an inorganic ionic crystal.
The current control software for the Brewer Ozone Spectrophotometer was created over 30 years ago for MS-DOS computers and has poor compatibility with modern software and hardware platforms.
 The new cross-platform software for controlling the Brewer is able to run on actual operating system for computers.
The current development in the area of computer graphics focuses itself on 3D visualization.
 This direction.
The current Internet is facing unprecedented demands.
The current methods of measuring screw threads are either time consuming or expensive.
 In addition.
The current paper describes a hybrid control algorithm for fuel consumption minimization of a compound hybrid excavator.
 The power train of the excavator integrates an engine assist motor.
The current rate of success in launching satellites and advances in onboard and ground image processing have led to a dramatic increase in the scale of remote sensing image data.
 This has resulted in considerable research on how to provide the best quality of experience to end users.
The current rural medical information construction is still imperfect.
 Large amount of medical data related to common rural disease is used at very low utilization rate.
 In order to improve the utilization of these data and to provide help for rural barefoot doctors and rural residents.
The current spaceborne synthetic aperture radar systems are operated to illuminate the scene along the satellite flying direction.
The current study aimed to devise eco-friendly.
The Dalvik virtual machine (VM) is an integral component used to execute applications in Android.
The Data hiding concept such as steganography can provide security and facilitate the safe exchange of important information in the digital world.
 This concept can be used with the various multimedia such as text.
the data in computational domain stored in digital format.
 This format of data.
The data structure course is important basic course of computer programming.
The debate about representation in the brain and the nature of the cognitive system has been going on for decades now.
 This paper examines the neurophysiological evidence.
The decreasing cost and size of video sensors has led to camera networks becoming pervasive in our lives.
The deficiency in rapid and in-field detection methods and portable devices that are reliable.
The definition and description of student success programs in the literature (e.
The depletion of the global IPv4 address pool made the deployment of IPv6.
The deployment of smart grids and renewable energy dispatch centers motivates the development of forecasting techniques that take advantage of near real-time measurements collected from geographically distributed sensors.
 This paper describes a forecasting methodology that explores a set of different sparse structures for the vector autoregression (VAR) model using the least absolute shrinkage and selection operator (LASSO) framework.
 The alternating direction method of multipliers is applied to fit the different LASSO-VAR variants and create a scalable forecasting method supported by parallel computing and fast convergence.
The deployment of technology across the globe towards efficient learning environments is growing rapidly.
 In the United Kingdom.
The deployments of technology across the global toward efficient learning environment are growing rapidly.
 In United Kingdom educational system.
The depth ordering of two surfaces.
The design and test of Multi-Processor System-on-Chips (MPSoCs) and development of distributed applications and/or operating systems executed on those hardware platforms is one of the biggest challenges in today's system design.
 This applies in particular when short time-to-market constraints impose serious limitations on the exploration of the design space.
 The use of virtual platforms can help in decreasing the development and test cycles.
 In this paper.
The design and utilization of Electronic Process Guides (EPGs) have been studied in Software Engineering (SwE) since the 1990s.
The design of an integrated coastal berths and entrance-channel system is a complex challenge.
The design of computer-assisted decision (CAD) systems for different biomedical imaging scenarios is a challenging task in computer vision.
The design of mechatronic systems is based on the integration of several disciplines.
The design of multi-dimensional control system for three-dimensional warehouse including the warehouse temperature and h umidity monitoring system design and anti-theft system design.
 Through the control design for the warehouse.
The design of the data storage abandons the abuse of traditional database low query efficiency.
 not easy to maintain.
 long development cycle of disadvantage.
The detection of negative emotions through daily activities such as writing and drawing is useful for promoting wellbeing.
 The spread of human-machine interfaces such as tablets makes the collection of handwriting and drawing samples easier.
 In this context.
The detection of tunnel lining cracks is a very key procedure in the inspection of tunnels.
 Traditional image-processing approaches are commonly based on the characteristic that the grayscale value of the crack is a local minimum.
The determination of network equipment weaknesses and the discovery of intrusion intention is one of the difficulties that troubled network security management personnel.
 Based on previous studies.
The development of a humoral immune response to influenza vaccines occurs on a multisystems level.
 Due to the orchestration required for robust immune responses when multiple genes and their regulatory components across multiple cell types are involved.
The development of automated morphological classification schemes can successfully distinguish between morphological types of galaxies and can be used for studies of the formation and subsequent evolution of galaxies in our universe.
 In this paper.
The development of computational thinking skills through computer programming is a major topic in education.
The development of interactive e-learning content requires special skills like programming techniques.
The development of mobile computing.
The development of nanotechnology gives new possibilities for fabrication of different x-ray optical elements.
 We present results of focusing properties the compound silicon linear Zone Plate (ZP) for first and second orders.
 The compound silicon linear ZP is fabricated by an electron beam lithography and lift-off technology.
 ZPs structures have been etched by ion-plasma up to 6 mu m deep.
 A linear ZP of the first and second orders fabricated for x-ray radiation 10kev energy.
The development of network technology has brought convenience to people's life.
The development of smart grid and the increasing scale of power system brings more and more pressure to the conventional power system simulators.
 The graphic processing unit which features the massive concurrent threads and excellent floating point performance brings a new chance to the area of power system simulation.
 This paper introduces a hierarchical parallel LU decomposition algorithm based on stratified elimination tree.
The development of software-intensive technical systems involves several engineering disciplines like mechanical.
The development of technologies nowadays is moving in such speed that the computer graphics and simulations are being convincingly displaced by the augmented reality.
 The AR Sandbox.
The development of wireless Vehicular Ad-Hoc Network (VANET) aimed to enhance road's safety and provide comfortable driving environment by delivering early warning and infotainment messages.
 Intentional jamming attacks target at undermining such a goal by disrupting wireless communications.
 While detecting jamming attacks is important towards enhancing road safety.
The Digital Dollhouse we proposed enhanced traditional psychological play therapy with digital sensors and computer graphics combined together.
 It obtains significant difference in reaction of children that compared with normal dollhouse.
 We proposed the approach of connecting concrete object touching (Haptic).
The digital reconstruction of single neurons from 3D confocal microscopic images is an important tool for understanding the neuron morphology and function.
 However the accurate automatic neuron reconstruction remains a challenging task due to the varying image quality and the complexity in the neuronal arborisation.
 Targeting the common challenges of neuron tracing.
The discovery of noncoding RNAs (ncRNAs) and their importance for gene regulation led us to develop bioinformatics tools to pursue the discovery of novel ncRNAs.
 Finding ncRNAs de novo is challenging.
The disjunctive normal form and conjunctive normal form of Boolean functions are very important to construct logic formulas in classical logic system.
 Shannon expansion in symbolic computation tree logic is generalized to prove the normal form of Boolean functions.
 In n-valued Lukasiewicz logic system L-n.
The dispersive water wave system is explored by the (G'/G) -expansion method.
 Many new solutions are obtained with the aid of this method.
 These solutions include hyperbolic function solutions.
The divergence in the computer architecture landscape has resulted in different architectures being considered mainstream at the same time.
 For application and algorithm developers.
The DnaJ proteins which function as molecular chaperone played critical roles in plant growth and development and response to heat stress (HS) and also called heat shock protein 40 based on molecular weight.
The domain name system (DNS) is a core Internet infrastructure that translates names to machine-readable information.
The domains of parallel and distributed computing have been converging continuously up to the degree that state-of-the-art server computer systems incorporate characteristics from both domains: They comprise a hierarchy of enclosures.
The dynamic economic dispatch problem is a high-dimensional complex constrained optimization problem that determines the optimal generation from a number of generating units by minimizing the fuel cost.
 Over the last few decades.
The dynamics of belief and knowledge is one of the major components of any autonomous system that should be able to incorporate new pieces of information.
 In order to apply the rationality result of belief dynamics theory to various practical problems.
The earlier remote password authenticationschemes required a service providing server to authenticate a legitimate userfor remote login.
The economic situation highlights the significant role of the application of computer in human society since the 21st century.
The educational landscape continues to transform significantly in a way that it engages students' interest and improves learning processes.
 Such an example is the gradual use of educational games.
The effectiveness of five global thresholding techniques.
The effectiveness of in situ remediation to treat contaminated aquifers is limited by the degree of contact between the injected treatment chemical and the groundwater contaminant.
 In this study.
The effectiveness of the convolutional neural network (CNN) has already been demonstrated in many challenging tasks of computer vision.
The effects of target emissivity on apparent thermal contrast as well as on detection range capabilities of thermal imagers in long wave infrared and middle wave infrared bands were evaluated.
 The apparent thermal contrast (to be seen by the thermal imager at standoff distance).
The effort and cost required to convert satellite Earth Observation (EO) data into meaningful geophysical variables has prevented the systematic analysis of all available observations.
 To overcome these problems.
The effort to develop a Digital Earth has made dramatic progress in terms of visualisation and visual data integration for use-cases which demand semantically rich analysis.
 To provide this analysis and ensure legitimate representations of the spatial data from which visualisation are derived.
The elastodynamic boundary integral equation method (BIEM) in real space and in the temporal domain is an accurate semi-analytical tool to investigate the earthquake rupture dynamics on non-planar faults.
The electrical properties of organic field-effect transistors (OFETs) are usually characterized by applying models initially developed for inorganic-based devices.
The electrification of sectors such as heating and transportation represents a challenge as well as an opportunity for distribution system operators.
 On the one hand.
The emergence of hardware multithread (HW-MT) architectures increased the performance of MT applications.
The emergence of the cloud computing paradigm has greatly enabled innovative service models.
The emerging use of multi-homed wireless devices along with simultaneous multi-path data transfer offers tremendous potentials to improve the capacity of multi-hop wireless networks.
 The use of simultaneous data transfer over separate disjoint paths in multi-hop wireless networks to increase network capacity is a less explored subject.
The emerging use of self-avatars for physical and motor rehabilitation leads to specific requirements for their real-time animation that combine properties from the fields of computer graphics and of biomechanics.
 We present a method for animating a self-avatar in real-time that allows for high-fidelity representation of whole-body kinematics using anatomical and reproducible bone-segment definition.
 The method requires little setup time and has low motion-to-photon latency.
The emerging use of Wireless Multimedia Sensor Networks (WMSNs) and communication facilities have increased the need for network security measures to protect different types of multimedia data either real time or non-real time during the transmission period.
 A lot of researchers are becoming more interested on secure WMSNs due to the broad application necessities which can be range over smart environment.
The eminent web-applications of today are data-intensive.
 The data generated is of the order of petabytes and zetabytes.
 Using relational databases for storing them only complicates the storage and retrieval in the DB and degradation of its performance.
 The big data explosion demanded the need for a more flexible.
The employment of embedded cameras in navigation and guidance of Unmanned Aerial Vehicles (UAV) has attracted the focus of many academic researches.
 In particular.
The energy consumption of computer systems has become an important economic and environmental issue.
 Many researchers have focused on the energy consumption of hardware.
The equality problem is usually one's first encounter with communication complexity and is one of the most fundamental problems in the field.
 Although its deterministic and randomized communication complexity were settled decades ago.
The equation-oriented (EO) approach is widely used for process simulation and optimization.
 Nevertheless.
The estimation of material properties is important for scene understanding.
The estimation of nutrient content of plants is considerably important in agricultural practices.
The estimation of space and time-dependent earthquake probabilities.
The Euler number of a binary image is an important topological feature for many image processing.
The evaluation and benchmarking of NoSQL databases is challenging due to the variety of query languages and also due to the denormalized scheme that allows to include non-scalar values and repeatable structures such as nested documents.
 In the rather young age of NoSQL databases.
The Event Index project consists in the development and deployment of a complete catalogue of events for experiments with large amounts of data.
The ever-increasing supercomputer architectural complexity emphasizes the need for high-level parallel programming paradigms.
 Among such paradigms.
The evolution of genes related to sex and reproduction in fish shows high plasticity and.
The ex vivo challenge assay is being increasingly used as an efficacy endpoint during early human clinical trials of HIV prevention treatments.
 There is no standard methodology for the ex vivo challenge assay.
The exact solution of the anti-symmetric quadratic truly nonlinear oscillator is derived from the first integral of the nonlinear differential equation which governs the behavior of this oscillator.
 This exact solution is expressed as a piecewise function including Jacobi elliptic cosine functions.
 The Fourier series expansion of the exact solution is also analyzed and its coefficients are computed numerically.
 We also show that these Fourier coefficients decrease rapidly and.
The excellent features of bearing vibration signal are helpful to obtain accurate diagnosis results for the failure of bearing.
 In this study.
The ex-core detector response calculation is an important part in reactor design.
The exemplar breakpoint distance problem is motivated by finding conserved sets of genes between two genomes.
 It asks to find respective exemplars in two genomes to minimize the breakpoint distance between them.
 If one genome has no repeated gene (called trivial genome) and the other has genes repeating at most twice.
The existence of large matter-antimatter asymmetry (CP violation) in the b-quark system as predicted in the Kobayashi-Maskawa theory was established by the B-Factory experiments.
The existence of matrix representations for any given finite-dimensional complex Lie algebra is a classic result on Lie Theory.
 In particular.
The existence of SpoU-TrmD (SPOUT) RNA methyltransferase superfamily was first predicted by bioinformatics.
 SpoU is the previous name of TrmH.
The existing complexities of teaching and learning computer programming are increased where students are diverse in their disciplinary backgrounds.
The existing literature about the backorder EOQ problem usually focuses on B2C (Business to-Customer) and neglects the impact of batch demands to the backlogging behavior in B2B (Business-to-Business) environment.
 This paper contributes by formulating the inventory problem for a wholesaler facing batch demands and using backorder to control her inventory.
 The wholesaler's non-differentiable average cost function encompasses a curtain like pattern.
 Based on its structural properties.
The existing railway wagon flow forecast method cannot meet the demand of the actual railway transport organization.
 The old system is embodied in the slow computing speed and low accuracy.
 The railway wagon flow forecast system based on Hadoop-Hazelcast is designed to improve the accuracy of the forecast.
The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives.
 Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources.
The Experimental Advanced Superconducting Tokamak (EAST) Device began operation in 2006.
 EAST visualization work has been paid more and more attention for simulating its running state and inner structure.
 The VEAST system had been developed to display the 3D model of EAST facility and some diagnostic data based on Java3D.
 Compared with the VEAST system.
The explosion of ever increasing geospatial data is today met with the challenge of maintaining it in spatial databases and utilization of traditional methods of spatial data processing.
 The sheer volume and complexity of spatial databases makes them an ideal candidate for use with parallel and distributed processing architectures.
 There is a lot of enthusiasm toward using MapReduce paradigm and distributed computing for processing of large volumes of vector data.
 As spatial data cannot be indexed using traditional B-tree structures used by R/DBMS.
The explosive growth of the location-enabled devices coupled with the increasing use of Internet services has led to an increasing awareness of the importance and usage of geospatial information in many applications.
 The mobile navigation apps (often called Maps).
The exponential random graph model (ERGM) is a well-established statistical approach to modelling social network data.
The expression levels of the protein tyrosine kinase Acki has been reported to be dysregulated in various cancers and involve in oncogenesis and progression.
The eXtensible Markup Language (XML) is a meta language that is widely used to provide a non-proprietary universal format for sharing hierarchical data among different software systems and application domains.
 Many organizations and content providers have been publishing and sharing their information through XML and its standard schemas.
 With the increased popularity of cloud application deployment.
The Extreme Learning Machine (ELM) and its variants are effective in many machine learning applications such as Imbalanced Learning (IL) or Big Data (BD) learning.
The face recognition applications are widely used in different fields like security and computer vision.
 The recognition process should be done in real time to take fast decisions.
 Principle Component Analysis (PCA) considered as feature extraction technique and is widely used in facial recognition applications by projecting images in new face space.
 PCA can reduce the dimensionality of the image.
The fact that technology have changed the lives of human beings cannot be denied.
 It has drastically reduced the effort needed to perform a particular task and has increased the productivity and efficiency.
 Computers especially have been playing a very important role in almost all fields in today's world.
 They are used to store large amount of data in almost all sectors.
The fact that the security facilities within a system are closely coupled and the security facilities between systems are unconnected results in an isolated protection structure for systems.
The FAdo system is a symbolic manipulator of formal language objects.
The failure detector approach for solving distributed computing problems has been celebrated for its modularity.
 This approach allows the construction of algorithms using abstract failure detection mechanisms.
The failure on all homogeneous devices due to the same reason is called homogeneous fault in networks.
 In contrast.
The fall is one of the most important research fields of solitary elder healthcare at home based on Internet of Things technology.
 Current studies mainly focus on the fall detection.
The fashion industry is one of the most flourishing fields for visual applications of IT.
 Due to the importance of the concept of look in fashion.
The fast marching method is a widely used numerical method for solving the Eikonal equation arising from a variety of scientific and engineering fields.
 It is long deemed inherently sequential and an efficient parallel algorithm applicable to large-scale practical applications is not available in the literature.
 In this study.
The fast retrieval in archival traffic data is essential for network security and forensic analysis.
 A bitmap index is a data structure enabling fast search over large data collections in a limited time.
The fast sweeping method is a popular algorithm for solving a variety of static Hamilton-Jacobi equations.
 Fast sweeping algorithms for parallel computing have been developed.
The fault cutting algorithm is important when applying the three-dimensional numerical manifold method to engineering simulation.
 This paper presents a primary approach to the fault cutting algorithm.
The field of parallel computing has experienced an increase in the number of computing nodes.
The fine periodic growth patterns on shell surfaces have been widely used for studies in the ecology and evolution of scallops.
 Modern X-ray CT scanners and digital cameras can provide high-resolution image data that contain abundant information such as the shell formation rate.
The Finite Element Method (FEM) is a common numerical technique used for solving Partial Differential Equations on large and unstructured domain geometries.
 Numerical methods for FEM typically use algorithms and data structures which exhibit an unstructured memory access pattern.
 This makes acceleration of FEM on Field-Programmable Gate Arrays using an efficient.
The Finnish Patient Data Repository is a nationwide electronic health record (EHR) system collecting patient data from all healthcare providers.
 The usefulness of the large amount of data stored in the system depends on the underlying data structures.
The first attempts to apply the k-induction method to software verification are only recent.
 In this paper.
The first part of the paper describes the mechanical structure of RTTRR small-sized modular robot.
 The second part is reserved for the determination of the geometric model of the robot.
The first quantum private comparison (QPC) protocol via cavity quantum electrodynamics (QED) is proposed in this paper by making full use of the evolution law of atom via cavity QED.
The first step of quantum measurement procedure is known as premeasurement.
The Flemish painters Quentin Massys and his son Jan Massys appear to be the authors of four works with a very similar motif.
The Flickr30k dataset has become a standard benchmark for sentence-based image description.
 This paper presents Flickr30k Entities.
The flood of real time social data.
The Fluid Implicit Particle method (FLIP) for liquid simulations uses particles to reduce numerical dissipation and provide important visual cues for events like complex splashes and small-scale features near the liquid surface.
 Unfortunately.
The flux distribution generated by the heliostat field of solar central receiver system (SCRS) over the receiver needs to be carefully controlled.
 It is necessary to avoid dangerous radiation peaks and temperature distributions to maximize the efficiency and keep the system in a safe state.
 These tasks imply both selecting the subset of heliostats to be activated and assigning each one to a certain aiming point at the receiver.
 The heliostat field is usually under human control and supervision.
The foreign real-time operating systems were widely used for the last years in Russia-developed aircraft as well as in the Russian Integrated Modular Avionics (IMA) research projects.
 But presently the question of their substitution with Russian analogues has arisen.
 This article provides the requirements of the operating system should satisfy in order to be used in the civil aviation.
The frequency transformation methods like fast fourier transform algorithms can be competently used in realization of discrete fourier transforms over Galois field.
The fully polarizable.
The fully polynomial-time approximation scheme (FPTAS) is a class of approximation algorithms for optimisation problems that is able to deliver an approximate solution within any chosen ratio in polynomial time.
 By generalising Bird and de Moor's Thinning Theorem to a property between three orderings.
The fully-functional succinct tree representation of Navarro and Sadakane (2014) [10] supports a large number of operations in constant time using 2n + o(n) bits.
The functional renormalisation group (fRG) has evolved into a versatile tool in condensed matter theory for studying important aspects of correlated electron systems.
 Practical applications of the method often involve a high numerical effort.
The Future Internet will integrate large-scale systems constructed from the composition of thousands of distributed services.
The gap between storing data in relational databases and transferring data in form of XML has been closed e.
 by SQL/XML queries that generate XML data out of relational data sources.
The generalized Hamming weights of a linear code have been extensively studied since Wei first use them to characterize the cryptography performance of a linear code over the wire-tap channel of type II.
 In this paper.
The geodesic curvature flow is an important concept in Riemannian geometry.
 The flow with level set formulation has many applications in image processing.
The global expansion of the aquaculture industry has brought with it a corresponding increase of novel viruses infecting different aquatic organisms.
 These emerging viral pathogens have proved to be a challenge to the use of traditional cell-cultures and immunoassays for identification of new viruses especially in situations where the novel viruses are unculturable and no antibodies exist for their identification.
 Viral metagenomics has the potential to identify novel viruses without prior knowledge of their genomic sequence data and may provide a solution for the study of unculturable viruses.
 This review provides a synopsis on the contribution of viral metagenomics to the discovery of viruses infecting different aquatic organisms as well as its potential role in viral diagnostics.
 High throughput Next Generation sequencing (NGS) and library construction used in metagenomic projects have simplified the task of generating complete viral genomes unlike the challenge faced in traditional methods that use multiple primers targeted at different segments and VPs to generate the entire genome of a novel virus.
 In terms of diagnostics.
The global skyline.
The global software development (GSD) paradigm has.
The goal of an intrusion detection system (IDS) is to monitor activities to detect breaches in security policies of a computer system or a network.
 This paper focuses on anomaly detection paradigm of IDS.
 The goal of anomaly-based IDS is to classify intrusion based on system and network activities outside of a normal region.
 In this paper we employ a multiple-detector set artificial immune system.
The goal of software testing should go beyond simply finding defects.
 Ultimately.
The goal of the article is to bridge the difference between theoretical and practical approaches to answering queries over databases with nulls.
 Theoretical research has long ago identified the notion of correctness of query answering over incomplete data: one needs to find certain answers.
The goal of this invited editorial is to introduce an IEEE-RITA special issue on trends in computers in education.
 The special issue integrates the thoroughly revised and extended versions of five outstanding papers presented at the 16th International Symposium on Computers in Education (SIIE'14).
The goal of this paper is to identify individuals by analyzing their gait.
 Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories.
 We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person.
The goal of this research was to investigate the effects of automated testing software on levels of student reflection and student performance.
 This was a self-selecting.
The goal of this study is to (i) understand the characteristics of high-.
The goal of this study is to propose a computer-aided diagnosis system to differentiate between four breast imaging reporting and data system (Bi-RADS) classes in digitised mammograms.
 This system is inspired by the approach of the doctor during the radiologic examination as it was agreed in BI-RADS.
The goal of this work was to develop an easy-to-use and engaging irrigation scheduling tool for cotton which operates on a smartphone platform.
 The model which drives the Cotton Smartlrrigation App (Cotton App) is an interactive ET-based soil water balance model.
 The Cotton App uses meteorological data from weather station networks.
The Gram-negative bacterium Riemerella anatipestifer is an important waterfowl pathogen.
The Graphics Processing Units (GPUs) have been used for accelerating graphic calculations as well as for developing more general devices.
 One of the most used parallel platforms is the Compute Unified Device Architecture (CUDA).
The GRAS gene family performs a variety of functions in plant growth and development processes.
The grasslands of Western Jilin Province in China have experienced severe degradation during the last 50 years.
 Radial basis function neural networks (RBFNN) and support vector machines (SVM) were used to predict the carbon.
The great number of heterogeneous interconnected operating systems gives greater access to intruders and makes it easier for malicious users to break systems security policy.
The growing computational power of modern CPUs allows increasingly complex signal processing applications to be successfully implemented and executed on general-purpose processors and operating systems.
 In this regard.
The growing number of scientific computation-intensive applications calls for an efficient utilization of large-scale.
The growing popularity of massively accessed Web applications that store and analyze large amounts of data.
The growth in large-scale data management capabilities and the successful care of patients with congenital heart defects have coincidentally paralleled each other for the last three decades.
The growth of energy consumption has been explosive in current data centers.
The growth of mobile technologies was evolutionary in the progression of technology.
The heart rate (HR) measurements based on the camera (visible light) can be used to detect HR in non-contact mode.
The hemibiotrophic fungal pathogen Leptosphaeria maculans is the causal agent of blackleg disease in Brassica napus (canola.
The heterogeneous wireless sensor networks are usually comprised by a large number of inexpensive sensor nodes.
The hidden weighted bit function (HWBF).
The Hirota bilinear method and Painleve-Backlund transformation are used to discuss the soliton solutions of the (3 + 1)-dimensional generalized shallow water equation.
 With the help of symbolic computation.
The history of agent development is a litany of expensive one-off solutions that are opaque to the uninitiated.
The HLA-G molecule presents immunomodulatory properties that might inhibit immune responses when interacting with specific Natural Killer and T cell receptors.
The holoprinter technology based on holographic stereograms has generated a fast development in holographic display applications by the holographic recording of a 2D image sequence with information of a 3D scene.
The Hour of Code is a one-hour introduction to computer science organized by Code.
The human insula is implicated in numerous functions.
 More and more neuroimaging studies focus on this region.
The human visual system (HVS) attempts to select salient areas to reduce cognitive processing efforts.
 Computational models of visual attention try to predict the most relevant and important areas of videos or images viewed by the human eye.
 Such models.
The hypercube network is one of the most popular parallel computing networks since it has a simple structure and is easy to implement.
 The locally twisted cube is a newly introduced variant of the hypercube which has the same number of nodes and same number of connections per node as the hypercube.
The ICH M7 guideline describes a consistent approach to identify.
The identification of plum varieties is generally done on the base of distinctive plant traits such as shape.
The ignition behaviour of a multiple-burner annular combustion chamber consisting of 12 or 18 bluff body premixed methane-air swirl burners was investigated experimentally.
 The study focusses on the mechanism of lightround.
The image intensities of low-backscattering areas in synthetic aperture radar (SAR) images are often seriously contaminated by the system noise floor and azimuthal ambiguity signal from adjacent high-backscattering areas.
The image thresholding techniques are considered as a must for objects segmentation.
The implementation of cryptography algorithms is constantly observing threat from multiple techniques in breaking secure system.
 The current trend in breaking the secure system of cryptography is by power analysis technique.
 In order to break the secure key.
The implementation of efficient multigrid preconditioners for elliptic partial differential equations (PDEs) is a challenge due to the complexity of the resulting algorithms and corresponding computer code.
 For sophisticated (mixed) finite element discretisations on unstructured grids an efficient implementation can be very time consuming and requires the programmer to have in-depth knowledge of the mathematical theory.
The importance of computer system security of banks can never be exaggerated.
 Conducting risk assessment of computer system security of banks can increase safety management and ensure normal operation.
 This paper firstly figures out risk assessment indexes for computer system security of banks through literature review and survey.
The importance of learning improvement is more and more recognized.
 Process capability maturity modeling elaborated by the Software Engineering community became applicable for any process-oriented activity assessment and improvement.
 This paper contributes to the solution of learning improvement problem based on the process quality attributes modeling approach.
 The consciousness as a learning process quality characteristic is introduced.
 Two-dimensional Learning process model based on R.
 Marzano taxonomy of learning objectives and on the staged Learning process maturity model has been developed and validated.
 (C) 2017 The Authors.
 Published by Elsevier B.
The importance of open data and the benefits it can offer have received recognition on the international stage with the signing of the G8 Open Data Charter in June 2013.
 The charter has an early focus on 14 high value areas.
The importance of skyline and dominance relationship analysis has been well recognized in multicriteria decision-making applications.
 In this paper.
The importance of Software Architecture (SA) design has been acknowledged as a very important factor for a high-quality software development.
 Different efforts in both industry and academia have produced multiple system development methodologies (SDMs) that include SA design activities.
 In addition.
The importance of the Internet for society is increasing.
 To ensure a functional Internet.
The importance of watermarking digital databases has increased by leaps and bounds due to the high vulnerability of digital assets to piracy attempts when they traverse through the internet.
 To deter piracy.
The impressive functions of brain circuits have inspired many scientists to attempt in designing neuron analogues by using artificial molecular systems or electronic devices.
The improved Monte-Carlo (MC) method for standard-less analysis in laser induced breakdown spectroscopy (LIBS) is presented.
 Concentrations in MC LIBS are found by fitting model-generated synthetic spectra to experimental spectra.
 The current version of MC LIBS is based on the graphic processing unit (GPU) computation and reduces the analysis time down to several seconds per spectrum/sample.
 The previous version of MC LIBS which was based on the central processing unit (CPU) computation requested unacceptably long analysis times of 10's minutes per spectrum/sample.
 The reduction of the computational time is achieved through the massively parallel computing on the GPU which embeds thousands of co-processors.
 It is shown that the number of iterations on the GPU exceeds that on the CPU by a factor >1000 for the 5-dimentional parameter space and yet requires >10-fold shorter computational time.
 The improved GPU-MC LIBS outperforms the CPU-MS LIBS in terms of accuracy.
The improved version of the author's previously declared asymmetric cipher protocol based on matrix power function (MPF) is presented.
 Proposed modification avoids discrete logarithm attack (DLA) which could be applied to the previously declared protocol.
 This attack allows us to transform the initial system of MPF equations to so-called matrix multivariate quadratic (MMQ) system of equations.
The incorporation of delay based computing.
The increase in authorship of nuclear physics publications has been investigated using the large statistical samples.
 Large collections of bibliographical metadata represent a very powerful tool for understanding of the past.
The increase in the number of cores per processor and the complexity of memory hierarchies make cache coherence key for programmability of current shared memory systems.
The increase of computer performance continues to support the practice of large-scale optimization.
 Computers with multiple computing cores and vector processing capabilities are now widely available.
 We investigate how the recently introduced Advanced Vector Instruction (AVX) set on Intel-compatible architectures can be exploited in interior point methods for linear and nonlinear optimization.
 We focus on data structures and implementation techniques that utilize the new vector instructions.
 Our numerical experiments demonstrate that the AVX instruction set provides a significant performance boost in our implementation on large-scale problem that have significant fill-in in the sparse Cholesky factorization.
The increase of mobile applications and social media are daily generating huge volumes of data.
 The variety of this data shapes an evolving term known as big data.
 Efficiently handling the big data seems a challenge to meet the rate of data growth.
 This challenge has played a significant role not only for indexing.
The increased availability of consumer-grade virtual reality (VR) head-mounted displays (HMD) has created significant demand for affordable and reliable 3D input devices that can be used to control 3D user interfaces.
 Accurate positioning of a user's body within the virtual environment is essential in order to provide users with convincing and interactive VR experiences.
 Existing full-body motion tracking systems from academia and industry have suffered from problems of occlusion and accumulated sensor error while often lacking absolute positional tracking.
 This paper describes a wireless Sensor Array System that uses multiple inertial measurement units (IMUs) for calculating the complete pose of a user's body.
 The system corrects gyroscope errors by using magnetic sensor data.
 The Sensor Array System is augmented by a positional tracking system that consists of a rotary-laser base station and a photodiode-based tracked object worn on the user's torso.
 The base station emits horizontal and vertical laser lines that sweep across the environment in sequence.
 With the known configuration of the photodiode constellation.
The increasing demand for complex and distributed software calls for novel software engineering methods and techniques.
The increasing number of applications of three-dimensional (3D) tumor spheroids as an in vitro model for drug discovery requires their adaptation to large-scale screening formats in every step of a drug screen.
The increasing performance and wider spread use of automated semantic annotation and entity linking platforms has empowered the possibility of using semantic information in information retrieval.
 While keyword-based information retrieval techniques have shown impressive performance.
The increasing usage of smartphones in daily life has received considerable attention in academic and industry driven research to be utilized in the health sector.
 There has been development of a variety of health-related smartphone applications.
The Indian Education sector has grown exponentially in the last few decades as per various official reports [22].
 Large amount of information pertaining to education sector is generated every year.
 This has led to the requirement for managing and analyzing the structured and unstructured information related to various stakeholders.
 At the same time there is a need to adapt to the dynamic global world by channelizing young talent in appropriate domains by cognizing and deriving the knowledge about individual student preferences hidden within the vast amount of education data.
 The derived knowledge is about getting finer information related to courses.
The information generated nowadays is growing in volume and complexity.
The inherent broadcast characteristics of wireless medium make wireless data transmission difficult to be shielded from unintended recipients.
The inherent uncertainty to factors such as technology and creativity in evolving software development is a major challenge for the management of software projects.
 To address these challenges the project manager.
The initial stage of the learning programming process is an arduous task.
 Novice students have to simultaneously learn the syntax of a programming language.
The integrability and multi-shock wave solutions of the DJKM equation are studied by means of Bell polynomials scheme.
The integration of Internet and mobile Internet.
The integration of the cloud and mobile device enables users more convenient to access.
The interaction simulations between particles and structures are often performed in the context of the combined discrete-finite element (DEM-FEM) method.
The interest behind the use of component based software engineering is to divide an information system in subsystems with less complexity.
The internal quality of intact persimmon cv.
 'Rojo Brillante' was assessed trough visible and near infrared hyperspectral imaging.
 Fruits at three stages of commercial maturity were exposed to different treatments with CO2 to obtain fruit with different ripeness and level of astringency (soluble tannin content).
 Spectral and spatial information were used for building classification models to predict ripeness and astringency trough multivariate analysis techniques like linear and quadratic discriminant analysis (LDA and QDA) and support vector machine (SVM).
 Additionally.
The internal structure of asphalt mixture revealed by X-ray computed tomography (CT) images provides useful information in a variety of civil engineering applications.
 This paper studied two image processing issues commonly encountered in asphalt X-ray CT images: image intensity unevenness caused by beam hardening effect and unrealistic image segmentation.
 Inspired by nonuniform illumination correction techniques.
The international conference IEEE SASO (Self-Adapting and Self-Organizing Systems) is the main forum for studying and discussing the foundations of a principled approach to engineering systems.
The International Software Benchmarking Standards Group (ISBSG) maintains a repository of data about completed software projects.
 A common use of the ISBSG dataset is to investigate models to estimate a software project's size.
The Internet of Things (IoT) comprises a complex network of smart devices.
The Internet of Things (IoT) is an emerging network paradigm that aims to obtain the interactions among pervasive things through heterogeneous networks.
 Security is an important task in the IoT.
 (Secur Commun Netw 7(10): 1560-1569.
The Internet of Things (IoT) is projected to soon interconnect tens of billions of new devices.
The Internet of Things (IoT) is the latest Internet evolution that incorporates a diverse range of things such as sensors.
The Internet of Things (IoT) represents a vision in which the Internet extends into the real world embracing everyday objects [9].
 Billions of objects are already connected to the Internet.
 These objects would intercommunicate without any human intervention and they would have different operating systems.
 Enterprises and developers should produce different versions of each application (same functionality) for each operating systems.
 For testing and evaluating all these versions.
The Internet of Things (loT) has become a novel paradigm that includes globally identifiable physical objects.
The Internet provides a convenient environment for data collection in psychology.
 Modern Web programming languages.
The Internet shopping optimization problem arises when a customer aims to purchase a list of goods from a set of web-stores with a minimum total cost.
 This problem is NP-hard in the strong sense.
 We are interested in solving the Internet shopping optimization problem with additional delivery costs associated to the web-stores where the goods are bought.
 It is of interest to extend the model including price discounts of goods.
 The aim of this paper is to present a set of optimization algorithms to solve the problem.
 Our purpose is to find a compromise solution between computational time and results close to the optimum value.
 The performance of the set of algorithms is evaluated through simulations using real world data collected from 32 web-stores.
 The quality of the results provided by the set of algorithms is compared to the optimal solutions for small-size instances of the problem.
 The optimization algorithms are also evaluated regarding scalability when the size of the instances increases.
 The set of results revealed that the algorithms are able to compute good quality solutions close to the optimum in a reasonable time with very good scalability demonstrating their practicability.
The Internet.
The intrinsic ill-posed nature of the inverse problem in near infrared imaging makes the reconstruction of fine details of objects deeply embedded in turbid media challenging even for the large amounts of data provided by time-resolved cameras.
 In addition.
The introduction of computer programming in K-12 has become mainstream in the last years.
The introduction of proteasome inhibitors in the treatment of multiple myeloma (MM) patients has been a therapeutic success.
 Peripheral neuropathy (PNP) remains one of the most frequent side-effects experienced by patients who receive these novel agents.
 Recent investigations on the mechanisms of PNP in patients treated with bortezomib have suggested genetic susceptibility to neurotoxicity.
 We used data from a genome-wide association study conducted on 646 bortezomib-treated German MM patients to replicate the previously reported associations between single-nucleotide polymorphisms (SNPs) in candidate genes and PNP in MM patients.
The introduction of targeted therapies has caused a paradigm shift in the treatment of metastatic clear cell (cc)-renal cell carcinoma (RCC).
 We hypothesized that determining differential kinase activity between primary and metastatic tumor sites may identify critical drivers of progression and relevant therapeutic targets in metastatic disease.
 Kinomic profiling was performed on primary tumor and metastatic tumor deposits utilizing a peptide substrate microarray to detect relative tyrosine phosphorylation activity.
 Pharmacologic and genetic loss of function experiments were used to assess the biologic significance of the top scoring kinase on in vitro and in vivo tumor phenotypes.
 Kinomics identified 7 peptides with increased tyrosine phosphorylation in metastases that were significantly altered (p<0.
 Based on these peptides.
The intrusion detection systems.
The invariant moment feature of target is taken as an important approach to human shape recognition.
 This paper gives a general review of Hu moments and Zernike moments in human shape recognition.
The invasive computing paradigm offers applications the possibility to dynamically spread their computation in a multicore/multiprocessor system in a resource-aware way.
 If applications are assumed to act maliciously.
The inversion of the Laplace-Beltrami operator and the computation of the Hodge decomposition of a tangential vector field on smooth surfaces arise as computational tasks in many areas of science.
The investigation of solar-like oscillations for probing star interiors has enjoyed a tremendous growth in the last decade.
 Once observations are over.
The issue of providing information security for data and computing resources in grid networks is reviewed.
 Specific features of architecture of distributed computing networks based on grid platforms are analyzed.
 Security threats specific for grid systems are typified.
 The available measures ensuring security for grid systems are considered.
The Jacobian matrix algorithm is often used to calculate the Lyapunov exponents of the chaotic systems.
 This study extends the algorithm to discrete fractional cases.
 The tangent maps with memory effect are presented.
 The Lyapunov exponents of one and two dimensional fractional logistic maps are calculated.
 The positive ones are used to distinguish the chaotic areas of the maps.
 (C) 2014 Elsevier B.
 All rights reserved.
The K Nearest Neighbor (kNN) method has widely been used in the applications of data mining and machine learning due to its simple implementation and distinguished performance.
The k-ary n-cube is one of the most attractive interconnection networks for parallel and distributed computing systems.
 In this paper.
The k-ary n-cube is one of the most attractive interconnection networks for parallel and distributed computing systems.
 In this paper.
The kernel RX (KRX) detector proposed by Kwon and Nasrabadi exploits a kernel function to obtain a better detection performance.
The key escrow problem and high computational cost are the two major problems that hinder the wider adoption of hierarchical identity-based signature (HIBS) scheme.
 HIBS schemes with either escrow-free (EF) or online/offline (OO) model have been proved secure in our previous work.
The kNN algorithm remains a popular choice for pattern classification till date due to its non-parametric nature.
The Knowledge Discovery in Distributed Databases is the process of extracting useful information from a collection of data stored in distributed databases.
 A distributed database is a collection of data replicated over a number of different computers.
 The best-suited structures for working with distributed databases are the Distributed Committee-Machines.
 Distributed Committee-Machines are a combination of neural networks that work in a distributed manner as a group in order to obtain better performance than individual neural networks in solving data mining tasks inside the KDD process.
 In this paper.
The Korteweg-de Vries (KdV)-type equations can describe the shallow water waves.
The Korteweg-de Vries (KdV)-type equations have been seen in fluid mechanics.
The Korteweg-de Vries equation (KdV) and the (2+ 1)-dimensional Nizhnik-Novikov-Veselov system (NNV) are presented.
 Multi-soliton rational solutions of these equations are obtained via the generalized unified method.
 The analysis emphasizes the power of this method and its capability of handling completely (or partially) integrable equations.
 Compared with Hirota's method and the inverse scattering method.
The K-SVD algorithm has been successfully utilized for adaptively learning the sparse dictionary in 2-D seismic denoising.
 Because of the high computational cost of many singular value decompositions (SVDs) in the K-SVD algorithm.
The Laboratory for Reactor Physics and Systems Behaviour at the PSI and the EPFL has been developing in recent years a new code system for reactor analysis based on OpenFOAM.
 The objective is to supplement available legacy codes with a modern tool featuring state-of-the-art characteristics in terms of scalability.
The lack of user engagement.
The Langlands Programme.
The large deployment of the NIST RBAC model has initiated popular research topics such as administration.
The large-scale aggregation and analysis of user opinions is becoming increasingly relevant to a variety of applications.
The large-scale relational databases normally have a large size and a high degree of sparsity.
 This has made database compression very important to improve the performance and save storage space.
 Using standard compression techniques (syntactic) such as Gzip or Zip does not take advantage of the relational properties.
The last two decades have witnessed an enormously rapid development of computer technologies which have undoubtedly affected all the fields of human activities.
The least squares support vector machine (LS-SVM) has emerged as a popular data-driven modeling method and been extensively studied in the machine learning community.
The least-mean-magnitude-phase (LMMP) algorithm is useful for complex-valued signal processing applications where control of magnitude and/or phase error information is needed to achieve good overall performance.
 Due to the highly-nonlinear nature of the update terms in the LMMP and related methods.
The Legendre sequence possesses several desirable features of pseudorandomness in view of different applications such as a high linear complexity (profile) for cryptography and a small (aperiodic) autocorrelation for radar.
The linear mixed effects model based on a full likelihood is one of the few methods available to model longitudinal data subject to left censoring.
The linear mixed model with an added integrated Ornstein-Uhlenbeck (IOU) process (linear mixed IOU model) allows for serial correlation and estimation of the degree of derivative tracking.
 It is rarely used.
The literature presents many application programming interfaces (APIs) and frameworks that provide state of the art algorithms and techniques for solving optimisation problems.
 The same cannot be said about APIs and frameworks focused on the problem data itself because with the peculiarities and details of each variant of a problem.
The local binary pattern (LBP) is a simple and efficient texture descriptor for image processing.
The LOCAL(A.
The long-term behavior of dynamical system is usually analyzed by means of basins of attraction (BOA) and most often.
The long-term goal of the Virtual Physiological Human and Digital Patient projects is to run 'simulations' of health and disease processes on the virtual or 'digital' patient.
The magnetic moment of magnetically labeled cells.
The main aim is to present recent developments in applications of symbolic computing in probabilistic and stochastic analysis.
The main aim of this paper is to develop four innovative linearization formulas for some nonsymmetric Jacobi polynomials.
 This means that we find the coefficients of the products of Jacobi polynomials of certain parameters.
 In general.
The main aspect of database protection is to prove the ownership of data that describes who is the originator of data.
 It is of particular importance in the case of electronic data.
The main contribution of this paper is the introduction of a continuous improvement cycle for devising teaching scenarios and conducting learning experiences in engineering.
 The proposed cycle consists of seven steps on which gamification theory and ABET criteria are combined.
 It arose from the adaptation of a gamification design framework.
The main contribution of this paper is to present a workoptimal parallel algorithm for LZW decompression and to implement it in a CUDA-enabled GPU.
 Since sequential LZW decompression creates a dictionary table by reading codes in a compressed file one by one.
The main goal of introducing an identity-based cryptosystem and certificateless cryptosystem was avoiding certificates' management costs.
The main idea of the structure-preserving method is to preserve the intrinsic geometric properties of the continuous system as much as possible in numerical algorithm design.
 The geometric constraint in the multi-body systems.
The main objective of this paper is to analyze the effect of MHD.
The main objective of this study was to configure the acquisition and analysis of low-field magnetic resonance imaging (MRI) to predict physico-chemical characteristics of Iberian loin.
The main purpose of the Semantic Web expresses the information as intelligent forms.
The main purpose of this paper is to design a self-tuning control algorithm for an adaptive cruise control (ACC) system that can adapt its behaviour to variations of vehicle dynamics and uncertain road grade.
 To this aim.
The main purpose of this study is the presentation skills evaluation of pre-service teachers via fuzzy logic.
 There are two different groups in this study.
 The first one consists of 14 instructors serving in the Computer Education and Instructional Technology (CEIT) Department of Firat University (FU).
 This group has supplied expert view in ranking the importance of the matters in the Presentation Evaluation Scale (PES) and in forming the rule base that sets ground for the operation of fuzzy logic mechanism.
 The second group consists of 41 juniors having the Operating Systems and Applications Course in the CEIT Department of Fa The students in this group make a presentation associated with the units of this course.
The main purpose of this work is to describe the case of an online Java Programming course for engineering students to learn computer programming and to practice other non-technical abilities: online training.
The main result of this paper is a simulation algorithm which.
The major drawback of the replication-based RAID (redundant arrays of independent disks) architectures is that.
The Maple package TimeS for time series analysis has a new feature and an improvement in forecasting by phase space reconstruction.
 An optional argument in the computational routines that allows the researcher to choose the different number of steps ahead to forecast.
 This update extends the running of the package with this new feature for the current versions of the Maple software too.
The massive amounts of data collected from numerous sources like social media.
The massive amounts of data processed in modern computational systems is becoming a problem of increasing importance.
 This data is commonly stored directly or indirectly through the use of data exchange languages.
The maximal ball (MB) algorithm is a well established method for the morphological analysis of porous media.
 It extracts a network of pores and throats from volumetric data.
 This paper describes structural modifications to the algorithm.
The maximum operational range of continuous variable quantum key distribution protocols has shown to be improved by employing high-efficiency forward error correction codes.
The MCS lock is one of the most prevalent queuing locks.
 It provides fair scheduling and high performance on massively parallel systems.
The measurement of the morphologic characteristics of evolving sediment beds around hydraulic structures is crucial for the understanding of the physical processes that drive scour.
 Although there has been significant progress towards the experimental characterization of the flow field in setups with complex geometries.
The mechanical and functional performance of nonwoven fabric critically depends on the fibre architecture.
 The fibre laydown process plays a key role in controlling this architecture.
 The fibre dynamic behaviour during laydown is studied through a finite element model which describes the role of the parameters in defining the area covered by a single fibre when deposited on the conveyor belt.
 The path taken by a fibre is described in terms of the radius of gyration.
The mechanical properties of Ni-base alloys have drawn considerable attention owing to their wide application in the hot components of aircrafts and gas turbines.
 To accurately measure the deformation of Ni-based alloys at high temperatures.
The mechanical properties of soft materials are critically important for a wide range of applications ranging from packaging to biomedical purposes.
 We have constructed a simple mechanical testing apparatus using off-the-shelf materials and open-source software for a total cost of less than $100.
 The device consists of a wooden frame supporting a central loading apparatus attached via drawer slides.
 To perform a mechanical test.
The method of multiple scales is a global perturbation technique that has resulted to be very useful in perturbed ordinary differential equations characterized by disparate time scales.
 The general principle behind the method is that the solution to the differential equation is uniformly expanded in terms of two or more independent variables.
The method presented in Duarte and da Mota (2009) and Avellar et al.
 (2014) to search for first order invariants of second order ordinary differential equation (2ODEs) makes use of the so called Darboux polynomials.
 The main difficulty involved in this process is the determination of the Darboux polynomials.
The migration of e-health systems to the cloud computing brings huge benefits.
The minimum spanning tree (MST) construction is a classical problem in Distributed Computing for creating a globally minimized structure distributedly.
 Self-stabilization is versatile technique for forward recovery that permits to handle any kind of transient faults in a unifiedmanner.
 The loop-free property provides interesting safety assurance in dynamic networks where edge-cost changes during operation of the protocol.
 We present a new self-stabilizing MST protocol that improves on previous known approaches in several ways.
The Mobile ad-hoc network do not take any static infrastructure and they depend on their neighbors to convey message.
 The mobile nodules can travel all over the network in an unrestricted fashion.
 Unlike wired networks.
The mobile app market continues to grow at a tremendous rate.
 The market provides a convenient and efficient distribution mechanism for updating apps.
 App developers continuously leverage such mechanism to update their apps at a rapid pace.
 The mechanism is ideal for publishing emergency updates (i.
The modeling and solving a transcendental eigenvalue problem are important issues in the transfer matrix method for linear multibody systems.
 Based on the recursive eigenvalue search algorithm for transfer matrix method for linear multibody system.
The modeling of wave propagation in microstructured materials should be able to account for various scales of microstructure.
 Based on the proposed new exponential expansion method.
The modified Zakharov-Kuznetsov (mZK) equation in the electrical transmission line is investigated in this paper.
 Different expressions on the parameters in the mZK equation are given.
 By means of the Hirota method.
The molecular structure of living organisms and the complex interactions amongst its components are the basis for the diversity observed at the macroscopic level.
 Proteins and nucleic acids are some of the major molecular components.
The Montage Image Mosaic Engine was designed as a scalable toolkit.
The most common way to deal with the uncertainty present in noisy sensorial perception and action is to model the problem with a probabilistic framework.
 Maximum likelihood estimation is a well-known estimation method used in many robotic and computer vision applications.
 Under Gaussian assumption.
The most commonly applied method of fluorescence detection for on-chip gel electrophoresis is finish line detection in epifluorescence configuration using highly sensitive photodiode or a cooled scientific camera as a photodetector.
 In this paper we aim to confirm that a simple optical configuration based on a limited number of components and a low-cost.
The most important characteristic of semi-supervised learning methods is the combination of available unlabeled data along with an enough smaller set of labeled examples.
The most important potential of E-learning system is the ability to adapt based on learner's status in order to support personalized learning.
The most popular approach for analyzing survival data is the Cox regression model.
 The Cox model may.
The motion capture is a useful technique for creating animations instead of manual labor.
The motion of a thin viscous film of fluid on a curved surface exhibits many intricate visual phenomena.
The motion of address resolution protocol (ARP) is done without any problem in a general environment.
The motion of the projectile is an easily observable phenomenon.
 The knowledge of the behavior of projectiles has been used extensively in warfare.
The Multi Objective Genetic Algorithms (MO-GAs) are one of the most widely used techniques that have the capability to find the solution to the problem having multiple conflicting objectives like Intrusion De- tection.
 It is a population based technique capable of producing a set of non-inferior solutions that exhibit the classification trade-offs for the user.
 This capabil- ity of MOGA can be exploited for generating optimal base classifiers and ensembles thereof for Intrusion De- tection.
 This paper explores the various MOGAs proposed in the literature along with their pros and cons.
 The motivation for the use of MOGA and its issues are high- lighted.
The multidisciplinary and versatile characteristics of biosensors make them very attractive topics to introduce science and engineering concepts into a secondary education classroom.
 Some of the techniques used in biosensors are easy to demonstrate and can be intriguing to students.
 We evaluated which application areas and techniques were most intriguing to students through a survey and then developed a 60 to 90 min introductory lesson plan with hands-on experience.
 The survey was conducted to determine students' greatest interests and motivations among four biosensor applications (medical diagnostics.
The multidrug and toxin extrusion (MATE) transporter family comprises 70 members in the Medicago truncatula genome.
The multiple exp-function method is a new approach to obtain multiple-wave solutions of nonlinear partial differential equations (NLPDEs).
 By this method.
The multi-population method has been widely used to solve unconstrained continuous dynamic optimization problems with the aim of maintaining multiple populations on different peaks to locate and track multiple changing peaks simultaneously.
The multivariate approach based on Principal Component Analysis (PCA) for anomaly detection received a lot of attention from the networking community one decade ago.
The nature of nonlinear molecular deformations in a homeotropically aligned nematic liquid crystal (NLC) is presented.
 We start from the basic dynamical equation for the director axis of a NLC with elastic deformations and adopt space curve mapping procedure to analyze the dynamics.
 The NLC is governed by an integro-differential perturbed nonlocal nonlinear Schrodinger equation and we solve the same using Jacobi elliptic function method aided with symbolic computation and construct an exact solitary wave solution.
 In order to better understand the effect of nonlocality on the director reorientations of nematic liquid crystal.
The nearest neighbors (NN) algorithm is a traditional algorithm utilized in many research areas like computer graphics.
The near-infrared (NIR) imaging method was required for the diagnosis of varicose veins.
The necessary and sufficient conditions for a single output system to be state equivalent to a nonlinear observer canonical form have been found by Krener and Isidori.
 In this paper.
The need for a novel automated mosquito perception and classification method is becoming increasingly essential in recent years.
The need for accelerating power grid simulation through high performance computing (HPC) has long been recognized.
The need for high-performance computing together with the increasing trend from single processor to parallel computer architectures has leveraged the adoption of parallel computing.
 To benefit from parallel computing power.
The need for operational monitoring of landscape processes on the national to global scale led to an increased demand for pixel-based composites using complete earth observation (EO) archives.
The need of scalable databases has raised rapidly in these past few years.
 Researchers and developers have tried to develop several techniques to create replications and partitions in non-relational databases.
The need to store and manipulate large volume of (unstructured) data has led to the development of several NoSQL databases for better scalability.
 Graph databases are a particular kind of NoSQL databases that have proven their efficiency to store and query highly interconnected data.
The need to store and query a set of strings - a string dictionary - arises in many kinds of applications.
 While classically these string dictionaries have accounted for a small share of the total space budget (e.
The need to understand the control strategies utilized by humans in their everyday activity requires the measurement of several variables.
The network systems of the world are fragile.
The new styles and ways of life lead to greater use of wireless networks.
The next generation Internet (also known as Internet-of-Things - IoT).
The next-generation high speed wireless technologies.
The nirk gene encoding the copper-containing nitrite reductase (CuNiR).
The nonlinear differential equation governing the periodic motion of the one-dimensional.
The nonlocal symmetry which is obtained from Lax pair and the residual symmetry relating to truncated Painleve expansion are derived.
 The link between the residual symmetry and the nonlocal symmetry which is obtained from Lax pair is presented.
 The residual symmetry can be localized to Lie point symmetry by prolonging the original equation to a larger system.
 The finite transformation of the residual symmetry is equivalent to the second type of Darboux transformation.
 Furthermore.
The notion Internet of Things (IoT) means all things in the global network can be interconnected and accessed.
 Wireless sensor network (WSN) is one of the most important applications of the notion and is widely used in nearly all scopes.
The number of applications that need to process data continuously over long periods of time has increased significantly over recent years.
 The emerging Internet of Things and Smart Cities scenarios also confirm the requirement for real time.
The number of elderly and chronically ill patients has grown significantly over the past few decades as life expectancy has increased worldwide.
The number of local minima of the potential energy landscape (PEL) of molecular systems generally grows exponentially with the number of degrees of freedom.
The number of metagenomic studies conducted each year is growing dramatically.
 Storage and analysis of such big data is difficult and time-consuming.
 Interestingly.
The number of mobile terminals that have both a cellular interface and a Wi-Fi interface has been growing in the recent years.
The number of protein structures in the PDB database has been increasing more than 15-fold since 1999.
 The creation of computational models predicting enzymatic function is of major importance since such models provide the means to better understand the behavior of newly discovered enzymes when catalyzing chemical reactions.
The Object Modeling Technique.
The objective of a rendering algorithm is to compute a photograph of a simulated reality.
The objective of self modeling curve resolution (SMCR) methods is to decompose a second-order bilinear data matrix into a range of chemically meaningful matrices without any knowledge about the chemical or physical model describing the considered system.
 In addition.
The objective of this article is to present a framework that couples cloud and high-performance computing for the parallel map projection of vector-based big spatial data.
 The past few years have witnessed a tremendous growth of a variety of high-volume spatial data-i.
The objective of this paper is to present a fusion model of an odor sensor and highly advanced optical sensor to evaluate total volatile basic nitrogen (TVB-N) content in chicken meat.
The objective of this research is to design a 2D inexpensive mechanism for testing the cargo separation in a wind tunnel.
 Our designed mechanism simulates the separation of a NACA-0012 airfoil from the upper wall of a wind tunnel.
 To perform the test.
The objective of this study is to create computer vision algorithms for autonomous multiclass identification of amber nuggets by their colour.
 By applying the proposed methods an automated production sorting system has been developed.
 This system can be used.
The objective of this study is to introduce an intelligent E-Learning software system.
The objective of this study is to investigate the use of an alternative working pair in a solar absorption cooling system.
 LiCl-H2O is the new examined pair and it is compared energetically and exegetically with the conventional pair LiBr-H2O.
The objective of this work was to improve kiwifruit drying with respect to quality.
 The drying of kiwifruit slices was studied with hot air drying (HAD) and hybrid hot air-infrared drying (HID) at 50.
The oncogenic Kaposi's sarcoma-associated herpesvirus (KSHV) is a principal causative agent of primary effusion lymphoma (PEL).
The oncogenic roles of sphingosine kinase 1 (SphK1) in various cancers.
The One-vs-One strategy is among the most used techniques to deal with multi-class problems in Machine Learning.
The ongoing trend towards multi-site software development not only brings the benefits but also creates additional challenges regarding remote communication and coordination.
 The Software Engineering Ontology (SE Ontology) was first developed to clarify the software engineering concepts and project information.
The online extraction of kernel principal components has gained increased attention.
The OpenCL specification tightly binds a command queue to a specific device.
 For best performance.
The open-platform communication (OPC) unified architecture (UA) (IEC62541) is introduced as a key technology for realizing a variety of smart grid (SG) use cases enabling relevant automation and control tasks.
 The OPC UA can expand interoperability between power systems.
 The top-level SG management platform needs independent middleware to transparently manage the power information technology (IT) systems.
The open-source software project OpenPhase allows the three-dimensional simulation of microstructural evolution using the multiphase field method.
 The core modules of OpenPhase and their implementation as well as their parallelization for a distributed-memory setting are presented.
 Especially communication and load-balancing strategies are discussed.
 Synchronization points are avoided by an increased halo-size.
The operable description of parallel mechanisms is the key to automatic derivation of structural analysis and synthesis.
 Conformal geometric algebra is introduced to describe robot mechanisms in this paper.
 A group of basis bivectors {e(23).
The optimization of a Product Line Architecture (PLA) design can be modeled as a multi-objective problem.
The organization of the mammalian genome into gene subsets corresponding to specific functional classes has provided key tools for systems biology research.
The original task scheduling algorithm of Hadoop cannot meet the performance requirements of heterogeneous clusters.
 According to the dynamic change of load of each task node and the difference of node performance of different tasks in the heterogeneous Hadoop cluster.
The origin-destination matrix is an useful tool for traffic and transport analysis.
 This kind of matrices are increasing in volume and complexity due to the availability of new technologies to gather new information.
 In this project we present a novel way of representing origin-destination matrices using a connection barchart and two co-ordinated maps.
 The main goal is to use the connection Barchart as the center of the interactive visualization providing an overview of the dataset.
 Through interaction.
The paper attempts to integrate the Quality Function Deployment (QFD) method in the New Product Development (NPD) cycle.
 We propose a methodology to evaluate the voice of the customers in designing innovative products.
 The results presented after applying the methodology focus on the first stage of requirements capture for both the design phase and for the development of the actual product phase.
 There was considered as case study an innovative.
The paper covers the principal requirements.
The paper cryptanalyses the symmetric key image encryption scheme proposed by Mrinal et al.
 The encryption scheme proposed by Mrinal et al.
 uses Rossler chaotic system for generating three chaotic sequences which are used for creating the cipher image.
 A secret key of 128 bits is used to generate the initial conditions for the Rossler chaotic system.
 The equations used for generating the initial conditions in Mrinal et al.
 encryption scheme have got a loophole.
 The loophole is utilized to carry out the cryptanalysis on the cipher image generated using Mrinal et al.
 encryption scheme.
 Simulation results show that the encryption scheme can be easily cryptanalysed to revealed the exact plain image.
 An improved version is proposed with statistical and security analyses to ascertain the practicability of the encryption scheme.
 (C) 2017 Elsevier GmbH.
 All rights reserved.
The paper deals with production performance evaluation and comparison among the EU selected countries in information and communication sectors in the period 2005 - 2012.
 We estimate a translog stochastic production frontier using Corrected Ordinary Least Squares (COLS) method in the total Information and communication sector (J) and also its two parts - Telecommunications (J61) and Computer programming.
The paper deals with the isomorphism between the constraint heterogeneous concept lattices and concept lattices with heterogeneous hedges.
 The essential point of the former approach encompasses the full diversification of data structures within a formal context.
 In particular.
The paper describes a concept of software tools for data stream processing.
 The tools can be used to implement parallel processing systems.
 Description of the task is presented in the first part of paper.
 The system is based on pipeline parallelism and was distributed for using on a cluster computer.
 The paper describes a base scheme and a main work algorithm of the system.
 An actual application example is presented.
 The system has some weak sides which are described at the end of paper.
 Direction of future research is presented at the end of the article.
The paper describes an improved parallel MPI-based implementation of VBARMS.
The paper describes the creation of modelling and simulative tool of Petri nets by means of which it is possible to simulate processes and it enables a vast tool support in the fields of model check.
The paper describes the Mathematica-based software for studying nonlinear control systems.
 The software relies on an algebraic method.
The paper describes the problem of unauthorized access to the data processed in distributed grid computing networks.
 Existing implementations of entity authentication mechanisms in grid systems are analyzed.
The paper develops a general regression framework for the analysis of manifold-valued response in a Riemannian symmetric space (RSS) and its association with multiple covariates of interest.
The paper discusses the possibilities of using non-relational databases as data warehouses' (DW) data mart - an area traditionally covered by relational databases.
 The authors highlight potential benefits when using non-relational databases (NoSQL) technology based on the main characteristics of classical relational data base management system (RDBMS) based DW.
 The paper outlines a creation and production process for DW using a NoSQL data mart and develops requirements for technology necessary for such processes based on practical experience when implementing NoSQL data marts with MongoDB and Clusterpoint DB.
 Since there currently do not exist of-the-shelf reporting solutions for most of the NoSQL data storages the authors develop requirements for a reporting solution necessary for NoSQL based DW.
The paper evaluates usability of information modelling tools on the most common operating systems (Windows.
The paper focuses on and examines the issues and problems related to remote evaluation of software engineering competences using progressive competence representation model.
 Authors suggested original approach for Master Program in Software Engineering competence evaluation as a combination academic competences and professional competences from European Competence model (e-CF).
 Examples of competence description for 16 subjects from proposed a Joint Master Program in Software Engineering are developed.
 Several types of scoring rubrics for Software Engineering competences evaluation are reviewed and rubrics' templates created.
 The developed models and templates can be used by universities and IT enterprises for training results evaluation as well as for competence evaluation for Software Engineering Master program's graduates.
 (C) 2017 The Authors.
 Published by Elsevier B.
The paper focuses on the challenge of generating theoretical support for software development.
The paper focuses on the field of ontology evaluation and visualization.
 Ontologies represent the essential technology for the development of the Semantic web applications.
 This technology has been proven to be useful in a range of applications for data manipulation and administration.
 The paper introduces an ontology visualization approach based on descriptive vectors.
 It offers the design of descriptive vectors representation for an ontology domain and also the algorithm design for generation of the descriptive vectors.
 This approach offers quick overview of the given ontologies content.
 In addition.
The paper introduces real logic: a framework that seamlessly integrates logical deductive reasoning with efficient.
The paper is concerned with analyzing data from an experimental antipodal laser-based chaos shift-keying communication system.
 Binary messages are embedded in a chaotically behaving laser wave.
The paper outlines an approach for improving the effectiveness and reliability of base isolation devices in civil engineering structures that undergo exceptional dynamic conditions.
 The strategy consists of designing the passive device in such a way to take into account the not-negligible soil-structure interaction effects.
 At this stage.
The paper overviews the origin and motivation of the unconventional molecular computing.
The paper presents a novel computer vision-based traffic surveillance system capable of processing aerial imagery to track vehicles and their movements.
 The system uses a preprocessed 1-Hz image sequence with a coverage of 64.
80 km(2) (25 sq mi) from an aerial camera array mounted on an airplane.
 The unique characteristics of the input data make this work challenging.
 Heuristic and machine-learning approaches are combined and evaluated to detect and track vehicles for the purpose of collecting speed.
The paper presents a novel distributed classifier of the convergence.
The paper presents a novel method for assessing the local structural identifiability question for a general non-linear state-space model.
 The method is a combination of (i) the application of a singular value decomposition to a parametric output sensitivity matrix that is created by simply integrating the model once and.
The paper presents a platform for distributed computing.
The paper presents a series of considerations regarding the role of the conceptual invariants concerning the prevention of the artefacts' obsolescence.
The paper presents an application of actor model to control braking torques on wheels of an articulated vehicle in an untripped rollover manoeuvre.
 The numerical model of the articulated vehicle and dynamic optimisation have been used to calculate appropriate braking torques for each wheel in order to restore stability.
 The optimisation problem requires the equations of motion to be integrated at each optimisation step and its time-consuming.
The paper presents design and implementation of the cellular automata (CA) model.
The paper presents the comparison study of relational.
The paper presents the formalism of the parametric integral equation system (PIES) for two-dimensional elastoplastic problems and the algorithm for its numerical solution.
 The efficiency of the proposed approach lies in the global modeling of a plastic zone.
The paper presents the issue of mapping database schema within the context of possible partial loss of their semantics.
The paper presents the research on the use of methods of computer image analysis and artificial neural modeling in the process of assessing the quality of greenhouse tomatoes variety Cappricia.
 The subject of the study was tomatoes of the sizes from 40mm to 67mm and the colours: 1-6.
The paper proposes a new model.
The paper reports a free-breathing black-blood CINE fast-spin echo (FSE) technique for measuring abdominal aortic wall motion.
 The free-breathing CINE FSE includes the following MR techniques: (1) variable-density sampling with fast iterative reconstruction; (2) inner-volume imaging; and (3) a blood-suppression preparation pulse.
 The proposed technique was evaluated in eight healthy subjects.
 The inner-volume imaging significantly reduced the intraluminal artifacts of respiratory motion (p = 0.
 The quantitative measurements were a diameter of 16.
8 mm and wall distensibility of 2.
0%) for the anterior and posterior walls.
The paper studies the gathering problem on grid and tree networks.
 A team of robots placed at different nodes of the input graph.
The Parabolic Synthesis methodology is an approximation methodology for implementing unary functions.
The pathogenicity of the different flu species is a real public health problem worldwide.
 To combat this scourge.
The perceptual adaptation of the image (PAI) is introduced by inspiration from Chevreul-Mach Bands (CMB) visual phenomenon.
 By boosting the CMB assisting illusory effect on boundaries of the regions.
The performance of discontinuous deformation analysis (DDA) needs to be improved for large-scale analysis.
 In this study.
The performance of magnetic resonance imaging (MRI) equipment is typically monitored with a quality assurance (QA) program.
 The QA program includes various tests performed at regular intervals.
 Users may execute specific tests.
The performance of many machine learning algorithms heavily relies on the distance metrics.
 Usually a distance metric is learned from a training set.
The periodical maintenance of railway systems is very important in terms of maintaining safe and comfortable transportation.
 In particular.
The p-facility Huff location problem aims at locating facilities on a competitive environment so as to maximize the market share.
 While it has been deeply studied in the field of continuous location.
The phase retrieval (PR) problem of recovering an image from its Fourier magnitudes is an important issue.
 Several PR algorithms have been proposed to address this problem.
 Recent efforts of exploiting sparsity were developed to improve the performance of PR algorithms.
The phylogenetic likelihood function (PLF) is the major computational bottleneck in several applications of evolutionary biology such as phylogenetic inference.
The physical design of data storage is a critical administrative factor for optimizing system performance.
 Improved system performance can be achieved by building indices.
 It must be noted that.
The pillars of Computer Science and Engineering (CSE) curriculum are Data Structures.
The point cloud is the most fundamental representation of three-dimensional geometric objects.
 Analyzing and processing point cloud surfaces is important in computer graphics and computer vision.
The point-inclusion problem is an important secure multi-party computation that it involves two parties.
The polygon retrieval problem is.
The polynomial fitting method is used widely to detect cycle slips of carrier phases of GNSS for its easily computer programming and simple algorithm.
The popularity of Bus Rapid Transit (BRT) makes Trans Jogja an alternative of a mass public transportation system for urban mobility.
The popularity of mobile devices has been steadily growing in recent years.
 These devices heavily depend on software from the underlying operating systems to the applications they run.
 Prior research showed that mobile software is different than traditional.
The popularity of smartphones usage especially Android mobile platform has increased to 80% of share in global smartphone operating systems market.
The possibility of using Neumann's method to solve the boundary problems for thin elastic shells is studied.
 The variational statement of the static problems for the shells allows for a problem examination within the distribution space.
 The convergence of Neumann's method is proven for the shells with holes when the boundary of the domain is not completely fixed.
 The numerical implementation of Neumann's method normally requires significant time before any reliable results can be achieved.
 This paper suggests a way to improve the convergence of the process.
The power system simulation software tools are traditionally designed for serial codes and optimized using single-processor computers.
 They are inadequate in terms of computational efficiency and execution time for the ever-increasing complexity of the power grid.
 Due to the above-mentioned sequential computing demerits.
The powertrain system of a typical proton electrolyte membrane hybrid fuel cell vehicle contains a lithium battery package and a fuel cell stack.
 A multi-objective optimization for this powertrain system of a passenger car.
The practice of continuous integration has firmly established itself in the mainstream of the software engineering industry.
The precise and very fast contactless measurement of object surfaces and edges with sub-micrometer resolution plays an important role in many applications nowadays.
 Optical high-precision measuring devices are used for that purpose.
 The use of narrowband point light sources for exposing an object generates a diffraction pattern.
The prediction of bankruptcy for financial companies.
The prediction problem of network security should be studied facing the Massive malicious attacks.
The predominant technical challenge of the upstream oil and gas industry has always been the fundamental uncertainty of the subsurface from which it produces hydrocarbon fluids.
 The subsurface can be detected remotely by.
The prefix table of a string is one of the most fundamental data structures of algorithms on strings: it determines the longest factor at each position of the string that matches a prefix of the string.
 It can be computed in time linear with respect to the size of the string.
The present paper aims to explore the Big Data concept in order to answer the question whether this is a necessity or a hidden agenda to promote disrupting new technologies.
 It was ascertained that.
The present paper establishes a homotopy based on Marcus-Wyse (for brevity M-) topology.
The present paper proposes a novel cluster-basedmethod.
The present paper reports on a trial to shed further light on the characterization.
The present study aimed to investigate potential gene markers for predicting the formation of carotid atheroma plaques using high-throughput bioinformatics methods.
 The GSE43292 gene expression profile was downloaded from the Gene Expression Omnibus database.
 Following data processing.
The present study aimed to screen several differentially expressed genes (DEGs) and differentially expressed microRNAs (miRNAs) for two types of mesenchymal stem cell (MSC) differentiation.
 Bone morphogenetic protein 6 (BMP-6) and dexamethasone were used to induce MSCs towards osteoblastic differentiation or adipocytic differentiation.
 The t-test in the Bioconductor bioinformatics software tool was used to screen DEGs and differentially expressed miRNAs in the two samples.
 Subsequent gene ontology (GO) and pathway analyses on the DEGs were performed using the GO and Kyoto Encyclopedia of Genes and Genomes databases.
The present study mainly investigates bubble behaviors in an agitated vessel.
 Computational fluid dynamics provides a method for exploring the complex fluid flow in an agitated vessel.
 A stirred tank model with transport equation for the interfacial area concentration and force equilibrium equation for bubbles under multi-forces is created for a 43-dm3 agitated vessel.
 The model considers the breakage and coalescence mechanism of the bubbles.
 The local gas holdup is measured by flber-optical probe.
 After this.
The present study reports a comprehensive nuclear magnetic resonance (NMR) characterization and a systematic conformational sampling of the conformational preferences of 170 glycan moieties of glycosphingolipids as produced in large-scale quantities by bacterial fermentation.
 These glycans span across a variety of families including the blood group antigens (A.
The present work addresses the development of a finite element formulation for handling bending.
The present work addresses the problem of determining under what conditions the impending slip state or the steady sliding of a linear elastic orthotropic layer or half space with respect to a rigid flat obstacle is dynamically unstable.
 In other words.
The present work aims to develop a methodology for classifying lung nodules using the LIDC-IDRI image database.
 The proposed methodology is based on image-processing and pattern-recognition techniques.
 To describe the texture of nodule and non-nodule candidates.
The present work introduces a curvelet-like directional filter and discusses its application to edge detection in general images and fracture detection in GPR data.
 The filter is essentially a curvelet of adjustable anisotropy and orientation that can be tuned on any given (target) wavenumber; while retaining the properties of curvelets.
The presented paper describes the design and implementation of R functions for twitter feeds analysis and visualization based on a combination of analytical technologies with big data processing tools.
 The main idea was to utilize the Hadoop processing framework and its storage and computational capabilities in analytical tasks designed and implemented in R language.
 For such purposes.
The presented simulation model of surface tension and wettability based on physical properties of liquids is designed for use in computer graphics.
 Due to the relatively small surface tension forces the model is useful for simulating liquid of small volume such as droplets.
 This model can be used in conjunction with various fluid simulation methods.
The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades.
 Prior solutions.
The primal-dual hybrid gradient method (PDHG) originates from the Arrow-Hurwicz method.
The primary objective of load balancing for distributed systems is to minimize the job execution time while maximizing the resource utilization.
 Load balancing on decentralized systems need effective information exchange policy so that with minimum amount of communication the nodes have up to date information about other nodes in the system.
The primary working mechanism behind nanofluids thermal conductivity can be distinguished as one of the most controversial issues of nanofluids.
 Between several theories proposed in literature.
The principal component analysis has been widely used in various fields of research (e.
The problem of automatically constructing a software component such that when executed in a given environment satisfies a goal.
The problem of classifying traffic flows in networks has become more and more important in recent times.
The problem of computing k-edge connected components ( k-ECCs) of a graph G for a specific k is a fundamental graph problem and has been investigated recently.
 In this paper.
The problem of data representation is fundamental to the efficiency of search algorithms for the traveling salesman problem (TSP).
 The computational effort required to perform such tour operations as traversal and subpath reversal considerably influence algorithm design and performance.
 We propose a new data structure-the k-level satellite tree-for representing a TSP tour with a discussion of properties in the framework of general tour operations.
 The k-level satellite tree representation is shown to be significantly more efficient than its predecessors for large-scale instances.
The problem of finding optimal configuration of automated/smart power distribution systems topology is an NP-hard combinatorial optimization problem.
 It becomes more complex when the time varying nature of loads is taken into account.
 In this paper.
The problem of invariance of keys including simple keys is investigated as applied to operations of table algebras that are modern analogues of classical relational Codd algebras.
 It is shown that the keys being considered are invariant with respect to the operations of intersection.
The problem of stereoscopic image quality assessment.
The procedure for reuse of finite element method (FEM) programs for heat transfer and structure analysis to solve advanced thermo-mechanical problems is presented as powerful algorithm applicable for coupling of other physical fields (magnetic.
The procedure of sharing the diagnostic medical reports and the scanned images of patients among doctors in a secured fashion with corresponding suggestions for joint treatment creates greatest care for the patients through quicker and crucial decisions.
 Intrusion Detection is the essential part of Information and Network Security domain in opposition against illegal access or malicious attacks.
 In this context.
The process of agglutination is commonly used for the detection of biomarkers like proteins or viruses.
 The multiple bindings between micrometer sized particles.
The process of detecting plant disease by human naked-eye is difficult and very expensive practice.
The processing of large volumes of RDF data require an efficient storage and query processing engine that can scale well with the volume of data.
 The initial attempts to address this issue focused on optimizing native RDF stores as well as conventional relational databases management systems.
 But as the volume of RDF data grew to exponential proportions.
The production of biofuels is a process that requires the adjustment of multiple parameters.
 Performing experiments in which these parameters are changed and the outputs are analyzed is imperative.
The programming of heterogeneous clusters is inherently complex.
The proliferation of heterogeneous computing platforms presents the parallel computing community with new challenges.
 One such challenge entails evaluating the efficacy of such parallel architectures and identifying the architectural innovations that ultimately benefit applications.
 To address this challenge.
The proliferation of information technologies and the diversity of problem domains that heavily rely on software tool applications promote computer-supported cooperative work as a challenging discipline that drives the development process of contemporary and future engineering methods.
The proliferation of large number of images has made it necessary to develop systems for indexing and organizing images for easy access.
 This has made Content-Based Image Retrieval (CBIR) an important area of research in Computer Vision.
 This paper proposes a combination of features in multiresolution analysis framework for image retrieval.
 In this work.
The proliferation of low-cost IEEE 802.
4 ZigBee wireless devices in critical infrastructure applications presents security challenges.
 Network security commonly relies on bit-level credentials that are easily replicated and exploited by hackers.
 Unauthorized access can be mitigated by physical layer (PHY) security measures that exploit device-dependent emission characteristics that are sufficiently unique to discriminate devices.
 RF distinct native attribute (RF-DNA) fingerprinting is a PHY-based security measure.
The proliferation of malicious entities in the distributed environment poses various serious threats to the protection of Mobile Agent Platform (MAP).
 Numerous researches have been proposed to ward off the inherent security risks.
The prospect of Line-of-Business Services (LoBSs) for infrastructure of Emerging Sensor Networks (ESNs) is exciting.
 Access control remains a top challenge in this scenario as the service provider's server contains a lot of valuable resources.
 LoBSs' users are very diverse as they may come from a wide range of locations with vastly different characteristics.
 Cost of joining could be low and in many cases.
The prospects for successful peripheral nerve repair using fibre guides are considered to be enhanced by the use of a scaffold material.
The protection of the volatile memory data is an issue of crucial importance.
The protein subunit is the most important basic unit of protein.
The purpose of digital forensic applications is to determine the flow of events as they unfold.
The purpose of this article is to determine the usefulness of the K-means method applied to decompose and reduce the TERM-BY-DOCUMENT matrix with use of the GPU computations.
 The reduction based on the truncated SVD decomposition is a long-term process for large matrices.
 The computational complexity of the SVD decomposition is O(n(3)).
 The use of the K-means decomposition before reduction should reduce the computational complexity.
 In addition.
The purpose of this paper is to develop constructive versions of Stafford's theorems on the module structure of Weyl algebras A (n) (k) (i.
The purpose of this paper is to employ graphics processing units for the numerical solution of large systems of weakly singular Volterra Integral Equations (VIEs).
The purpose of this paper is to present a comparative study between relational and non-relational database models in a web-based application.
The purpose of this paper is twofold: to develop sound and complete rules.
The purpose of this study is to evaluate the effect of using Facebook on enriching the learning experiences of students in programming learning.
 In another word.
The purpose of this study is to evaluate transfer learning with deep convolutional neural networks for the classification of abdominal ultrasound images.
 Grayscale images from 185 consecutive clinical abdominal ultrasound studies were categorized into 11 categories based on the text annotation specified by the technologist for the image.
 Cropped images were rescaled to 256 x 256 resolution and randomized.
The purpose of this study is to examine the effectiveness of review question and content object as advanced organizer used for prior knowledge activation in an introductory computer programming.
 The students' engagement when using the strategies was examined to reach the primary findings.
 Content object (CO) as the advanced organizer to activate prior knowledge used before a new programming concept was learnt.
 Review questions (RQ) on programming concepts and solutions were designed to encourage the paper-pen method.
 Findings have shown similar performance in post-test.
 The outcome of this study showed CO useful to foster better learning programming.
 (C) 2015 Published by Elsevier Ltd.
The purpose of this study is to investigate whether the use of computer graphics software would affect the elementary school children's cognition.
 For the purpose.
The purpose of this study was to determine if modeling school and classroom effects was necessary in estimating passage reading growth across elementary grades.
 Longitudinal data from 8367 students in 2989 classrooms in 202 Reading First schools were used in this study and were obtained from the Progress Monitoring and Reporting Network maintained by the Florida Center for Reading Research.
 Oral reading fluency (ORF) was assessed four times per school year.
 Five growth models with varying levels of data (student.
The purpose of this study was to evaluate the correlation between histological invasiveness and the computed tomography (CT) value and size in pure ground-glass nodules (GGNs) to determine optimal follow-up or resection strategies.
 Between 2001 and 2014.
The qualities of color images captured by digital imaging devices are vulnerable to the scene illumination settings of a given environment.
 The colors of captured objects may not be accurately reproduced when the illumination settings are uncontrollable or not known a priori.
 This undesirable property can inevitably degrade the qualities of captured images and lead to difficulties in subsequent image-processing stages.
 Considering that the task of controlling scene illumination is nontrivial.
The quantitative simulation of forest fire spreading plays an essential role in designing quick risk management and implementing effective suppression policies.
 As a preferable modelling approach.
The Quantity Based Aggregation (QBA) policy is very simple.
The quaternion Fourier transform (QFT) is one of the key tools in studying color image processing.
The quest to observe gravitational waves challenges our ability to discriminate signals from detector noise.
 This issue is especially relevant for transient gravitational waves searches with a robust eyes wide open approach.
The Random Decrement Technique (RDT).
The rapid development of social networks.
The rapid development of the Internet.
The rapid development of the latest distributed computing paradigm.
The rapid development of urban areas due to population and infrastructure growth has led to increased traffic congestion.
The rapid growth of digital data storage of medical or health.
The rapid growth of location-based services has motivated the development of continuous range queries in networks.
 Existing query algorithms usually adopt an expansion tree to reuse the previous query results to get better efficiency.
The rapid growth of residential broadband connections and Internet-enabled home devices have driven the success of many useful applications such as video streaming and remote healthcare.
The rapid growth of unstructured data over the last few years.
The rapid increase of information and accessibility in recent years has activated a paradigm shift in algorithm design for artificial intelligence.
The rapid integration of technology into our professional and personal lives has left many education systems ill-equipped to deal with the influx of people seeking computing education.
 To improve computing education.
The rapid progress of high-performance computing entails new challenges related to solving large scientific problems for various subject domains in a heterogeneous distributed computing environment (e.
The real-time simulation of smoke sense of reality is a hotspot and difficulty in the research field of computer graphics.
 Based on the improved particle system method has carried on the real-time simulation to smoke.
The recent advancement in the field of distributed computing depicts a need of developing highly associative and less expensive cache memories for the state-of-art processors i.
The recent advancements in Wireless Sensor Network(WSN)s are the basis for many advanced technologies like lOTs.
 The constraint-full WSN has the support of operating systems like TinyOS.
The recent rise of the NoSQL movement motivates investigation on the performance impact that new persistence approaches can bring in the model-driven re-engineering of a consolidated object-oriented software architecture.
 We report comparative experimental performance results attained by combining a pattern-based domain logic with a persistence layer based on different paradigms and we describe how data model is persisted in various implementation based on MySQL.
The recent surge of network I/O performance has put enormous pressure on memory and software I/O processing subsystems.
 We argue that the primary reason for high memory and processing overheads is the inefficient use of these resources by current commodity network interface cards (NICs).
 We propose FlexNIC.
The recent usage control model (UCON) is a foundation for next-generation access control models with distinguishing properties of decision continuity and attribute mutability.
 A usage control decision is determined by combining authorizations.
The recent years have seen increasingly widespread use of highly concurrent data structures in both multi-core and distributed computing environments.
The recently developed slantlet transform (SLT for short) is a wavelet with two zero moments.
The refractive index is crucial in determining the detailed nature of the propagation of the electromagnetic waves in a medium.
 There is a growing demand for high resolution measurement of the refractive index because newly synthesized materials are usually small and optical devices and the elements of metamateirals are being miniaturized.
 In addition.
The region model analysis is an important method for analyzing the interior fire hazard.
 Based on the mass conservation equation and energy conservation equation of the pre-flashover interior fire hazard development.
The registration concept is one of the most important and popular aspects of digital image processing.
 Using suitable computer programming techniques and transformation between two images.
The regular hybrid flow shop scheduling problem.
The relationship between customers and suppliers remains a challenge in agile software development.
 Two trends seek to improve this relationship.
The reliability of RAID system can utilize reconstruction operations to recover data when the disk fails; however.
The reproducibility of experiments is one of the main principles of the scientific method.
The requirement to develop an organization makes collaboration with other organizations necessary.
The requirements concerning the technical availability as part of the overall equipment effectiveness increase constantly in production nowadays.
 Unplanned downtimes have to be prevented via efficient methods.
 Predictive.
The research method of identification of the electronic equipment's power-on characteristics based on the data of bus voltage ripple is introduced.
 The main purpose is to achieve the recognition of the equipment's power-on characteristics through a certain algorithm design.
The research on heat transfer models of geothermal ground heat exchangers (GHEs) of ground-coupled heat pump (GCHP) system has recently advanced greatly.
The research on underwater image segmentation has to deal with the rapid increasing volume of images and videos.
 To handle this issue.
The research purpose was to extend and adapt the principles of Service-Oriented Architecture while combining the patterns-oriented design and model-driven engineering approaches.
 We analyzed the existing approach for developing multi platform and multi-devices systems and studied four different approaches to the development: the Responsive Web Design.
The rise and fall of artificial neural networks is well documented in the scientific literature of both computer science and computational chemistry.
 Yet almost two decades later.
The rise in the cases of motor impairing illnesses demands the research for improvements in rehabilitation therapy.
 Due to the current situation that the service of the professional therapists cannot meet the need of the motor impaired subjects.
The rise of bioinformatics is a direct response to the political difficulties faced by genomics in its quest to be a new biomedical innovation.
The rise of location-based services has enabled many opportunities for content service providers to optimize the content delivery to user's wireless devices based on her location.
 Since the sharing precise location remains a major privacy concern among the users.
The Robust Conjugate Direction Search (RCDS) method is used to optimize the collimation system for the Rapid Cycling Synchrotron (RCS) of the China Spallation Neutron Source (CSNS).
 The parameters of secondary collimators are optimized for a better performance of the collimation system.
 To improve the efficiency of the optimization.
The Rogers-Ramanujan identities and various analogous identities (Gordon.
The role of arboviruses causing acute febrile illness in sub-Saharan Africa is receiving more attention.
 Reports of dengue in tourists were published nearly 10 years ago in Namibia.
The role of Boolean functions is prominent in several areas including cryptography.
The role of the design pattern in the form of software metric and internal code architecture for object-oriented design plays a critical role in software engineering regarding production cost efficiency.
 This paper discusses code reusability that is a frequently exercised cost saving methodology in IT production.
 After reviewing existing literature towards a study on software metrics.
The root-knot nematode.
The RSA cryptosystem and elliptic curve cryptography (ECC) have been used practically and widely in public key cryptography.
 The security of RSA and ECC respectively relies on the computational hardness of the integer factorization problem (IFP) and the elliptic curve discrete logarithm problem (ECDLP).
 In this paper.
The runjags package provides a set of interface functions to facilitate running Markov chain Monte Carlo models in JAGS from within R.
 Automated calculation of appropriate convergence and sample length diagnostics.
The salient region is the most important part of an image.
 The salient portion in images also attracts the most attention when people search for images in large-scale datasets.
The scalability of systems is essential in scenarios where there is a lot of concurrent users.
 Some systems have peak access during certain times; others have a lot of concurrent users regularly.
The scarcity of fossil fuels is affecting the efficiency of established modes of cargo transport within the transportation industry.
 Efforts have been made to develop innovative modes of transport that can be adopted for economic and environmental friendly operating systems.
 Solid material.
The screening of a number of chiral stationary phases (CSPs) with different modifiers in supercritical fluid chromatography to find a chromatographic method for separation of enantiomers can be time-consuming.
 Computational methods for data analysis were utilized to establish a hierarchical screening strategy.
The seasonality of fruits and vegetables makes it impossible to consume and use them throughout the year.
The second-order KdV equation was introduced as a model to describe the wave propagation in a weakly nonlinear and weakly dispersive system where the existence of multi-soliton solution is raised as an open question.
 In this paper.
The Secure Shell Protocol (SSH) is a well-known standard protocol.
The security game is a basic model for resource allocation in adversarial environments.
 Here there are two players.
The security implications of social bots are evident in consideration of the fact that data sharing and propagation functionality are well integrated with social media sites.
 Existing social bots primarily use Really Simple Syndication and OSN (online social network) application program interface to communicate with OSN servers.
 Researchers have profiled their behaviors well and have proposed various mechanisms to defend against them.
 We predict that a web test automation rootkit (WTAR) is a prospective approach for designing malicious social bots.
 In this paper.
The security of cryptographic systems is a major concern for cryptosystem designers.
The security of quantum key distribution (QKD) relies on the Heisenberg uncertainty principle.
The security of traditional identity-based signature (IBS) is totally built upon the assumption that the private key is absolutely secure.
The seed-and-expand scheme is appropriate for multiple view stereo.
The selection of which requirements should be implemented in the next software release is an important and complex task in the software development process.
The Semantic Web uses ontological descriptions.
The Semantic Web uses the Resource Description Framework (RDF) and the Simple Protocol and Query/Update Languages (SPARQL/SPARUL) as standardized logical data representation and manipulation models allowing machines to directly interpret data on the Web.
 As Semantic Web applications grow increasingly popular.
The semantics of programming languages is being studied for a long time.
The sequential Monte Carlo (SMC) implementation of the probability hypothesis density (PHD) filter suffers from low computational efficiency since a large number of particles are often required.
The serodiagnosis for tegumentary leishmaniasis (TL) presents problems related to the sensitivity and/or specificity of the tests.
 In the present study.
The Session Initiation Protocol (SIP) is a signaling communications protocol.
The session initiation protocol (SIP) is a signaling communications protocol.
The Session Initiation Protocol (SIP) is a signaling protocol widely applied in the world of multimedia communication.
 Numerous SIP authenticated key agreement schemes have been proposed with the purpose of ensuring security communication.
 Farash recently put forward an enhancement employing smart cards counted on Zhang et al.
 In this study.
The several noise level estimation algorithms that have been developed for use in image processing and computer graphics generally exhibit good performance.
The severity and number of attacks on computer networks are rapidly increasing.
 Most existing security solutions concentrate on resisting one or several attacks.
 We propose an integrated router security framework.
The severity of sustained injury resulting from assault-related violence can be minimised by reducing detection time.
The shift to the in-memory data processing paradigm has had a major influence on the development of cluster data processing frameworks.
 Numerous frameworks from the industry.
The shrinking processor feature and operating voltages of processor circuits are making them increasingly vulnerable to soft faults.
The SimProgramming teaching approach has the goal to help students overcome their learning difficulties in the transition from entry-level to advanced computer programming and prepare them for real-world labour environments.
The simulation of NC machining system with the use of the technology of the computer graphics.
The simulation of the light flux density distribution on a receiver plays an important role in energy estimation.
The simultaneous three-dimensional (3D) visualization of intracranial tumors.
The singularity in the traditional spherical polar coordinate system at the poles is a major factor in the lack of scalability of atmospheric models on massively parallel machines.
 Overset grids such as the Yin-Yang grid introduced by Kageyama and Sato [1] offer a potential solution to this problem.
 In this paper a three-dimensional.
The skyline operator determines points in a multidimensional dataset that offer some optimal trade-off.
 State-of-the-art CPU skyline algorithms exploit quad-tree partitioning with complex branching to minimise the number of point-to-point comparisons.
 Branch-phobic GPU skyline algorithms rely on compute throughput rather than partitioning.
The small-scale geologic inhomogeneities or discontinuities.
The Smart Grid control systems need to be protected from internal attacks within the perimeter.
 In Smart Grid.
The smart home field has witnessed rapid developments in recent years.
 Internet of Things applications for the smart home are very heterogeneous and continuously increasing in number.
The smart-grid concept takes the communications from the enclosed and protected environment of a substation to the wider city or nationwide area.
 In this environment.
The social relationships graph i.
 sociogram allows a transparent view of people in their surroundings to whom we have a certain relationship.
 Such options have already depicted many tools.
 They generally use data from social networks.
The software development process for embedded systems is getting faster and faster.
The softwarization of networks promises cost savings and better scalability of network functions by moving functionality from specialized devices into commercial off-the-shelf hardware.
 Generalized computing hardware offers many degrees for adjustment and tuning.
The soil-borne gram-positive bacteria Aneurinibacillus migulanus strain Nagano shows considerable potential as a biocontrol agent against plant diseases.
 In contrast.
The solution of large sparse systems of linear constraints is at the base of most interactive solvers for physically-based animation of soft body dynamics.
 We focus on applications with hard and tight per-frame resource budgets.
The Spiking Neural P (SN P) system is defined as a type of parallel computing mechanism bio-inspired by the behavior of the soma.
 Several authors have been employing these systems in order to create efficient arithmetic divisor circuits exploiting at maximum their intrinsic parallel processing.
The spin-change dynamics of a model with ultra-cold hyperfine-spin-1 atoms confined in an optical superlattice is discussed.
The spindle rotational accuracy is one of the important issues in a machine tool which affects the surface topography and dimensional accuracy of a workpiece.
 This paper presents a machine-vision-based approach to radial error measurement of a lathe spindle using a CMOS camera and a PC-based image processing system.
 In the present work.
The split feasibility problem (SFP).
The Square Kilometre Array (SKA) is an ambitious project aimed to build a radio telescope that will enable breakthrough science not possible with current facilities over the next 50 years.
 Because of this long expected operational period.
The staircase and fractional part functions are basic examples of real functions.
 They can be applied in several parts of mathematics.
The standard way of answering queries over incomplete databases is to compute certain answers.
The statelessness of functional computations facilitates both parallelism and fault recovery.
 Faults and non-uniform communication topologies are key challenges for emergent large scale parallel architectures.
 We report on HdpH and HdpH-RS.
The statistical analysis of massive and complex data sets will require the development of algorithms that depend on distributed computing and collaborative inference.
 Inspired by this.
The Statistical Learning Theory (SLT) defines five assumptions to ensure learning for supervised algorithms.
 Data independency is one of those assumptions.
The Steady State Superconducting Tokamak (SST-1) consists of many distributed and heterogeneous plant/experiment systems viz.
 Water-Cooling.
The steep learning curve associated with computer programming can be a daunting prospect.
The structural design paradigm has evolved from an engineering-oriented approach to a holistic one that accommodates various concerns including form.
The student dropout rate in universities is fascinating.
The study aimed to determine if computer vision techniques rooted in deep learning can use a small set of radiographs to perform clinically relevant image classification with high fidelity.
 One thousand eight hundred eighty-five chest radiographs on 909 patients obtained between January 2013 and July 2015 at our institution were retrieved and anonymized.
 The source images were manually annotated as frontal or lateral and randomly divided into training.
The study for the establishment of a complete flight information system that Provide airport management authorities as passengers aboard the aircraft before departure and after the arrival of the various kinds of information.
The study of sending and receiving secret messages is called cryptography.
The study of the classification of Apples and Oranges in a warehouse has been undertaken in a three-qubit system using the method of repeated iterations in Grover's algorithm and Ventura's algorithm separately.
 Operator describing an inversion about average has been constructed as a square matrix of order eight.
The study of the methodologies useful to support the assembly of parts is a challenging engineering task which can benefit of the most recent innovations in computer graphics and visualization technologies.
 This paper presents a proposal for an innovative methodology based on Virtual and Augmented Reality useful to support the components' assembly.
 The herein introduced strategy is based upon a four stages procedure: at first the designer conceives the assembly sequence using a CAD system.
The study of tumor growth biology with computer-based models is currently an area of active research.
 Different simulation techniques can be used to describe the complexity of any real tumor behavior.
The style and dynamics of volcanic eruptions control the level and type of hazards posed for local populations and can have a temporary long-range impact on climate if eruptions are extremely energetic.
 The purpose of this study is to provide a statistical approach to ash morphometrics in order to provide a means by which to evaluate diverse eruption styles and mechanisms of fragmentation.
 The methodology presented can be applied to tephra deposits worldwide and may aid volcanic hazard mitigation by better defining a volcano's history of explosive behavior.
 Ash-sized grains were collected from tephra deposits on Mount Erebus.
The subject of this paper is the detection and mitigation of data injection attacks in randomized average consensus gossip algorithms.
 It is broadly known that the main advantages of randomized average consensus gossip are its fault tolerance and distributed nature.
 Unfortunately.
The success of machine learning algorithms strongly depends on the feature extraction and data representation stages.
 Classification and estimation of small repetitive signals masked by relatively large noise usually requires recording and processing several different realizations of the signal of interest.
 This is one of the main signal processing problems to solve when estimating or classifying P300 evoked potentials in brain-computer interfaces.
 To cope with this issue we propose a novel autoencoder variation.
The successful use of intelligent agents in healthcare has attracted researchers to apply this emerging software engineering paradigm in more advanced and complex applications.
 Main success factor is the natural mapping of real world medical problems into cyber world.
 Multi-agent architecture can easily model the heterogeneous.
The succession of archaeological units at Notarchirico offers technical and environmental arguments aimed at the characterization of the variability of the European Ancient Palaeolithic.
 Analysis shows the presence of distinctive features within the Acheulean assemblage besides the presence of handaxes.
 Hominins used flint and limestone on the site: a lithological analysis shows a predominance of the same raw materials in every unit and a partial selective exploitation.
The suffix array is an important indexing data structure for biological sequence analysis.
 The increasing size of genomic data necessitates the use of a computer cluster to speed up the computation.
 In this paper.
The suites of numerical models used for simulating climate of our planet are usually run on dedicated high-performance computing (HPC) resources.
 This study investigates an alternative to the usual approach.
The surface flow generated by a bubble plume is a technique proposed to collect the surface-floating substances.
 This has a great importance when handling the oil layer formed during large oil spill accidents due to the need to protect naval systems.
The surface roughness of spaghetti is an important factor influencing noodle quality during and after cooking.
 In this study.
The synthesis of patient data of a certain medical image modality by applying an image processing pipeline starting from other modality is receiving a lot of interest recently.
The system is designed to protect the key data of enterprise.
 It consists of three modules: module for host information security monitoring and auditing.
The target of this research is to determine how program slicing contributes to program comprehension and to enhance its functionality by applying the slicing tree concept to its implementation.
 Slicing tree is a concept that refers to automatically repeating program slicing while the slicing criterion is changeable until the program decomposes into its atomic parts.
 Using this technique offers several advantages over traditional program slicing.
The task of epitope discovery and vaccine design is increasingly reliant on bioinformatics analytic tools and access to depositories of curated data relevant to immune reactions and specific pathogens.
 The Immune Epitope Database and Analysis Resource (IEDB) was indeed created to assist biomedical researchers in the development of new vaccines.
The task of generating network-based evidence to support network forensic investigation is becoming increasingly prominent.
 Undoubtedly.
The task of generating network-based evidence to support network forensic investigation is becoming increasingly prominent.
 Undoubtedly.
The teaching and evaluation of computer programming is very difficult.
 The checking of lab topics and making recommendations to all students is a time consuming activity and therefore each teacher had to use tools to automatically check the solutions for problems and exercises proposed.
 In this article we designed a MOOC (Massive Open Online Course) type application that can be integrated into LMS (Learning Management System) Moodle.
 This learning environment provides a user interface through which the student interacts by proposing solutions to the proposed problems meanwhile evaluating the results.
 In recent years the e-learning applications have developed a lot and that is why support platforms for e-learning have evolved proposing new approaches and a range of technical solutions.
 These solutions have transformed the learning / evaluation in a more efficient activity due the immediately feed-back offered to students.
 In many universities are using MOOC platforms.
The teaching of algorithms and programming concepts to students of the first years of Computer Science course has been a major challenge.
The technological advancements in mobile connectivity services such as GPRS.
The technology of VR (Virtual Reality) is the result of the progress of science and technology since the 20th century.
The term 'crowdsourcing' was initially introduced in 2006 to describe an emerging distributed problem solving model by online workers.
 Since then it has been widely studied and practiced to support software engineering.
 In this paper we provide a comprehensive survey of the use of crowdsourcing in software engineering.
The term NoSql was coined in order to unify numerous products that enable data persistence and access in rather different ways.
The Test Suite Minimization Problem (TSMP) is a NP-hard real-world problem that arises in the field of software engineering.
 It consists in selecting a minimal set of test cases from a large test suite.
The theory of chaos and chaotic neural networks (CNNs) has been widely investigated in the past two decades.
The theory of network reliability has been applied to many complicated network structures.
The theory of three-way decisions is to consider a decision-making problem as a ternary classification one which is realized by the acceptance.
The tissue microenvironment functions as a crucial player in carcinogenesis.
The Top500 supercomputers ranking has been held twice a year according to Linpack performance for more than 20years.
The topology of interconnection networks plays an important role in the performance of parallel and distributed computing systems.
 In this paper.
The torsion distribution of cell paths in two-phase flow of a cell suspension through a porous medium is an important quantity for chromatographic processes.
 It can be estimated from cell tracking in sequences of three-dimensional images of a suspension pumped through the medium.
The traditional bag-of-words approach has found a wide range of applications in computer vision.
 The standard pipeline consists of a generation of a visual vocabulary.
The traditional k out of n Visual Secret Sharing (VSS) scheme encodes a secret binary image into n shares of random pattern.
 If the shares are printed onto transparencies.
The traditional OLAP (On-Line Analytical Processing) systems store data in relational databases.
 Unfortunately.
The traditional RDBMS has been consistent for the normalized data structures.
 RDBMS served well for decades.
The traditional research of information dissemination is mostly based on the virus spreading model that the information is being spread by probability.
The traditional rubber hand illusion is a psychological experiment where participants are under the illusion that a rubber hand is part of their own body.
 This paper examines the use of real.
The Traffic Aware Planner (TAP).
The transformation of constraint logic programs (CLP programs) has been shown to be an effective methodology for verifying properties of imperative programs.
 By following this methodology.
The transition phenomenon of few-cycle-pulse optical solitons from a pure modified Korteweg-de Vries (mKdV) to a pure sine-Gordon regime can be described by the non-autonomous mKdV-sinh-Gordon equation with time-dependent coefficients.
 Based on the Bell polynomials.
The transparent plates (such as organic glass.
The travelling salesman problem (TSP) is a well-known NP-hard problem.
 It is difficult to efficiently find the solution of TSP even with the large number of gene instances.
 Evolutionary approaches such as genetic algorithm have been widely applied to explore the huge search space of TSP.
The tremendous interest in enzymes as biocatalysts has led to extensive work in enzyme engineering.
The tremendous parallel computing ability of cloud computing encourages investigators to research its drawbacks and advantages on processing large-scale scientific applications such as workflows.
 The current cloud market is composed of numerous diverse public clouds and a local private cloud.
The Triboelectric Nanogenerator (TENG).
The TV-centric gaming concept creates an environment where people can play games with each other in the living room gathered around the TV screen.
 Games are controlled through mobile devices.
The twenty-first century vision for toxicology involves a transition away from high-dose animal studies to in vitro and computational models (NRC in Toxicity testing in the 21st century: a vision and a strategy.
 is reported to have one of the most developed green building markets.
 The country wide success must start from the success of different local markets.
 So how many important local green building markets are there in the U.
 and why those areas became important? The answers can provide useful implications for developing countries like China to better promote their green building markets.
 To explore the question in a numerical way.
The uncontrolled nature of user-assigned tags makes them prone to various inconsistencies caused by spelling variations.
The use of composite materials in industry applications is constantly growing.
The use of computer programming in K-12 spread into schools worldwide in the 70s and 80s of the last century.
The use of computer vision techniques in post-harvest processing of agricultural products has increased considerably in recent years due to their non-destructive and rapid monitoring abilities.
 Image processing.
The use of computers and complex software is pervasive in archaeology.
The use of computers in statistical physics is common because the sheer number of equations that describe the behaviour of an entire system particle by particle often makes it impossible to solve them exactly.
 Monte Carlo methods form a particularly important class of numerical methods for solving problems in statistical physics.
 Although these methods are simple in principle.
The use of mobile devices.
The use of narrative structures in learning environments for teaching computer programming is an acknowledgment of the storytelling importance in the classroom.
 Automatic generation of such narratives is one of the research themes of GEMS.
 This paper presents an adaptive specification of the mechanism that generates narratives considering only the characters emotional facet.
 The writing process the of specification included an emotional state machine ornated with adaptive methods.
 Such methods help to organize the behavioral specification of the state machine.
 The end result takes the form of an executable prototype which demonstrates the operation of the narratives generation mechanism.
The use of proteomics bioinformatics substantially contributes to an improved understanding of proteomes.
The use of smartphone technology is increasingly considered a state-of-the-art practice in travel data collection.
 Researchers have investigated various methods to automatically predict trip characteristics based upon locational and other smartphone sensing data.
 Of the trip characteristics being studied.
The use of surrogate models is a standard method for dealing with complex real-world optimization problems.
 The first surrogate models were applied to continuous optimization problems.
 In recent years.
The use of ultrasound (US) imaging as an alternative for real-time computer assisted interventions is increasing.
 Growing usage of US occurs despite of US lower imaging quality compared to other techniques and its difficulty to be used with image analysis algorithms.
 On the other hand.
The use of Unmanned Aerial Vehicles (UAVs) is growing significantly for many and varied purposes.
 During the mission.
The use of virtual prototypes and digital models containing thousands of individual objects is commonplace in complex industrial applications like the cooperative design of huge ships.
 Designers are interested in selecting and editing specific sets of objects during the interactive inspection sessions.
 This is however not supported by standard visualization systems for huge models.
 In this paper we discuss in detail the concept of rendering front in multiresolution trees.
The use of Virtual Reality environments in power substations offers a new paradigm for supervisory control.
 The existence of a tridimensional model.
The use of white light based Three Fringe Photoelasticity (TFP)/RGB Photoelasticity has gained importance in the recent years.
 With recent advances in TFP.
The usefulness of online and hybrid delivery methods in education has long been realized and with the advancement of computer and communication technologies and the Web based authoring tools.
The utilization of ontologies in knowledge management has become very well accepted in many approaches for instance database.
The validation of any database mining methodology goes through an evaluation process where benchmarks availability is essential.
 In this paper.
The verification and validation (V&V) of agent-based models (ABMs) is challenging.
 The underlying structure of the model and the agents can change over time.
 Furthermore.
The vertical distribution of plant physiological composition plays an important role in vegetation growth and carbon stock.
 The hyperspectral LiDAR was thought to be the most promising solution to assess the vertical distribution of vegetation physiological parameters.
The VIALACTEA project has a work package dedicated to Tools and Infrastructure and.
The video game industry is becoming increasingly important due to its revenues and growing capabilities.
 User eXperience (UX) is an important factor which contributes to the acceptance of a video game.
 The UX is usually assessed at the end of the development process.
The virtualization has been considered as one of core technologies in cloud computing to overcome low efficiency of resource utilization for physical machines by multiplexing operating systems and/or applications on physical servers in a form of virtual machine.
The visibility of retinal microvasculature in optical coherence tomography angiography (OCT-A) images is negatively affected by the small dimension of the capillaries.
The vision of the surrounding and people that are within eyeshot influences the human well-being and safety.
 The rationale of system development that allows recognizing faces from difficult perspectives online and informing timely about approaching people is undisputed.
 The manuscript describes the methods of automatic detection of equilibrium face points in the bitmap image and methods of forming 3D face model.
 The optimal search algorithm for equilibrium points has been chosen.
 The method of forming 3D face model basing on a single bitmap image and building up the face image rotated to the preset angle has been proposed.
 The algorithm for estimating the angle and algorithm of the face image rotation have been implemented.
 The manuscript also reviews the existing methods of forming 3D face model.
 The algorithm for the formation of 3D face model from a single bitmap image and a set of individual 3D models have been proposed as well as the algorithm for forming different face angles with the calculated 3D face model aimed to create biometric vectors cluster.
 Operation results of the algorithm for face images formation from different angles have been presented.
 (C) 2017 Published by Future Academy www.
FutureAcademy.
The volume and variety of data published on the Semantic Web is constantly increasing with a growing number of entities and stakeholders expressing their data in the form of OWL and/or RDFS ontologies.
The volume of data generated by modern astronomical telescopes is extremely large and rapidly growing.
The way data structures organize data is often a function of the sequence of past operations.
 The organization of data is referred to as the data structure's state.
The weak separation between user-and kernel-space in modern operating systems facilitates several forms of privilege escalation.
 This paper provides a survey of protection techniques.
The web browser is the new desktop.
 Not only do many users spend most of their time using the browser.
The white-box attack is a new attack context in which it is assumed that cryptographic software is implemented on an un-trusted platform and all the implementation details are controlled by the attackers.
The widespread dissemination of smart mobile devices offers new perspectives for timely data collection in large-scale scenarios.
The widespread distribution of smart mobile devices offers promising perspectives for the timely collection of huge amounts of data.
 When realizing sophisticated mobile data collection applications.
The widespread use of smart mobile devices (e.
The widespread use of the Internet has significantly changed the behavior of homebuyers.
 Using online real estate agents.
The wireless local area network (WLAN) communication is a rapidly growing approach for data sharing.
 A wireless network provides network access to mobile devices.
 Benefits of WLAN are like flexibility.
The wireless sensor network (WSN) is more vulnerable than the wired network because it is relatively easy for an adversary to eavesdrop.
The wireless sensor network represents distributed environment that consists of sensors and actuators to monitor and control real-world objects and devices.
 Web services and service-oriented architecture are fundamental elements to provide on-demand applications.
 The integration of business processes with wireless sensor network and service-oriented architecture will result in a better alignment of enterprise applications.
 This article uses multiple-domain matrix extensions called the business process model and notation-multiple-domain matrix to analyze structures of and relations among processes modeled in the business process model and notation with wireless sensor network elements and services.
 By applying an organization-activity multiple-domain matrix partitioning algorithm.
The word Open Source is everywhere with Linux Technology and GNU foundation.
 In addition to open source software's and operating systems.
the work focuses on the development of a software solution for observation and management of an industrial unit for automation.
 The actual software solution is designed for tablets and mobile phones running the Android operating system.
 All presented paradigms can also be applied to other mobile platforms and operating systems.
 Custom software allows the operator to control and manage devices remotely.
The work reported here focuses on the controllability expressions in the mathematical modeling of dehydration process of food concentrates in producing powder using spray-DIC (spray-Detente Instantanee Controlee or spray-instant controlled pressure drop).
 This paper presents the second-order partial differential equations for mathematical modeling of moisture and heat transfer in spray-DIC process.
 This paper proposes the enhancement in the simple model of DIC technique with controllability expression to be used in the spray-DIC.
 The controllability expression in the drying process models gives better results when compared to the models without the controllability expression.
 The results were computed and shown by MATLAB 2013 with Windows 8 operating systems.
 The controllability expression in dehydration process model using the spray-DIC drier manage to succesfully control the dehydration process.
The working status of huge rotating mechanical equipment can be judged with the method of detection and processing its acoustics.
 It is analyzed that the variation of the sound reflected the variation of the material percentage in the equipment.
 The sound caused by different material percentage is collected and analyzed in time and frequency domain in this paper.
 A practical material percentage detection algorithm is proposed.
 The frequency spectrum of quantitative analysis for the material percentage is determined.
 The procedure of algorithm design includes calibration and quantitative detection.
 The field detection results show its effectiveness.
The World Health Organization estimates that nearly 500 million malaria tests are performed annually.
 While microscopy and rapid diagnostic tests (RDTs) are the main diagnostic approaches.
The World Wide Web has evolved gradually from a document delivery platform to an architecture for distributed programming.
 This largely unplanned evolution is apparent in the set of interconnected languages and protocols that any Web application must manage.
 This paper presents Ur/Web.
The worldwide mobile software market has grown dramatically since feature phones became popular in the early 1990s.
 In practice.
The woven fabric is a flexible object and to specify its parameters.
The writers propose a successive improved dynamic programming (SIDP) algorithm for hydropower reservoir operation based on an analysis of concavity.
The WRKY family.
Theileria equi and Babesia caballi are the causative agents of equine piroplasmosis (EP).
There are billions of lines of sequential code inside nowadays' software which do not benefit from the parallelism available in modern multicore architectures.
 Automatically parallelizing sequential code.
There are currently more mobile devices than people on the planet.
 This number is likely to multiply many folds with the Internet of Things revolution in the next few years.
 This may treasure an unprecedented computational power especially with the wide spread of multicore processors on mobile phones.
 This paper investigates and proposes a new methodology for mobile cluster computing.
There are many data pre-processing techniques that aim at enhancing the quality of classifiers induced by machine learning algorithms.
 Functional expansions (FE) are one of such techniques.
There are many datastore systems to choose from that differ in many ways including public versus private cloud support.
There are many types of dependencies between software requirements.
There are n people.
There are numerous reasons leading to change in software such as changing requirements.
There are repetitive patterns in strategies of manipulating source code.
 For example.
There are several commercial financial expert systems that can be used for trading on the stock exchange.
There are some quasi-tridiagonal system of linear equations arising from numerical simulations.
There are some unobvious dead lock problems caused by inappropriate acquiring and releasing resources among current transactions.
There are two solutions in data security field for ensuring that only legitimate recipients will have access to the intended data: Steganography and cryptography.
 These solutions can be used for providing a high level of security.
 With the exponential growth of challenges in the field of computer security.
There are various methods for the identification of duplicate records.
 Besides the importance of methods quality.
There exist numerous state of the art classification algorithms that are designed to handle the data with nominal or binary class labels.
There exist two key problems about data aggregation that should be thoroughly explored - algorithm design in networking layer.
There has been significant recent interest in sensing systems and 'smart environments'.
There have been many changes in statistical theory in the past 30 years.
There have been many robust image steganography methods that are invented in recent decades.
There is a challenge of developing computer programming visual language in the sphere of interdisciplinary research - cybernetics and philosophy.
There is a lot of emphasis on theoretical education rather than practical ability training.
There is a misconception of what programming is at the early stages of learning programming for Computer Science (CS) minors.
 More researches in this field have revealed that the lack of problem-solving skills.
There is a strong movement asserting the importance of quality education all over the world and for students of all ages.
 Many educators believe that in order to achieve this 21st century skills must be taught and that digital literacy should be coupled with rigorous Computer Science principles and computational thinking.
 Accordingly this work will describe a didactic experience in an introductory programming course by describing the context.
There is a strong relationship between scientific research and technology advancement.
 The former generally focuses on studying phenomena happening in the real world.
There is an enormous growth of industrial applications using internet communication.
 Secure network is a prime objective for the survival of any organization.
 Network monitoring and defence systems have become an integral part of network security for identifying and preventing potential attacks.
 Intrusion Detection and Prevention Systems (IDPS) are network based defence systems which combines Intrusion Detection System (IDS) and a firewall.
 In contrast to IDS.
There is an increasing demand for personalization of disease screening based on assessment of patient risk and other characteristics.
 For example.
There is an increasing demand for tools that support land use planning processes.
There is an increasing interest in the field of parallel and distributed data mining in grid environment over the past decade.
 As an important branch of spatial data mining.
There is an increasing need for building models that permit interior navigation.
There is collection of methods for finding explicit travelling wave solutions of nonlinear partial differential equations (NLPDEs) in the literature.
 Quite a large amount of these methods employ Balancing Principle in their methodology and they work for positive integer values only.
 Yet there is no well established formula for balancing and there is no specific rule to determine adequate balancing principle that works for any number which is positive/or negative and/or rational.
 In this study.
There is little or no information available on what actually happens when a software vulnerability is detected.
 We performed an empirical study on reporters of the three most prominent security vulnerabilities: buffer overflow.
Thermal wave radar imaging (TWRI) was developed to detect manufacturing cracks in automotive powder metallurgy components (transmission sprockets) in their green (unsintered) state.
 The crack detection capability of the TWRI phase was validated by two sets of cracked/crack-free green and sintered sprockets which were sectioned after TWRI measurements.
 An automatic defect recognition (ADR) TWR image processing method was also developed to differentiate cracks from local defects.
 Measurement results demonstrated that TWRI is superior to conventional lock-in thermography imaging (LITI) in both flaw detection resolution and speed.
Thermodynamic data are needed for all kinds of simulations of materials processes.
 Thermodynamics determines the set of stable phases and also provides chemical potentials.
Thermodynamic processes are complex phenomena that can be understood as a set of successive stages.
 When treating processes.
These days the number of processors in computer systems is increasing in a very fast speed.
Thin cap fibroatheroma (TCFA) or vulnerable plaque is responsible for the majority of coronary artery death.
 Virtual Histology Intravascular Ultrasound (VH-IVUS) image is a clinically available method for visualizing color coded tissue maps.
This article addresses the effects of glycerol (GLY) concentrations on the mechanical properties of calcium polyphosphate (CPP) bone substitute structures manufactured using binder jetting additive manufacturing.
 To achieve this goal.
This article addresses the problem of creating interactive mixed reality applications where virtual objects interact in images of real world scenarios.
 This is relevant to create games and architectural or space planning applications that interact with visual elements in the images such as walls.
This article addresses the problem of testing the difference between two correlated agreement coefficients for statistical significance.
 A number of authors have proposed methods for testing the difference between two correlated kappa coefficients.
This article adopts Computer simulation and graph theoretic algorithm to choose the most reasonable place to convey drugs.
 And this article use mathematical modeling and computer programming to solve the problem of transport drugs.
This article advocates the use of new architectural features commonly found in many-cores to replace the machine model underlying Unix-like operating systems.
 We present a general Abstract Many-core Machine Model (AM(3)).
This article analyses the knowledge needed to understand a computer program within the philosophy of information.
 Floridi's method of levels of abstraction is applied to the relation between an ideal programmer and a modern computer seen together as an informational organism.
 The results obtained by the mental experiment known as the Knowledge Game are applied to this relation.
This article analyzes dynamic changes in mobile phone popularity based on phone features.
 The time period.
This article applies Item Response Models into College Computer Programming Language Test.
 According to the different characteristics of Programming Language Test.
This article argues that the study of literary representations of landscapes can be aided and enriched by the application of digital geographic technologies.
 As an example.
This article considers the ensuring of secure data processing in distributed computer systems (DCSs).
This article considers up-to-date problems of parallel computing systems development in Russia.
 The study describes an engineering of an interface for organization of high-performance system with programmable structure and for its partition to specific subsystems focused on applied tasks.
 The solution is based on the concept of homogeneous computing systems (HCS) relying on the model of collective of calculators.
 HCS concept was formulated in the Institute of Mathematics of the USSR under the leadership of E.
 Evreinov in 1962 [1].
 The approach is particularly relevant in terms of growing popularity of information networks which combining numerous personal computers (PCs).
 It's obviously necessary to develop software tools to integrate disparate calculators into the system and identify the required number of sub-systems for specific problem solving.
 An interference into the hardware and software parts of the calculators must be avoided during the software development process.
 Programmable structure of the computer system is provided at the stage of unification of calculators into software subsystem interface called an agent of DCS PS.
 The experience of the development of the Interface organization of distributed computing systems with programmable structure and allocation of subsystems for specific applications.
 The software described in the present document has been tested while an experimental organization of the DCS PS at the department of the computer engineering of NSTU.
This article contains a description of the structure.
This article deals with information system of structural monitoring.
This article deals with the main features of accessibility implemented by a mobile operating system.
This article defines a new way to perform intuitive and geometrically faithful regressions on histogram-valued data.
 It leverages the theory of optimal transport.
This article describes a new open source scientific workflow system.
This article describes how practical lectures can be innovated by the experience of a practical case study.
 The design and implementation of the geographical information system BotanGIS brought several innovations to the study branch Geoinformatics at the Faculty of Science of Palacky University in Olomouc.
 This article deals mainly with introducing a new practical example into the course Database Systems.
 This new theme explains the design and inner structure of the relational database BotanGIS.
 The valuable contribution lies in the way a real database example from practice is explained to the students in the process of education.
 The instruction started with a visit to the botanical garden and the collection greenhouses.
 Subsequently.
This article describes the approach of experts of the National Research Moscow State University of Civil Engineering (MGSU) to the construction of modern operating systems in buildings using BIM-technology.
 This article was performed within the Russian State task.
This article discusses the potential of BNs to complement the analytical toolkit of agricultural extension.
 Statistical modelling of the adoption of agricultural practices has tended to use categorical (logit/probit) regression models focusing on a single technology or practice.
This article discusses the related art network security management.
This article elaborates an evaluation of seven software requirements prioritization methods (ANP.
This article examines the effectiveness of different forms of performance-based adaptive automation (PBAA).
 Using data from three experiments (N = 10.
This article explains.
This article focuses on testing a path-oriented querying approach to hierarchical data in relational databases.
 The authors execute a user study to compare the path-oriented approach and traditional SQL from two perspectives: correctness of queries and time spent in querying.
 They also analyze what kinds of errors are typical in path-oriented SQL.
 Path-oriented query languages are popular in the context of object-orientation and XML.
This article has probed an innovative conversion from micromouse competition to a systematic curriculum design that integrates practice with theory.
 Micromouse competition is an event where small robot mice are designed to solve a 16x16 maze.
 The process includes multi-disciplinary knowledge such as electrical and computer engineering.
This article introduces the educational functions of a computer program developed and put into practice for the computer-aided instruction of the finite element method.
 An interactive simulation with graphical visualization is used as a tool to teach and learn the concepts and procedures related to the construction and solution of finite element stiffness equations.
 The tool encompasses the stages of processing finite element equations from element modeling to equation solving.
 Any operation of this educational tool is not an emulation of computational processes.
This article investigates the evolution of data quality issues from traditional structured data managed in relational databases to Big Data.
 In particular.
This article is about the assessment of several tools for k-mer counting.
This article leads the concept of data visualization into the huge amounts of news data.
This article presents a cognitive agent-based software tool for teaching software change control decisions.
 The tool simulates the cognitive processes of software professionals involved in the software change decision.
 It allows students to assess their reasoning in comparison with the agent's one.
 The experimental results proved that the proposed tool was well accepted by students.
 (C) 2015 Wiley Periodicals.
This article presents a new method to optimally partition a geometric domain with capacity constraints on the partitioned regions.
 It is an important problem in many fields.
This article presents a parallel algorithm to model the nonlinear dynamic interactions between ultrasonic guided waves and fatigue cracks.
 The Local Interaction Simulation Approach (LISA) is further developed to capture the contact-impact clapping phenomena during the wave crack interactions based on the penalty method.
 Initial opening and closure distributions are considered to approximate the 3-D rough crack microscopic features.
 A Coulomb friction model is integrated to capture the stick-slip contact motions between the crack surfaces.
 The LISA procedure is parallelized via the Compute Unified Device Architecture (CUDA).
This article presents a proposal for the detection of programming source code similitude in academic environments.
 The objective of this proposal is to provide support to professors in detecting plagiarism in student homework assignments in introductory computer programming courses.
 The developed tool.
This article presents a robotic dataset collected from the largest underground copper mine in the world.
 The sensor measurements from a 3D scanning lidar.
This article presents a signature-free distributed algorithm which builds an atomic read/write shared memory on top of a fully connected peer-to-peer n-process asynchronous message-passing system in which up to t < n/3 processes may commit Byzantine failures.
 From a conceptual point of view.
This article presents a summary of the development of the computer code AZKIND.
 This code is based on multi-group neutron diffusion theory.
This article presents an implementation of Otsu's segmentation method and a case study using multiple images.
 Otsu's method performs nonparametric and unsupervised image thresholding.
This article presents an instructional framework for collaborative learning.
This article presents an introductory microcontroller programming course on digital signal processing for undergraduate university level.
 The course is intended to provide insight into information technology and to prepare students for more complex exercises later on in their studies.
 Solutions to overcome pedagogical obstacles like the fear of new technologies and to minimise technological incompatibilities between different operating systems while setting up a programming tool chain are presented.
 This leads to an increased scalability of the course.
This article presents an ongoing research project on the development of e-content in relation to the cognitive theory of multimedia learning.
 Previously.
This article presents EarSketch.
This article presents recent efforts in improving the efficiency and scalability of the mixed cell computation step in the context of the Polyhedral Homotopy method.
 Solving systems of polynomial equations is an important problem in applied mathematics.
 The Polyhedral Homotopy method is an important numerical method for this task.
 In this method.
This article presents results of experiments.
This article presents the core elements of a cross-platform tactile capabilities interface (TCI) for humanoid arms.
 The aim of the interface is to reduce the cost of developing humanoid robot capabilities by supporting reuse through cross-platform deployment.
 The article presents a comparative analysis of existing robot middleware frameworks as well as the technical details of the TCI framework that builds on the existing YARP platform.
This article presents vision-based formation flight control for aerial robots with a special focus on failure conditions in visual communication.
This article proposes a novel architecture to perform modular multiplication in the Residue Number System (RNS) by using sum of residues.
 The highly parallel architecture is implemented using VHDL and verified by extensive simulations in ModelSim SE.
 The pipelined and non-pipelined versions of the design are implemented on ASIC and FPGA platforms to allow a broad comparison.
 The proposed architecture requires only one iteration to complete modular multiplication and achieves 12-90 % less delay as compared to the existing RNS and binary modular multipliers.
 The complexity of the proposed design is also less than the existing state-of-the-art RNS-based modular multipliers.
 The high scalability and flexibility of the proposed architecture allows it to be used for a wide range of high-speed applications.
This article proposes an automatic image processing method that can be an effective diagnostic tool to detect and grade the severity of diabetic retinopathy.
 This computer vision-based algorithm imitates the logic and medical sense used by ophthalmologist in detecting the abnormality and its location in the image for grading the severity of the disease.
 The detection is based on finding abnormalities.
This article provides an enriched technology acceptance model explaining the impact of both classic and additional variables on software engineering tools acceptance within Information Systems Development courses.
This article reports an algebraic criterion of the eigenvalue assignment.
This article reports on the development of two courses designed for students in higher education game development programs during the period of 2006 to 2015.
 The students are from three different arts and design-related strands of the program.
This article studies the application of multiobjective evolutionary algorithms for solving the energy-aware scheduling problem of workflows in a distributed system that is composed by a federation of datacenters.
This chapter discusses several methods for forward-looking (FL) explosive hazard detection (EHD) using FL infrared (FLIR) and FL ground penetrating radar (FLGPR).
 The challenge in detecting explosive hazards with FL sensors is that there are multiple types of targets buried at different depths in a highly-cluttered environment.
 A wide array of target and clutter signatures exist.
This comprehensive.
This contribution deals with the potential of the so called active knowledge database in conjunction with the agricultural resort.
 The authors of this contribution initially focus attention on the defining the term active databases.
This contribution presents the functionalities and multimedia contents of the e-learning module on virtual prototyping of garments within the ERASMUS+ project entitled e-Learning Course for Innovative Textile Fields - Advan2Tex.
 Use of advanced information technologies and systems can assure the textile and garment manufacturing companies the competitive advantages.
This demonstration illustrates the possibilities of new 3D technologies in conveying large scale historical photographic databases in interactive 3D virtual environments.
 We illustrate the visualization of the State Library of Western Australia (SLWA)'s photographic collection containing over 1 million photographs dating back to the 1850s utilizing Curtin's Hub for Immersive Visualization and eResearch (HIVE).
 Our application was intended to explore the possibilities in visualizing cultural data sets on the HIVE's Cylinder.
This essay examines the consequences of the so-called 'big data' technologies in biomedicine.
 Analyzing algorithms and data structures used by biologists can provide insight into how biologists perceive and understand their objects of study.
This essay offers paths for scholars influenced by the critical social sciences and theoretical humanities to contribute to the construction of concepts and digital practices of data that will allow data to better align with their approaches to scholarly inquiry.
 In particular.
This essay provides practical advice about how to do transparent and reproducible data analysis and writing.
 We note that doing research in this way today will not only improve the cumulation of knowledge within a discipline.
This exploratory work is concerned with generation of natural language descriptions that can be used for video retrieval applications.
 It is a step ahead of keyword-based tagging as it captures relations between keywords associated with videos.
This is a theoretical study on the interfacial water waves with a free surface in a two-layer system.
This is the first report on a myophage that infects Arthrobacter.
 A novel virus.
This letter presents an image processing technique based on the theory of the Hilbert transform to determine the coastal bathymetry from marine radar image sequences.
 Use of the Hilbert transform enables the difficulties and complications of inhomogeneous image analysis to be avoided.
 In addition.
This note analyzes the relationship between economic crises and tourism performance in Spain during the period 1970-2013 using machine learning techniques.
 Specifically.
This paper acquaints with a created application for generating Rainbow Tables and the results of testing Rainbow Tables.
This paper addresses a joint downlink and uplink aware cell association problem in a multi-tier heterogeneous network in which base stations (BSs) have finite number of resource blocks (RBs) to distribute among the users.
 An optimization problem is defined to maximize the sum of weighted utility of long term data rate in downlink and uplink through cell association and RB distribution while maintaining quality of service (QoS).
 Separate outage requirements are considered as QoS constraints for downlink and uplink of a user.
 Using outage QoS constraints renders the problem suitable for fast fading environments.
 We propose a distributed scheme for the cell association problem.
 As users cannot measure the uplink attributes by listening to the reference signals of BSs.
This paper addresses one of the major web end-user software engineering (WEUSE) challenges.
This paper addresses the continuing problem in the United States of a lack of female professionals in Computer Science.
 The research team conducted surveys of middle school students and working adults to examine their attitudes.
This paper addresses the makespan minimization problem in scheduling flexible job shops whenever there exist separable sequence-dependent setup times.
 An extension to the neighborhood search functions of Mastrolilli and Gambradella.
This paper addresses the Parallel Machine Scheduling Problem with Step Deteriorating Jobs.
 This problem arises from real environments in which processing a job later than at a specific time may require an extra processing time.
 This time-dependent variation is known in the literature as step deterioration and has several practical applications (production planning.
This paper addresses the potential Korteweg-de Vries equation.
 The singular 1-soliton solution is obtained by the aid of ansatz method.
 Subsequently.
This paper addresses the problem of augmented reality on images acquired from non-central catadioptric systems.
 We propose a solution which allows the projection of textured objects to images of these type of systems and.
This paper addresses the problem of developing user interfaces for Ubiquitous Computing (UC) and Ambient Intelligence (AmI) systems.
 These kind of systems are expected to provide a natural user experience.
This paper addresses the problem of finding credible sources among Twitter social network users to detect and prevent various malicious activities.
This paper addresses the problem of identifying different flow environments from sparse data collected by wing strain sensors.
 Insects regularly perform this feat using a sparse ensemble of noisy strain sensors on their wing.
This paper addresses the problem of integrating ontological knowledge bases into complex software applications by proposing a library for exposing ontology access and manipulation as web services.
 The proposed framework is an extension of our previous work.
This paper addresses the problem of joint inference and optimization in wireless networks.
 An optimization framework based on information-geometric network inference is developed and implemented (using a real radio emulation testbed) with scalable solutions to infer the end-to-end rate distributions of stochastic network flows from link rate measurements.
 The proposed low-complexity solutions apply when the underlying network inference (network tomography) problem can be decomposed to smaller-size subproblems that are solved independently by partially inferring only the flow rates of interest.
 The solutions are extended to infer flow rates jointly with link loss rates when retransmissions are considered over unreliable wireless links.
 By using the inferred distributions of flow rates.
This paper addresses the road toll pricing and capacity investment problem in a congested road network in a multicriteria decision-making framework.
 A goal programming approach is used in which the following four major goals are considered: (1)cost recovery.
This paper addresses the stability and tracking control problem of an underactuated four rotor unmanned flying robot vehicle.
 Algorithm design combines adaptive law with the sliding mode control term to deal with uncertainties associated with flying environment.
This paper aims to develop and evaluate information assurance and security learning materials on hardware/firmware attacks for undergraduate education.
 Topics include firmware worms.
This paper aims to provide a road map for future works related to reverse engineering field of expertise.
 Reverse Engineering.
This paper analyses and evaluates parallel implementations of an optimization algorithm for perishable inventory control problems.
 This iterative algorithm has high computational requirements when solving large problems.
This paper analyzes the advantages and trends of holographic projection technology and the characteristics of large sports performance arrangement using the method of literature.
This paper analyzes the basic linear generation algorithm.
This paper analyzes the perceived dissimilarity between postures of humanoid robots.
This paper briefly describes a visualization method of the Otrokovice Municipality in the Nineteen-thirties.
 The rapid growth of this town began at the beginning of the 20th century.
This paper brings explicit considerations of distributed computing architectures and data structures into the rigorous design of Sequential Monte Carlo (SMC) methods.
 A theoretical result established recently by the authors shows that adapting interaction between particles to suitably control the effective sample size (ESS) is sufficient to guarantee stability of SMC algorithms.
 Our objective is to leverage this result and devise algorithms which are thus guaranteed to work well in a distributed setting.
 We make three main contributions to achieve this.
This paper compares smart control models for heating supply air among five different climate conditions to discuss the effectiveness of machine learning tools in terms of control and energy efficiency.
 A thermostat on/off control is typically used to maintain room temperature at a desired level.
 Advanced computing technologies have recently been introduced to complement the conventional on/off controls to improve control efficiency in heating systems.
This paper compares the performance of the popular adaptive h-refinement (hR) technique for the finite-element method (FEM) with the adaptive version of the recently presented operator-customized wavelet basis (OCWB).
 This new method is a combination of the second-generation wavelet theory with hierarchical basis.
This paper concentrates on modeling probabilistic events with fuzzy probability measures in the object-oriented databases.
 Instead of crisp probability measures or interval probability measures of objects and classes.
This paper considers relational databases containing uncertain attribute values when some knowledge is available about themore or less certain value (or disjunction of values) that a given attribute in a tuplemaytake.
 We propose a possibility-theory-based model suited to this context and extend the operators of relational algebra to handle such relations in a compact.
This paper considers robust low-rank matrix completion in the presence of outliers.
 The objective is to recover a low-rank data matrix from a small number of noisy observations.
 We exploit the bilinear factorization formulation and develop a novel algorithm fully utilizing parallel computing resources.
 Our main contributions are i) providing two smooth loss functions that promote robustness against two types of outliers.
This paper considers the problem of many-to-many disjoint paths in the hypercube Q(n) with f faulty vertices and obtains the following result.
 For any integer k with 1 <= k= 2).
This paper considers the two-stage capacitated facility location problem (TSCFLP) in which products manufactured in plants are delivered to customers via storage depots.
 Customer demands are satisfied subject to limited plant production and limited depot storage capacity.
 The objective is to determine the locations of plants and depots in order to minimize the total cost including the fixed cost and transportation cost.
This paper deals with a (2+1)-dimensional nonlinear evolution equation (NLEE) generated by the Jaulent-Miodek hierarchy for nonlinear water waves via the Hirota's bilinear method and Pfaffian.
This paper deals with constructing more general exact solutions of the coupled Higgs equation by using the (G'/G.
This paper deals with how to determine which features should be included in the software to be developed.
 Metaheuristic techniques have been applied to this problem and can help software developers when they face contradictory goals.
 We show how the knowledge and experience of human experts can be enriched by these techniques.
This paper deals with iterative formation control problems for multi-agent systems with switching topologies.
 Our approach combines consensus protocol algorithms conducted with an iterative learning control update.
 This leads to networks dynamically changing with evolution along two directions: a finite time axis and an infinite iteration axis.
 A distributed algorithm is constructed based on nearest neighbor information.
This paper deals with the problem of securing the configuration phase of an Internet of Things ( IoT) system.
 The main drawbacks of current approaches are the focus on specific techniques and methods.
This paper demonstrates how procedural modeling and computer graphics techniques can be combined for creating fast.
This paper describes a collocated numerical scheme for multi-material compressible Euler equations.
This paper describes a computational approach for analyzing and visualizing the aesthetics of color from the perspective of color theory.
 Our study is grounded in the works of Johannes Itten.
This paper describes a computationally efficient parallel-computing framework for mesoscopic transportation simulation on large-scale networks.
 By introducing an overall data structure for mesoscopic dynamic transportation simulation.
This paper describes a framework for software-based networking in smart factories (SF) that enables them to easily adapt the communication network to changing requirements.
 Similar to cloud-based systems.
This paper describes a fully coupled finite element/finite volume approach for simulating field-scale hydraulically driven fractures in three dimensions.
This paper describes a general-purpose parallel scheme for efficiently focusing synthetic aperture radar (SAR) data on multicore-based shared-memory architectures.
 The rationale of the proposed tiling-based parallel focusing model is first discussed.
This paper describes a high performing.
This paper describes a learning parallel constraint programming (CP) solver designed for solving CP problems with several instances on massively parallel computing platforms comprising multi-core parallel machines or Many Integrated Cores.
 The CP solver proposed in this work is based on a Portfolio parallelization that employs a linear reward inaction learning algorithm in order to obtain the best possible performance for a large set of instances of the same problem.
 The linear reward inaction algorithm enables the prediction of the number of cores to be assigned to each search strategy based on previous experiments.
This paper describes a multi-institution effort to develop a data science as a service platform.
 This platform integrates advanced federated data management for small to large datasets.
This paper describes a new method to automatically generate digital bas-reliefs with depth-of-field effects from general scenes.
 Most previous methods for bas-relief generation take input in the form of 3D models.
This paper describes a novel cyber attack-resilient server inspired by the concept of biological diversity.
 The server consists of several virtual machines running different operating systems and different implementations of the same server protocol specification.
 This approach is based on the observation that not all implementations are affected by the same vulnerability.
This paper describes a novel method to design a simple real time GPS (Global Positioning System) receiver system for navigation.
This paper describes a rapid algorithm deployment platform for Smart Grid research.
 Accounting for the complex interplay of power system dynamics and communication delays in the network by means of rapid code deployment during algorithm design can improve the evaluation of Smart Grid control schemes and their impact on grid power quality.
 Our novel approach bridges the gap between the implementation of highly realistic multi-timeframe simulations.
This paper describes a strategy to identify the authorship of online handwritten documents.
 We regard our research framework to that of a retrieval problem and adapt the so called codebook based Vector of Local Aggregate descriptor (VLAD) that has been promising for the object retrieval application in image processing.
 The codebook comprises a set of code vectors with associated Voronoi cells computed from a clustering algorithm on a set of feature vectors along the online trace.
This paper describes a wearable encounter-type haptic device suitable for combined usage with a visual display.
 The features of the device lie in a driving mechanism that enables an encounter-type haptic display and the compact implementation of the entire device including the driving mechanism.
 The driving mechanism displays a natural haptic sense based on a smooth transition between follow-up and constraint of finger movements.
 The compactness is important because it contributes to preserving the quality of visual information when used together with a visual display.
 To test the basic performance of the device.
This paper describes an efficient parallel algorithm that uses many-core GPUs for automatically deriving Unique Input Output sequences (UIOs) from Finite State Machines.
 The proposed algorithm uses the global scope of the GPU's global memory through coalesced memory access and minimises the transfer between CPU and GPU memory.
 The results of experiments indicate that the proposed method yields considerably better results compared to a single core UIO construction algorithm.
 Our algorithm is scalable and when multiple GPUs are added into the system the approach can handle FSMs whose size is larger than the memory available on a single GPU.
This paper describes an implementation of a highly scalable parallel computational facility with high speedup efficiency using relatively low-cost hardware.
This paper describes an implementation process for a domain-specific computer programming language: the Building Environment Rule and Analysis (BERA) Language.
 As the growing area of Building Information Modeling (BIM).
This paper describes and analyses a secure and efficient authentication protocol (SEAP) designed for mobile ad-hoc networks (MANETs).
 The SEAP protocol is a server coordination-based pairwise symmetric key management protocol which works on a hierarchical network architecture and supports dynamic membership.
This paper describes the control of computations in a distributed computing environment (DCE) on the basis of its meta-monitoring and simulation modeling.
 Computations are controlled by a multiagent system with a given organizational structure.
 Resource allocation is carried out by agents with the use of economic mechanisms for controlling their supply and demand.
 Controlling actions for agents are formed on the basis of the simulation modeling of functional processes of the DCE.
 Data about the DCE resources and processes are collected and emergency situations in the DCE nodes are detected and prevented by the meta-monitoring system of this environment.
 The research results are the techniques for selecting control actions and the methods for intellectual processing and effective storage of data.
This paper describes the course that was developed at the authors' University to introduce all first-year engineering students to the fundamentals of computer programming within the context of solving engineering problems.
 This two credit-hour.
This paper describes the design and implementation of a tool to extract the IMDb dataset files and import them into a database.
 This approach differs from other published tools or research in that the previous work used relational databases.
 This tool uses document oriented data structures.
This paper describes the design of a traffic assignment model that predicts flows for each segment of an urban network with a higher resolution than a traditional four stage model.
This paper describes the development of a pedestrian microsimulation model that was developed based on the agent based modeling approach.
This paper describes the development.
This paper describes the main features of a state-of-the-art Monte Carlo solver for radiation transport which has been implemented within COOLFluiD.
This paper describes the memory architecture to improve the data transfer and storage in a small satellite.
 The main objective during the design stage of the architecture is to find a good balance between power consumption.
This paper describes the performance of the Brain Project.
This paper describes the use of cyberinfrastructure to create interactive applications as part of the Useful to Usable (U2U) project.
 These applications transform historical climate data.
This paper describes the use of domain decomposition methods for accelerating wave physics simulation.
 Numerical wave-based methods provide more accurate simulation than geometrical methods.
This paper describes the use of parallelization techniques to reduce dynamic power consumption in hardware implementations of the Trivium stream cipher.
 Trivium is a synchronous stream cipher based on a combination of three non-linear feedback shift registers.
This paper describes UML-based foundations for model driven architecture and forward engineering of UML static models.
 In this paper.
This paper designs and implements an embedded security gateway based on double-homed structure which composed of software and hardware parts.
 The core of hardware platform is based on two S3C6410 processors and one EP1C18F4620 FPGA.
 The software is based on reduced Linux kernel 3.
 The gateway uses Net-filter/IP-tables firewall.
This paper develops a method based on genetic algorithm (GA) for reliability-based optimization (RBO) of structures through a software application.
 This method employs GA as an optimization technique in which reliability constraints of RBO problem are evaluated using finite element reliability analysis modules offered by OpenSees.
 Since Tcl is a programmable and interpreted language.
This paper develops a model for cancer screening and cancer incidence data.
This paper develops a new framework to analyze and design iterative optimization algorithms built on the notion of integral quadratic constraints (IQCs) from robust control theory.
 IQCs provide sufficient conditions for the stability of complicated interconnected systems.
This paper develops an Internet-of-Things data highway embracing end sensors.
This paper develops and compares several optimization approaches for the version planning and release problem.
 This problem is new.
This paper discusses a possible approach to distributed visualization and rendering system infrastructure organization.
This paper discusses how to apply the advantage of the computer to complete the management work of the track and field athletics.
This paper discusses some of the main issues that are generally found in protection systems of low-voltage electrical installations and presents a methodology that can be used to analyze and upgrade the existing power system protective devices in these low-voltage distribution systems.
 The upgrades of existing operating systems.
This paper discusses the automated visual identification of individual great white sharks from dorsal fin imagery.
 We propose a computer vision photo ID system and report recognition results over a database of thousands of unconstrained fin images.
 To the best of our knowledge this line of work establishes the first fully automated contour-based visual ID system in the field of animal biometrics.
 The approach put forward appreciates shark fins as textureless.
This paper discusses the design and interface of NUClear.
This paper discusses the evolution of longitudinal train dynamics (LTD) simulations.
This paper discusses the positivity preserving by using rational cubic Ball interpolant of the form cubic/quadratic with two parameters.
 The sufficient condition for the positivity is derived on one parameter meanwhile the other one is a free parameter to control the final shape of the interpolating curves.
 The degree smoothness achieved is C-1.
 From numerical results.
This paper elaborate the application of MAPLE in the calculation of the influence line of the plane statically determinate truss.
This paper elaborates on the issue of the representation of the administrative information.
This paper establishes the capacity region for a class of source coding function computation setups.
This paper examines the central place of the list and the associated concept of an identifier within the scaffolding of contemporary institutional order.
 These terms are deliberately chosen to make strange and help unpack the constitutive capacity of information systems and information technology within and between contemporary organizations.
 We draw upon the substantial body of work by John Searle to help understand the place of lists and identifiers in the constitution of institutional order.
 To enable us to ground our discussion of the potentiality and problematic associated with lists we describe a number of significant instances of list-making.
This paper explains how a software architecture was planned.
This paper explores advantages that a publish/subscribe middleware can offer for the implementation of an efficient and reliable electric vehicle (EV) charging control infrastructure.
 We propose distributed optimization publish and subscribe (DOPS).
This paper explores the issue of group integrity of tuple subsets regarding corporate integrity constraints in relational databases.
 A solution may be found by applying the finite-state machine theory to guarantee group integrity of data.
 We present a practical guide to coding such an automaton.
 After creating SQL queries to manipulate data and control its integrity for real data domains.
This paper explores the use of a constructivist 21st-century learning model to implement a week-long workshop.
This paper extends a recently proposed robust computational framework for constructing the boundary representation (brep) of the volume swept by a given smooth solid moving along a one parameter family h of rigid motions.
 Our extension allows the input solid to have sharp features.
This paper extends the hierarchical multiscale approach developed earlier by the authors to model the coupled hydro-mechanical behaviour for saturated granular soils.
 Based on a hierarchical coupling of the finite element method (FEM) and the discrete element method (DEM).
This paper focuses on learning a smooth skeleton structure from noisy data-an emerging topic in the fields of computer vision and computational biology.
 Many dimensionality reduction methods have been proposed.
This paper focuses on manners of filling the gaps in existing standards that are used in tridimensional Web technologies.
 We proposed the way of encoding huge 3D data sets in lossless PNG format and use of programmable rendering pipeline to decoding PNG file.
 It allows to reduce significantly the file with 3D data.
This paper focuses on the problem of distributed composite hypothesis testing in a network of sparsely interconnected agents.
This paper focuses on the security assessment of electricity distribution networks (DNs) with vulnerable distributed energy resource (DER) nodes.
 The adversary model is a simultaneous compromise of DER nodes by strategic manipulation of generation setpoints.
 The loss to the defender (DN operator) includes loss of voltage regulation and cost of induced load control under supply-demand mismatch caused by the attack.
 A three-stage defender-attacker-defender (DAD) game is formulated: in Stage 1.
This paper generalizes the well-known Diffusion Curves Images (DCI).
This paper gives a comprehensive discussion on applying the cloud computing technology as the new information infrastructure for the next-generation power system.
This paper highlights the various solutions of the Travelling Salesman Problem.
This paper includes the results of an online survey that was conducted by the American Society of Civil Engineers (ASCE) task committee on computing education to assess the evolution of computing in architecture.
This paper introduces a CMOS vision sensor chip in a standard 0.
18 mu m CMOS technology for Gaussian pyramid extraction.
 The Gaussian pyramid provides computer vision algorithms with scale invariance.
This paper introduces a design and fabrication pipeline for creating floating forms.
 Our method optimizes for buoyant equilibrium and stability of complex 3D shapes.
This paper introduces a hand gesture recognition sensor using ultra-wideband impulse signals.
This paper introduces a new method for clustering of documents.
This paper introduces a new method to solve the cross-domain recognition problem.
 Different from the traditional domain adaption methods which rely on a global domain shift for all classes between the source and target domains.
This paper introduces a nonlinear certainty-equivalent approximation method for dynamic stochastic problems.
 We first introduce a novel.
This paper introduces a novel approach to providing high school students with access to computer science experiences as part of an Algebra unit on linear functions.
 The approach is being developed and tested as part of a funded National Science Foundation study.
 The unit piloted in the study integrates computational thinking and computer modeling into a project-based Algebra unit on linear functions.
 Literature on computational thinking.
This paper introduces a novel domain-specific compiler.
This paper introduces a parallel and distributed algorithm for solving the following minimization problem with linear constraints: minimize f(1)(x(1))+ .
 + f(N) (x(N)) subject to A(1)x(1) + .
 + A(N)x(N) = c.
This paper introduces a system for automatic evaluation of correctness and originality of source codes submitted by students enrolled in courses dealing with computer programming.
 Automatic correctness checking consists of searching for plagiarisms in assignments submitted earlier and checking the correct implementation of algorithms.
 User interface is implemented as a Moodle module using its Plagiarism API.
 The complete system is published with GPLv3 license; therefore other learning institutions can use it as well.
This paper introduces a technique to derive a wordlevel abstraction of the function implemented by a combinational logic circuit.
 The abstraction provides a canonical representation of the function as a polynomial Z = F(A) over the finite field F-2k.
This paper introduces PSCAN; a port scanning-based network covert channel that violates non-discretionary system security policy that does not allow data transfer from a given process (the sender) to another given process (the receiver).
 Using PSCAN.
This paper introduces the potential for reusing UI elements in the context of Model-Based UI Development (MBUID) and provides guidance for future MBUID systems with enhanced reutilization capabilities.
 Our study is based upon the development of six inter-related projects with a specific MBUID environment which supports standard techniques for reuse such as parametrization and sub-specification.
This paper investigates a self-organized critical approach for dynamically load-balancing computational workloads.
 The proposed model is based on the Bak-Tang-Wiesenfeld sandpile: a cellular automaton that works in a critical regime at the edge of chaos.
 In analogy to grains of sand.
This paper investigates and studies the acceleration of irregular/regular algorithms via Integrate Graphic Processing Unit (Integrated GPU) known as Accelerated Processing Unit (APU) that is fused on the same die with the CPU.
This paper investigates the current automatics methods used to generate efficient and accurate signatures to create countermeasures against attacks by polymorphic worms.
 These strategies include autograph.
This paper investigates the development of a mild hybrid powertrain system through the integration of a conventional manual transmission equipped powertrain and a secondary power source in the form of an electric motor driving the transmission output shaft.
 The primary goal of this paper is to study the performance of partial power-on gear shifts through the implementation of torque hole filling by the electric motor during gear changes.
 To achieve this goal.
This paper investigates the effect of tracking errors in heliostats used in solar tower power plants and proposes an approach based on low-cost distributed electronics capable of limiting their impact.
 An analysis carried out through a parallel model sets the specifications for design of a closed-loop solar tracker based on a low-cost six-axis digital e-compass.
 A proof of concept system is devised to test the accuracy of the proposed strategy.
 This approach allows the solar tracker to perform a run-time detection and correction of heliostat tracking errors.
This paper investigates the integrability of a generalized seventh-order Korteweg-de Vries equation arising in fluids and plasmas.
 By means of singularity structure analysis.
This paper investigates the physical-layer security of a multiuser peer-to-peer (MUP2P) relay network for amplify-and-forward (AF) protocol.
This paper investigates the reduction of dynamic power for streaming applications yielded by asynchronous dataflow designs by using clock gating techniques.
 Streaming applications constitute a very broad class of computing algorithms in areas such as signal processing.
This paper investigates the robust simultaneous stabilization (RSS) and robust adaptive simultaneous stabilization (RASS) problem for a set of port-controlled Hamiltonian (PCH) systems.
 Some results for designing a family of robust simultaneous controllers with tuning parameters for such systems are proposed.
This paper is about variable selection with the random forests algorithm in presence of correlated predictors.
 In high-dimensional regression or classification frameworks.
This paper is aimed at employing analytical target cascading (ATC) to solve the distributed production planning problem.
 In the ATC hierarchy.
This paper is an extension of a previous one presented at the conference Cyberworlds 2015.
 In that work we addressed the problem to fit a given set of data points in the least-square sense by using a polynomial Bezier curve.
 This problem arises in many scientific and industrial domains.
This paper is concerned with the bi-isochronous centers problem for a cubic systems in -equivariant vector field.
 Being based on bi-centers condition.
This paper is concerned with the traveling wave solutions and analytical treatment of the infiltration equation.
 Based on the new technique.
This paper is focused on analyzing the details of one of the most widely used Q&A technical discussion forums on the Internet: StackOverflow.
 StackOverflow is a huge source of information for both academics and industry practitioners and its analysis can provide useful insights.
 This is evident by a growing body of work dedicated to the study of this platform.
 There are several papers that have taken the 'platform' as an analyzing point to gain a few useful insights into the practices adopted by software developers.
This paper is focused on performance optimization of applications based on non-relational databases.
 Firstly there will be explained differences between relational and non-relational databases and also compared the types of non-relational databases.
 Then the advantages and disadvantages between MongoDB and ElasticSearch (SQL) database systems will be described and compared.
 After both database systems were implemented on the web server.
This paper is mainly discuss about software component testing as an interesting challenge in Component Based Software Engineering (CBSE) area.
This paper is on the construction and use of a shared memory abstraction on top of an asynchronous message-passing system in which up to t processes may commit Byzantine failures.
 This abstraction consists of arrays of n single-writer/multi-reader atomic registers.
This paper is the result of the development of the Web application to register the Annual Work Program.
This paper mainly studies the image contour detection algorithm which can distinguish edges of different strengths.
 Based on the study of Probability-of-Boundary operator.
This paper outlines a low-cost.
This paper poses the problem of fabricating physical construction sets from example geometry: A construction set provides a small number of different types of building blocks from which the example model as well as many similar variants can be reassembled.
 This process is formalized by tiling grammars.
 Our core contribution is an approach for simplifying tiling grammars such that we obtain physically manufacturable building blocks of controllable granularity while retaining variability.
This paper presents a case study to describe the features and the phases of the two agent methodologies.
 The Gaia methodology for agent oriented analysis and design.
This paper presents a cloud-based system framework based on Bigtable and MapReduce as the data storage and processing paradigms for providing a web-based service for viewing.
This paper presents a computational framework developed to improve both the serial and parallel performance of two dimensional.
This paper presents a computer vision-based algorithm that automatically detects the components of an interior partition and infers its current state using 2D digital images.
 The algorithm relies on four integrated shape and color-based modules.
This paper presents a concurrent garbage collection method for functional programs running on a multicore processor.
 It is a concurrent extension of our bitmap-marking non-moving collector with Yuasa's snapshot-at-the-beginning strategy.
 Our collector is unobtrusive in the sense of the Doligez-Leroy-Gonthier collector; the collector does not stop any mutator thread nor does it force them to synchronize globally.
 The only critical sections between a mutator and the collector are the code to enqueue/dequeue a 32 kB allocation segment to/from a global segment list and the write barrier code to push an object pointer onto the collector's stack.
 Most of these data structures can be implemented in standard lock-free data structures.
 This achieves both efficient allocation and unobtrusive collection in a multicore system.
 The proposed method has been implemented in SML#.
This paper presents a controlled experiment in which the goodness of using Theory Belbin roles for the integration of software development teams is explored.
 The study takes place in an academic environment with students from the Engineering of Software and compares the quality of the readability of the code generated by integrated teams with roles Compatible - according to the Theory of Belbin- and traditional teams.
This paper presents a decision support system (DSS) called DSScreening to rapidly detect inborn errors of metabolism (IEMs) in newborn screening (NS).
 The system has been created using the Aide-DS framework.
This paper presents a distributed approach that scales up to segment tree crowns within a LiDAR point cloud representing an arbitrarily large forested area.
 The approach uses a single-processor tree segmentation algorithm as a building block in order to process the data delivered in the shape of tiles in parallel.
 The distributed processing is performed in a master-slave manner.
This paper presents a distributed computing architecture for solving a distribution optimal power flow (DOPF) model based on a smart grid communication middleware (SGCM) system.
 The system is modeled as an unbalanced three-phase distribution system.
This paper presents a distributed Prony analysis algorithm using data fusion approach.
 This classic approach can be found in Kalman filter's measurement update.
 Distributed optimization algorithms.
This paper presents a documentation and development method to facilitate the certification of scientific computing software used in the safety analysis of nuclear facilities.
 To study the problems faced during quality assurance and certification activities.
This paper presents a domain decomposition-type method for solving real symmetric (Hermitian) eigenvalue problems in which we seek all eigenpairs in an interval [alpha.
This paper presents a fast concurrent binary search tree algorithm.
 To achieve high performance under contention.
This paper presents a fine grain parallel version of the 3D Delaunay Kernel procedure using the OpenMP (Open Multi-Processing) API.
 A set S = {p(1).
This paper presents a focus on the error propagation aspect which exists in interconnected networks.
 For handling the issues and considerations implied by this negative property.
This paper presents a formal method for designing cryptographic processor datapaths on the basis of arithmetic circuits over Galois fields (GFs).
 The proposed method describes GF arithmetic circuits in the form of hierarchical graph structures.
This paper presents a formal model of a decision making system for public transport routes.
 The approach focuses on (1) environmental and societal sustainability aspects of green software engineering.
This paper presents a framework for a strapdown Inertial Navigation System (INS) algorithm design by using Lie group and Lie algebra.
 The general way to solve Lie group differential equations is introduced.
 Investigations reveal that this general Lie group method provides a simpler unified way to solve differential equations involving direction cosine matrix.
This paper presents a framework for human detection based on the joint component model using extended feature descriptors.
 This framework provides two contributions for handling the partially occluded problem of pedestrian detection in crowded environments.
This paper presents a halftoning-based watermarking method that enables the embedding of a color image into binary black-and-white images.
 To maintain the quality of halftone images.
This paper presents a Knowledge-Based Intelligent Decision system (KIDs) that takes information from a vision sensor within the manufacturing process and generates automatic planning/path-planning decisions in for collision avoidance in virtual CAM production.
 In this paper.
This paper presents a linear programming model for solution of the time-cost tradeoff problem.
 Although several analytical models have been developed for time-cost optimization (TCO).
This paper presents a literature review in the field of summarizing software artifacts.
This paper presents a machine-learning classifier where computations are performed in a standard 6T SRAM array.
This paper presents a method for detecting anomalous power consumption patterns attacks.
This paper presents a method for modification of single flash cycle power output.
 The thermodynamic process of the new method consists of extracting a fraction of hot wellhead geothermal fluid for the purpose of superheating saturated steam entering the turbine.
 Computer programming scripts were developed and optimized based on mathematical proposed models for the different components of the systems.
 The operating parameters such as separator temperature.
This paper presents a method for solving cryptarithms (variously known as alphametics.
This paper presents a method for the reconnection of contour lines from scanned color images of topographical maps based on graphics processing unit (GPU) implementation.
 The extraction of contour lines.
This paper presents a method for tracking a 3D textureless object which undergoes elastic deformations.
This paper presents a method that permits the calculus of the dimensions of the component links of a quadrilateral mechanism that ensures the optimisation of the variation on a cinematic cycle of the connection forces in the joints of the mechanism when some functional constraints are imposed.
 The connection forces are obtained in an analytical form.
This paper presents a method to accelerate target recognition processing in advanced driver assistance systems (ADAS).
 A histogram of oriented gradients (HOG) is an effective descriptor for object recognition in computer vision and image processing.
 The HOG is expected to replace conventional descriptors.
This paper presents a method to compute the quasi-conformal parameterization (QCMC) for a multiply-connected 2D domain or surface.
 QCMC computes a quasi-conformal map from a multiply-connected domain S onto a punctured disk D (S) associated with a given Beltrami differential.
 The Beltrami differential.
This paper presents a method to solve - in real time the three dimensional workspace generation problem for arbitrary serial manipulators.
 Our approach is based on Monte Carlo simulation.
This paper presents a methodological solution to The Battle of Background Leakage Assessment for Water Networks (BBLAWN) competition.
 The methodology employs two constrained multiple-objective optimization problems and is implemented in the context of a software application for the generic hydraulic optimization and benchmarking of water distribution system (WDS) problems.
 The objectives are the combined infrastructure and operational costs and system-wide leakage.
This paper presents a microkernel architecture for constraint programming organized around a small number of core functionalities and minimal interfaces.
 The architecture contrasts with the monolithic nature of many implementations.
 With this design.
This paper presents a Monte Carlo approach for reliability assessment of distribution systems with distributed generation using parallel computing.
 The calculations are carried out with a royalty-free power flow simulator.
This paper presents a network security laboratory project for teaching network traffic anomaly detection methods to electrical engineering students.
 The project design follows a research-oriented teaching principle.
This paper presents a network security laboratory to teach data analysis for detecting TCP/IP covert channels.
 The laboratory is mainly designed for students of electrical engineering.
This paper presents a new algorithm for volume-constrained expected compliance minimization of continuum structures with probabilistic loading directions using analytically determined exact objective and gradient functions.
 The algorithm is based upon the finding that for a particular set of statistical parameters the integration in the expected compliance function can be done symbolically and automatically using symbolic manipulation software.
 In this study.
This paper presents a new approach to infer worldwide malware-infected machines by solely analyzing their generated probing activities.
 In contrary to other adopted methods.
This paper presents a new approach to network traffic control based on the pattern theorem.
 In order to generate unique detection patterns for the process of traffic analysis.
This paper presents a new controlled quantum dialogue (CQD) protocol based on the cluster entangled states.
 The security analyses indicate that the proposed scheme is secure under not only various well-known attacks but also the collusive attack.
This paper presents a new encryption scheme implemented at the physical layer of wireless networks employing orthogonal frequency-division multiplexing (OFDM).
 The new scheme obfuscates the subcarriers by randomly reserving several subcarriers for dummy data and resequences the training symbol by a new secure sequence.
 Subcarrier obfuscation renders the OFDM transmission more secure and random.
This paper presents a new enhanced stego block chaining (ESBC) technique for data hiding to address the bandwidth and transmission time issues of simple stego block chaining (SBC).
 In simple SBC data hiding method.
This paper presents a new hybrid modeling technique for the efficient simulation of guided wave generation.
This paper presents a new infrastructure for creating specialized databases and database management systems (DBMSs).
 Some principles of this infrastructure are formulated based on an analysis of various NoSQL solutions.
 The main features of the proposed approach are: (a) a flexible system of type definitions that allows one to create data structures based on different paradigms and (b) different forms of supporting structured data and mapping them onto the file system.
 Experiments are carried out to implement RDF graphs.
This paper presents a new linear velocity estimator based on the unscented Kalman filter and making use of image information aided with inertial measurements.
 The proposed technique is independent of the scale factor in case of planar observed scene and does not require a priori knowledge of the scene.
 Image moments of virtual objects.
This paper presents a new non-invasive technique of granulometric analysis based on the fusion of two imaging techniques.
This paper presents a new parallel domain decomposition algorithm based on integer linear programming (ILP).
This paper presents a novel algorithm and architecture design for 18-band quasi-class-2 ANSI S1.
11 1/3 octave filterbank.
 The proposed design has several advantages such as lower group delay.
This paper presents a novel and effective edge-preserving image smoothing method for edge-aware image manipulation.
 The method formulates the smoothing as a problem of minimizing a convex object function with a constraint and an efficient solution to the optimization problem is presented.
 Specifically.
This paper presents a novel approach for estimating the ego-motion of a vehicle in dynamic and unknown environments using tightly-coupled inertial and visual sensors.
 To improve the accuracy and robustness.
This paper presents a novel automated procedure for discovering expressive shape specifications for sophisticated functional data structures.
 Our approach extracts potential shape predicates based on the definition of constructors of arbitrary user-defined inductive data types.
This paper presents a novel frequency-domain approach for distributed harmonic analysis (DHA) of a multi-area interconnected electric power system within restructured environment.
 The proposed approach is based on a decentralized structure in which harmonic analysis of an area is independently conducted.
This paper presents a novel implementation of GPU computing for boundary element method (BEM) formulation of plates.
 The new GPU code written in CUDA Fortran alters three kernels: the calculation of influence matrices.
This paper presents a novel inexact full adder based on carbon nanotube field-effect transistors (CNTFET) for approximate computations.
This paper presents a novel list-based scheduling algorithm called Improved Predict Earliest Finish Time for static task scheduling in a heterogeneous computing environment.
 The algorithm calculates the task priority with a pessimistic cost table.
This paper presents a novel method for capturing the 3D profile of the inside of a rolling off-road vehicle tyre at the tyre-road contact region.
 This method captures the contact region at all times as the vehicle negotiates obstacles.
 The system uses a pair of inexpensive digital cameras (capable of capturing up to 300 frames per second) and features a purely mechanical stabilisation system to ensure that the cameras capture the contact region at any wheel speed or vehicle acceleration.
 The captured images are processed using 3D computer vision techniques using an open source computer vision library called OpenCV.
 Stereo image pairs are used to create clouds of 3D points showing the profile of the inside surface with good accuracy.
 Various obstacles were traversed with the deformed tyre profile being compared to the undeformed profile.
 The system improves on current measurement techniques used to measure the contact patch by capturing a large region of the contact patch.
This paper presents a novel method to detect free-surfaces on particle-based volume representation.
 In contrast to most particle-based free-surface detection methods.
This paper presents a novel relational database architecture aimed for visual objects classification and retrieval based on the content of images.
 Most methods search and classify images based on their content use algorithms for generating and comparing keypoint descriptors.
 We present a new method to quickly compare these visual features.
This paper presents a novel robust color image zero-watermarking scheme based on SVD and visual cryptography.
 We firstly generate the image feature from the SVD of the image blocks.
This paper presents a novel skeleton design for avatar animation.
 This design includes new bones.
This paper presents a novel two-step method for automated design of self-stabilization.
 The first step enables the specification of legitimate states and an intuitive (but imprecise) specification of the desired functional behaviors in the set of legitimate states (hence the term shadow).
 After creating the shadow specifications.
This paper presents a real-time image acquisition system with an improved image quality assessment module to acquire high-quality near infrared (NIR) images.
 Thermal imaging plays a vital role in a wide range of medical and military applications.
 The demand for high-throughput image acquisition and image processing has continuously increased especially for critical medical and military purposes where executions under real-time constraints are required.
 This work implements an NIR image quality assessment module.
This paper presents a Real-Time Operating System ( RTOS) implementation.
 This RTOS is based on the OSEK-OS 2.
This paper presents a reliable machine vision system to automatically detect inserts and determine if they are broken.
 Unlike the machining operations studied in the literature.
This paper presents a review of current denial of service (DoS) attack and defence concepts.
This paper presents a robust color image watermarking algorithm based on fuzzy least squares support vector machine (FLS-SVM) and Bessel K form (BKF) distribution.
This paper presents a robust.
This paper presents a schematic eye model designed for use by virtual environments researchers and practitioners.
 This model.
This paper presents a set of experiments in formal modelling and verification of a deadlock avoidance algorithm of an Automatic Train Supervision System (ATS).
 The algorithm is modelled and verified using four formal environment.
This paper presents a short overview of today's unstable environment of communication that implies serious actions regarding the development of new solutions that ensures confidential communications for mobile devices.
 The paper analyzes cryptography and steganography performances on mobile platforms.
 We used SmartSteg project to test the performances on different types of operating systems and devices.
 We take in consideration e the functionality.
This paper presents a simple and efficient reliable broadcast algorithm for asynchronous message-passing systems made up of n processes.
This paper presents a strategy based on the approach of designing and inserting into helicopter vibration isolation systems mountable mechanisms with springs of adjustable sign-changing stiffness for system stiffness control.
 A procedure to extend the effective area of stiffness control is presented; a set of parameters for sensitivity analysis and practical mechanism design is formulated.
 The validity and flexibility of the approach are illustrated by application to crewmen seat suspensions and vibration isolators for equipment protection containers.
 The strategy provides minimization of vibrations.
This paper presents a study about how efficient implementation of software changes helps students to better understanding of programming techniques and algorithms.
 The study is performed during last three years on first year students.
This paper presents a study of the collective knowledge in information technology (IT) and the comparative analyses of innovative trends in the standardisation of the roads of knowledge in the subfields of software engineering (SE).
 The focus is on the amount of required innovation that will be necessary in the examples database of standardised units in IT and SE for the improvement of the information systems (IS).
 The goal is to determine how to obtain appropriate knowledge in IT and SE to model the excellence of IS.
 The contribution to the modelling of IS excellence in PDCA (Plan-Do-Check-Act) is presented.
 (C) 2015 Elsevier B.
 All rights reserved.
This paper presents a study of the multi-objective optimal design of a sliding mode control for an under-actuated nonlinear system with the parallel simple cell mapping method.
 The multi-objective optimal design of the sliding mode control involves six design parameters and five objective functions.
 The parallel simple cell mapping method finds the Pareto set and Pareto front efficiently.
 The parallel computing is done on a graphics processing unit.
 Numerical simulations and experiments are done on a rotary flexible arm system.
 The results show that the proposed multi-objective designs are quite effective.
This paper presents a study on encryption algorithms identification by means of machine learning techniques.
 Plain text files.
This paper presents a study on how to accommodate wind power into multiple regions.
This paper presents a study on the use of low resolution infrared images to detect ticks in cattle.
 Emphasis is given to the main factors that influence the quality of the captured images.
This paper presents a substantially simplified axiomatization of Map Theory and proves the consistency of this axiomatization (called MT) in ZFC under the assumption that there exists an inaccessible ordinal.
 Map Theory axiomatizes lambda calculus plus Hilbert's epsilon operator.
 All theorems of ZFC set theory including the axiom of foundation are provable in Map Theory.
This paper presents a systematic approach to develop a resilient software system which can be developed as emerging services and analytics for resiliency.
 While using the resiliency as a good example for enterprise cloud security.
This paper presents a systematic literature review in the Internet of Things and Ambient Intelligence areas.
 The goal was to identify the best software tools that allow end users.
This paper presents a systematic solution of the kinematics of the planar mechanism from the aspect of Assur groups.
 When the planar mechanism is decomposed into Assur groups.
This paper presents a tool that has been developed to support teaching of computer programming in practical laboratory sessions.
 The tool has been implemented as a plugin for the Eclipse IDE and can be deployed in a distributed manner to provide real-time monitoring of the students' activity.
This paper presents a variable transformation strategy for enriching the variables' information content and defining the project target in actual data mining applications based on relational databases with data at different grains.
 In an actual solution for assessing the schools' quality based on official school survey and students tests data.
This paper presents a very simple and efficient encryption scheme based on controlled chaotic maps and ADPCM (Adaptive Differential Pulse Code Modulation) coding.
This paper presents a virtual try-on system to correctly visualize 3D objects (e.
This paper presents an advanced depth intra-coding approach for 3D video coding based on the High Efficiency Video Coding (HEVC) standard and the multiview video plus depth (MVD) representation.
 This paper is motivated by the fact that depth signals have specific characteristics that differ from those of natural signals.
This paper presents an analysis of the security performance and evaluation of the hardware architecture of the redundant bit security (RBS) cryptosystem.
 RBS is a lightweight symmetric encryption algorithm that targets resource-constrained RFID devices.
 Unlike the existing cryptosystems.
This paper presents an analysis of the state of the art solutions for mapping a relational database and an ontology by adding reasoning capabilities and offering the possibility to query the inferred information.
 We analyzed four approaches: Jena with D2RQ.
This paper presents an analytical study on PARSEC benchmark suite in order to examine the auto-vectorization potential of emerging workloads by ICC and GCC compilers.
 For investigating auto-vectorization potential.
This paper presents an approach aimed at mining a new type of pattern in data.
This paper presents an approach for creating digital clothing with application in a virtual dressing room.
 The clothes are made digital by scanning real clothes using a RGB-D sensor.
 While creating digital clothing using specialized programs.
This paper presents an approach for design of framework for the development of a Metacognitive Support System for Novice Programmers (MSSNP) learning Introductory Computer Programming.
 The framework is designed based on Tobias and Everson's model of metacognitive instruction.
This paper presents an approach of static and dynamic visualizations synchronized along with source code in a web-based programming environment.
This paper presents an approach to designing a file manager for speech interfaces.
 Speech interfaces are those which primarily use speech recognition and synthesis for interacting with the user for input and output.
 The present system used in various graphical user interface (GUI) operating systems for file managers is based on the 'What you see is what you get' model providing a 'point and click' approach to support a hierarchical folder structure.
 While the 'point and click' is suitable for GUI.
This paper presents an effective way to enhance the secret key guessing ratio in machine learning based power analysis attack on secure systems such as smartcards.
 The power supply current traces are obtained by varying the atmospheric temperature for all possible values of key.
 The collected power supply current traces are then pre-processed by using wavelet transform.
This paper presents an empirical study based on a set of measures to evaluate the usability of mobile applications running on different mobile operating systems.
This paper presents an evolutionary optimal fuzzy system with information fusion of heterogeneous distributed computing and polar-space dynamic model for online motion control of Swedish redundant robots.
 The intelligent fuzzy system incorporated with the parallel metaheuristic Bacteria Foraging Optimization (BFO)Artificial Immune System (AIS).
This paper presents an experience in developing professional ethics by an approach that integrates knowledge.
This paper presents an explicit.
This paper presents an extension to the Palladio Component Model (PCM).
This paper presents an initiative that seeks to develop some skills of 3-D visualization by using the capabilities of programming and graphical representation of a computer algebra system.
This paper presents an innovative solution based on Time-Of-Flight (TOF) video technology to motion patterns detection for real-time dynamic hand gesture recognition.
 The resulting system is able to detect motion-based hand gestures getting as input depth images.
 The recognizable motion patterns are modeled on the basis of the human arm anatomy and its degrees of freedom.
This paper presents an unsupervised approach to feature binary coding for efficient semantic image retrieval.
 Although the majority of the existing methods aim to preserve neighborhood structures of the feature space.
This paper presents analysis on two 3D mesh to 2D map strategies applied to unwrap images of rock tunnels and facilitate visualization of large datasets.
This paper presents and evaluates an advance hybrid key management architecture for supervisory control and data acquisition (SCADA) networks (HSKMA).
This paper presents Atlas.
This paper presents distributed algorithmic solutions that employ opportunistic inter-agent communication to achieve dynamic average consensus.
 In our solutions each agent is endowed with a local criterion that enables it to determine whether to broadcast its state to its neighbors.
 Our starting point is a continuous-time distributed coordination strategy that.
This paper presents efforts in the direction of teaching computer programming concepts to young students (kids in the range 8 15 years old).
 This work is both innovative and attractive for Romanian primary and secondary schools in the field of computer programming.
 The authors believe that involvement in this work field could prove beneficial for the national educational policy.
This paper presents elastic transactions.
This paper presents HAMSTER.
This paper presents Integrated Circuit (IC) fault detection of a Printed Circuit Board (PCB) model using thermal image processing.
 The thermal image is captured and processed from the PCB model by the finite element method (FEM).
 The histogram features are extracted from the ICs hotspots which are used as inputs in a classifier model.
 The effective features are minimized by the principal component analysis method.
 In this work.
This paper presents overview and student functions of a programming education support tool pgtracer utilizing fill-in-the-blank questions.
 Pgtracer runs under Moodle and provides fill-in-the-blank questions composed of a C++ program and a trace table to the students.
 The tool can provide questions having various difficulty levels from the same program.
 This can be realized by changing the position of the blanks of the program and trace table.
 When a student fills the blanks.
This paper presents preliminary results of the application of two-Kinect cameras system on a two wheeled indoor mobile robot for off-line optimal path planning and execution.
 In our approach.
This paper presents research work related to the development of Wireless Sensor Networks (WSN) gathering environmental data from the surface of the Moon.
 Data aggregation algorithms are applied to reduce the amount of the multi-sensor data collected by the WSN.
This paper presents software suitable for undergraduate students to implement computer programs that compose music.
 The software offers a low floor (students easily get started) but also a high ceiling (complex compositional theories can be modelled).
 Our students are particularly interested in tonal music: such aesthetic preferences are supported.
This paper presents the Multi-Role Project method (MRP).
This paper presents the conceptual foundations of a software system's solution modelling activity.
This paper presents the design and evaluation of a Web-based collaborative learning environment called EduCo for learning and practicing team-based exercises in computer science and software engineering courses.
 EduCo's defining characteristic is integrating a number of services for software development activities.
This paper presents the design and experimental validation of a new model-free data-driven iterative reference input tuning (IRIT) algorithm that solves a reference trajectory tracking problem as an optimization problem with control signal saturation constraints and control signal rate constraints.
 The IRIT algorithm design employs an experiment-based stochastic search algorithm to use the advantages of iterative learning control.
 The experimental results validate the IRIT algorithm applied to a non-linear aerodynamic position control system.
 The results prove that the IRIT algorithm offers the significant control system performance improvement by few iterations and experiments conducted on the real-world process and model-free parameter tuning.
This paper presents the development.
This paper presents the features and functions of a software tool called Fatigue Prediction Utility for Abaqus/CAE (FPU).
 It is designed as a plugin for the Abaqus commercial FE code allowing for fatigue predictions based on the results of Abaqus FE analyses.
 It also contains an interface for data transfer between Abaqus and PragTic.
This paper presents the gait generation and navigation algorithms of an autonomous self-reconfiguring mobile robot platform.
This paper presents the implementation of a research project focused on the use of robotics as an effective tool in teaching technical disciplines to dyslexic children.
 Dyslexia is the most common learning disability and affects between 5 and 12 percent of all students.
 The goal of this project is to develop educational programs which will boost school performance of dyslexic children by using robotics in Science.
This paper presents the information system of multidimensional data analysis and data mining by identification of associative dependences in multidimensional data.
This paper presents the initial findings of an ongoing research effort focused on identifying.
This paper presents the possibilities of combing public-key encryption and digital signature algorithms which are actually based on different mathematical hard problems.
 Since the output of the combination produces an Encrypted signed message.
 In general.
This paper presents the results obtained from the implementation of an infrastructure to improve technological services of email.
This paper presents the results of an online survey that was conducted in 2014 to assess the evolution of computing in architecture.
This paper presents the results of two educational experiments carried out to determine whether the process of specifying requirements (catalog-based reuse as opposed to conventional specification) has an impact on effectiveness and productivity in co-located and distributed software development environments.
 The participants in the experiments were 76 students enrolled in three courses on project management for software development at the University of Murcia.
This paper presents the software framework established to facilitate cloud-hosted robot simulation.
 The framework addresses the challenges associated with conducting a task-oriented and real-time robot competition.
This paper presents the use of a computer cluster with heterogeneous computing components to provide concurrency and multi-level parallelism at coarse grain and massive fine-grain for multiview video coding (MVC) applications.
 MVC involves coding of multiple video sequences that are taken from the same scene but different perspective.
 In addition to motion estimation (ME) used in conventional video coding for single view video for exploiting inter-frame temporal similarities.
This paper presents the work carried out by the authors to apply Intel MKL PARDISO (a parallel sparse matrix solver) to the load flow solution algorithms of MATPOWER and OpenDSS.
 The goal is to explore the potential execution time reduction obtained when working with large power systems and multi-core installations.
 Test systems of different sizes were solved in order to observe the time reduction as function of the system size and the number of cores used in the parallel execution.
 Results show that except for the Full Newton-Raphson (NR) algorithm.
This paper presents two within-subjects studies (n=23) exploring how different combinations of visual and auditory feedback influence perceived realism.
This paper proposed a universal method for implementing PTP (Precision Time Protocol) for test systems with the 1000M Ethernet Interface.
 To achieve the synchronization accuracy of sub-microsecond.
This paper proposed an energy efficient adder employing multistage latency and approximate computing technology.
 The delay of the adder decreases after the critical path of the adder is divided into multiple short stages with series of predictors.
This paper proposed the several real life applications for big data analytic using parallel computing software.
 Some parallel computing software under consideration are Parallel Virtual Machine.
This paper proposes a concise and effective approach termed discriminative feature representation (DFR) for low dose computerized tomography (LDCT) image processing.
This paper proposes a design procedure to satisfy steady state and stable transient characteristics of closed loop and poles of controller of strongly stable generalized predictive control(GPC) systems.
 Strongly stable control system is defined as a system having both of stable poles of closed loop systems and stable poles of controllers.
 The strong stability is important for safety.
This paper proposes a flexible control framework for relational personal data that enforces data originators' dissemination policies.
 Inspired by the sticky policy paradigm and mandatory access control.
This paper proposes a framework for the development of sensor node software for various operating systems in a sensor network environment.
 The proposed development framework consists of attributes.
This paper proposes a high-performance graphics processing unit (GPU)-based parity computing scheduler.
This paper proposes a mapping between two product quality and software processes models used in the industry.
This paper proposes a new approach for power system transient stability simulation.
This paper proposes a new cryptosystem system that combines DNA cryptography and algebraic curves defined over different Galois fields.
 The security of the proposed cryptosystem is based on the combination of DNA encoding.
This paper proposes a new distributed architecture for supervised classification of large volumes of earth observation data on a cloud computing environment.
 The architecture supports distributed execution.
This paper proposes a new group-based accelerating structure called hybrid structure for the ray tracing of dynamic scenes.
This paper proposes a new high-level approach for optimising field programmable gate array (FPGA) designs.
 FPGA designs are commonly implemented in low-level hardware description languages (HDLs).
This paper proposes a new lightweight cipher VAYU.
 VAYU has a balanced Feistel structure.
 VAYU cipher supports 64 bit plaintext and 128/80 bit key length and it has a total of 31 rounds.
 It needs only 1290 GEs for 128 bit key length.
 It also results in minimal memory size as compared to all other existing lightweight ciphers.
 This paper discusses the security analysis of VAYU cipher design which is adequate against linear and differential cryptanalysis.
This paper proposes a new method to generate interpolation paths between two given molecular conformations.
 It relies on the As-Rigid-As-Possible (ARAP) paradigm used in Computer Graphics to manipulate complex meshes while preserving their essential structural characteristics.
 The adaptation of ARAP approaches to the case of molecular systems is presented in this contribution.
 Experiments conducted on a large set of benchmarks show how such a strategy can efficiently compute relevant interpolation paths with large conformational rearrangements.
This paper proposes a novel approach for building a Natural Language Interface to a Relational Database (NLI-RDB) using Conversational Agent (CA).
This paper proposes a novel distance estimation method to build a forward collision avoidance assist system (FCAAS) containing techniques of lane marking detection.
This paper proposes a novel high-speed visual target tracking system based on mixed rotation invariant description (MRID) and skipping searching method.
 MRID is a novel rotation invariant description of texture and edge information by annular histograms and dominant direction.
 It overcomes rotation variant and large computation issues in conventional LBP-HOG feature description.
 The skipping searching method used in tracking can remarkably decrease the computation time by avoiding repeated searching operations.
 The proposed tracking system contains an image sensor.
This paper proposes a novel method of semi-regular remeshing for triangulated surfaces to achieve superior triangles lead to advanced visualization of 3D model.
 It is based on mesh segmentation and subdivision surface fitting which uses curvature-adapted polygon patches.
 Our contribution lies in building a sophisticated system with three stages.
This paper proposes a novel unsupervised damage detection approach based on a memetic algorithm that establishes the normal or undamaged condition of a structural system as data clusters through a global expectation-maximization technique.
This paper proposes a parallel computation model for the self-organizing map (SOM) neural network applied to Euclidean traveling salesman problems (TSP).
 This model is intended for implementation on the graphics processing unit (GPU) platform.
 The Euclidean plane is partitioned into an appropriate number of cellular units.
This paper proposes a parallel regression formulation to reduce the computational time of variable selection algorithms.
 The proposed strategy can be used for several forward algorithms in order to select uncorrelated variables that contribute for a better predictive capability of the model.
 Our demonstration of the proposed method include the use of Successive Projections Algorithm (SPA).
This paper proposes a parallelized online optimization of low voltage distribution network (LVDN) operation.
 It is performed on a graphics processing unit (GPU) by combining the optimization procedure with the load flow method.
 In the case study.
This paper proposes a recognition algorithm for solving the puzzle of Quickly Response (QR) identification at highly deviated shooting angle.
 This algorithm includes five procedures.
This paper proposes a simple approach to measure the elbow joint angle (EJA) using galvanic coupling system (GCS).
This paper proposes a simple method to add optional live-streaming to a highly interactive class in a way that reduces the number of students physically present in the classroom and doesn't hurt student performance.
 The effect of allowing live-streaming was analyzed by running an experiment where partway through the semester students in a flipped advanced level Computer Graphics class were given the option to attend the class remotely using a live-stream of the class with required use of a multi-featured audience response system (TeachBack).
 The results show that allowing remote attendance with TeachBack decreased the number of students attending physically from 93% to 75% of registered students and had no effect on unexcused absences.
 Students believed that their remote attendance was just as effective for their learning as attending classes face-to-face and analysis of the data confirmed their beliefs; they did however generally prefer being physically present in class.
 Our results suggest that appropriate use of live-streaming coupled with an Audience Response System can reduce some of the demand for large lecture halls by allowing a self-selected subset of students to attend some or all of the classes remotely.
 A large percentage of the students in the class felt that a live-streaming option should be added to most classes and this paper demonstrates that this goal is feasible and pedagogically justifiable.
This paper proposes a statistical framework to develop user-adapted spoken dialog systems.
 The proposed framework integrates two main models.
 The first model is used to predict the user's intention during the dialog.
 The second model uses this prediction and the history of dialog up to the current moment to predict the next system response.
 This prediction is performed with an ensemble-based classifier trained for each of the tasks considered.
This paper proposes a trajectory of vehicles encountering an incident site and offers six indices for evaluating the impact of the incident on traffic patterns based on global positioning system (GPS) data.
 Traffic incidents are the major contributors to non-recurrent delays.
 Mitigating their effects necessitates an appropriate and efficient allocation of equipment and personnel for a clearance plan.
 Reliable real-time traffic impact evaluations are essential prerequisites for this task.
 Most of the existing duration models.
This paper proposes an approach for a robust tracking method to the objects intersection with appearances similar to a target object.
 The target is image sequences taken by a moving camera in this paper.
 Tracking methods using color information tend to track mistakenly a background region or an object with color similar to the target object since the proposed method is based on the particle filter.
 The method constructs the probabilistic background model by the histogram of the optical flow and defines the likelihood function so that the likelihood in the region of the target object may become large.
 This leads to increasing the accuracy of tracking.
 The probabilistic background model is made by the density forests.
 It can infer a probabilistic density fast.
 The proposed method can process faster than the authors' previous approach by introducing the density forests.
 Results are demonstrated by experiments using the real videos of outdoor scenes.
This paper proposes an approach using taxonomic relatedness for answer-type recognition and type coercion in a question-answering system.
 We introduce a question analysis method for a lexical answer type (LAT) and semantic answer type (SAT) and describe the construction of a taxonomy linking them.
 We also analyze the effectiveness of type coercion based on the taxonomic relatedness of both ATs.
 Compared with the rule-based approach of IBM's Watson.
This paper proposes an explicit definition of green software requirements and a tool to support their evaluation .
 The proposed evaluation tool describes the green efficiency by considering the energy consumption as the main aspect to be studied during the development stage.
 This approach consists of building a multiple regression model.
This paper proposes an image encryption scheme based on Cellular Automata (CA).
 CA is a self-organizing structure with a set of cells in which each cell is updated by certain rules that are dependent on a limited number of neighboring cells.
 The major disadvantages of cellular automata in cryptography include limited number of reversal rules and inability to produce long sequences of states by these rules.
 In this paper.
This paper proposes an image information hiding algorithm which bases on the HVS and MBNS.
 This algorithm uses the characteristics of Human Visual System (HVS) to embedded more large amounts of data according to different area to achieve different degree of embedded.
This paper proposes an ultra lightweight cipher ANU.
 ANU is a balanced Feistel-based network.
 ANU supports 64 bit plaintext and 128/80 bit key length.
This paper proposes and tests models that provide quick searching and retrieval of continuations and the longest repeated suffix for data sequences.
This paper provides a selective eraser of curvature extrema for B-spline curves.
 It is introduced as an extension to the standard least squares method for approximating a series of points using a B-spline.
 The extension consists in adding constraints to produce segments of curve with monotone increase or decrease of curvature.
 The primal-dual interior point method is used to solve the constrained optimization problem.
 The method requires gradients that are computed using B-spline symbolic operators.
This paper provides an ethnographical study of the ways in which infrastructure matters in the production of knowledge in the social worlds of rare diseases.
 We analyse the role played by a relational database in this respect.
This paper provides an overview of the state of the art technologies for software development in cloud environments.
 The surveyed systems cover the whole spectrum of cloud-based development including integrated programming environments.
This paper reports on an investigation into the optimum design for a passive tuned mass damper system which takes account of soil-structure interaction.
 A multi-objective optimisation approach for minimising the displacement.
This paper reports on the effects of a teaching tool that controls actual robots during introductory education in computer programming among high school students.
 The effects of the tool were investigated by means of a questionnaire following two types of experience based programming classes.
This paper reports our research in developing a cyberinfrastructure platform to support multivariate visualization of data collected from distributed sensor network.
 Three new techniques were introduced in this platform: (1) a hybrid data caching strategy that takes advantages of a scalable and distributed time series database.
This paper reports recent advances in the development of a symbolic asymptotic modeling software package MEMSALab which will be used for automatic generation of asymptotic models for arrays of micro and nanosystems.
This paper reports recent advances in the development of a symbolic asymptotic modeling software package.
This paper reports results of an action research conducted in an outreach program aimed at introducing secondary school students to the exciting world of computer programming.
 The key challenges were to introduce programming and problem solving session that can be done in a very short time (within 1 to 2 hours) and in a fun way.
 This was achieved by using a physical mobile robot called RoboKar to engage students with the problem solving activities involving programming.
 From the entry and exit surveys.
This paper reports the use of robots in teaching the essence of Science.
This paper reviews the concept of designing an open.
This paper seeks to contribute to the growing literature on children and computer programming by focusing on a programming language for children in Kindergarten through second grade.
 Sixty-two students were exposed to a 6-week curriculum using ScartchJr.
 They learned foundational programming concepts and applied those concepts to create personally meaningful projects using the ScratchJr programming app.
 This paper addresses the following research question: Which ScratchJr programming blocks do young children choose to use in their own projects after they have learned them all through a tailored programming curriculum? Data was collected in the form of the students' combined 977 projects.
This paper starts with the basic process of music recognition to complete the study on extraction and realization of seven musical characteristics of the music features characterization.
This paper studied anti-attack performance testing methods in large network.
 Anti-attack performance testing process in large networks is different from traditional testing process.
This paper studies how to use the USB interface to achieve communication between Sunplus 61A MCU and ordinary personal computer.
 The design of the entire system has 5 aspects.
This paper studies robust resource-allocation algorithm design for a multiuser multiple-input-single-output (MISO) cognitive radio (CR) downlink communication network.
 We focus on a secondary system that provides wireless unicast secure layered video information to multiple single-antenna secondary receivers.
 The resource-allocation algorithm design is formulated as a nonconvex optimization problem for the minimization of the total transmit power at the secondary transmitter.
 The proposed framework takes into account a quality-of-service (QoS) requirement regarding video communication secrecy in the secondary system.
This paper studies simultaneous feature selection and extraction in supervised and unsupervised learning.
 We propose and investigate selective reduced rank regression for constructing optimal explanatory factors from a parsimonious subset of input features.
 The proposed estimators enjoy sharp oracle inequalities.
This paper studies the evolution of crescent-shaped dune under the influence of injected flux.
 A scaling law and a wind tunnel experiment are carried out for comparison.
 The experiment incorporates a novel image processing algorithm to recover the evolutionary process.
 The theoretical and experimental results agree well in the middle stage of dune evolution.
This paper studies the group consensus problem in networks of multi-agent systems.
This paper studies the l(q)(0 < q < 1) regularized least squares regression (l(q)LS) problem.
This paper studies the resource allocation algorithm design for secure information and renewable green energy transfer to mobile receivers in distributed antenna communication systems.
 In particular.
This paper studies the software implementation in an ultrasonic imaging system of Total Focusing Method.
 In order to accomplish real-time requirements parallel programming techniques have been used.
This paper studies the temporal behavior of communication flows in the Internet.
 Characterization of flows by temporal patterns supports traffic classification and filtering for network management and network security in situations where full packet data is pot accessible (e.
This paper studies the time complexity of gene expression programming based on maintaining elitist (ME-GEP).
 Using the theory of Markov chain and the technique of artificial fitness level.
This paper surveys value systems for developmental cognitive robotics.
 A value system permits a biological brain to increase the likelihood of neural responses to selected external phenomena.
 Many machine learning algorithms capture the essence of this learning process.
This paper tackles the problem of automatic detection of knee osteoarthritis.
 A computer system is built that takes as input the body kinetics and produces as output not only an estimation of presence of the knee osteoarthritis.
This paper utilizes Jumarie's modified Riemann-Liouville derivative and extended G'/G-expansion method to discuss the soliton solutions of the nonlinear time fractional parabolic equations.
 Exact solutions are expressed in terms of hyperbolic and trigonometric functions.
 These solutions may be useful and desirable to explain some nonlinear physical phenomena in genuinely nonlinear fractional calculus.
 Several constraint conditions naturally emerge from the results obtained and these conditions are also listed.
 (C) 2016 Elsevier GmbH.
 All rights reserved.
This paper was aimed to investigate the influence of online coding courses on logical reasoning ability towards fifth and sixth graders.
 Students used Code.
org online programming courses about three months.
 Through Raven's Standard Progressive Matrices before and after testing to understand the change of students' logical reasoning ability in the experiment.
 And based on Technology Acceptance Model.
This poster details a within-subjects study (n=17) investigating the effects of vibrotactile stimulation on illusory self-motion.
This practical report analyzes a programming class using a micro robot (MR).
This publication presents a computer method for segmenting microcalcifications in mammograms.
 It makes use of morphological transformations and is composed of two parts.
 The first part detects microcalcifications morphologically.
This research deals with a novel approach to classification.
 New classifiers are synthesized as a complex structure via evolutionary symbolic computation techniques.
 Compared to previous research.
This research focuses on improving relational databases design of novice designers.
 Teaching conceptual data modelling is a challenge to Information Systems educators.
 Our goal is to test the effectiveness of the learning from errors approach in the area of conceptual data modelling of relational databases.
 For understanding the difficulties that novice designers encounter in the activity of conceptual modelling.
This research investigated the application of 915 MHz microwaves in pasteurizing.
This research is focused on the use of active infrared thermography as a non-destructive testing technique for damage detection in carbon fiber reinforced plastics (CFRPs).
 The aim of this study is to examine the efficiency of various mathematical methods in thermographic data processing.
This research paper summarizes the results of an implementation of fuzzy multilevel methodology to rank software reliability measures.
This research presents an encryption technique based on the programmable cellular automata (PCA) theory for realizing a hardware cryptosystem that can be applied in high-speed data communication networks.
 The PCA blocks proposed in this article makes the encryption system much more resistant on different types of attacks and the cellular automata's parallel information processing property give high speed.
 In this paper it is shown how a series of simple elements called cells interact using certain evolution rules and topologies to form a larger body that acts like an encryption system.
 The proposed architecture is implemented efficiently in hardware.
This research quantifies the rate and volume of oil and gas released from two natural seep sites in the Gulf of Mexico: lease blocks GC600 (1200 m depth) and MC118 (850 m depth).
 Our objectives were to determine variability in release rates and bubble size at five individual vents and to investigate the effects of tidal fluctuations on bubble release.
 Observations with autonomous video cameras captured the formation of individual bubbles as they were released through partially exposed deposits of gas hydrate.
 Image processing techniques determined bubble type (oily.
This research suggests maximum power point tracking (MPPT) for the solar photovoltaic (PV) power scheme using a new constant voltage (CV) fractional order incremental conductance (FOINC) algorithm.
 The PV panel has low transformation efficiency and power output of PV panel depends on the change in weather conditions.
 Possible extracting power can be raised to a battery load utilizing a MPPT algorithm.
 Among all the MPPT strategies.
This research was conducted within the framework of a National Science Foundation sponsored summer Research Experience for Undergraduate (REU) students.
 This research considers small-scale and mathematical models of simple one-story structures that are subjected to free and base-motion excitations and installed with and without passive damping devices to gain an understanding of their dynamic behavior while reviewing active and semi-active damping means being applied and researched today.
 Using computer programming and numerical methods.
this section based on the GIS technology.
This study aimed to investigate the effectiveness of game-based learning in supporting IT education particularly in network security topics.
 To attain this objective.
This study aimed to screen lymphatic metastasis-related microRNAs (miRNAs) in lung adenocarcinoma and explore their underlying mechanisms using bioinformatics.
 The miRNA expression in primary lung adenocarcinoma.
This study aims to advocate that a visual programming environment offering graphical items and states of a computational problem could be helpful in supporting programming learning with computational problem-solving.
 A visual problem-solving environment for programming learning was developed.
This study aims to analyze the users' form of interaction and problems in terms of interface design considering the people's constant interaction with mobile devices which has an important place in their daily life.
 For this purpose.
This study aims to find out the effect of computer based teaching activities on the academic achievement and retention of technical programme Students of Computer Programming on Vocational Foreign Language in Computer Technologies Department.
 The study was conducted in Vocational School of Technical Science in Suleyman Demirel University with 30 students of computer programming.
 The experimental group (15 students) was taught with computer assisted instructional software and the control group (15 students) was taught with traditional methods.
 Achievement test was developed to measure the success of second class students of Vocational Foreign Language lesson was used as pretest.
This study aims to investigate the initial perceived knowledge and skills of high school students in information technology (IT) security and the effect of an online security support tool.
This study combined theoretical.
This study concentrates on finding all truncated impossible differentials in substitution-permutation networks (SPNs) ciphers.
 Instead of using the miss-in-the-middle approach.
This study considers linear precoding for secure transmission in a multiuser peer-to-peer relay network with finite alphabet input.
 Under the assumption that the global channel-state-information is available.
This study contains an examination of the missing data structures.
This study describes image processing systems based on an artificial neural network to estimate tool wear.
 The Single Category-Based Classifier neural network was used to process tool image data.
 We present a method to determine the rate of tool wear based on image analysis.
This study designed a path planning method based on fuzzy algorithm and genetic fuzzy algorithm for the security patrol robot.
This study develops a novel cervical precancerous detection system by using texture analysis of field emission scanning electron microscopy (FE-SEM) images.
 The processing scheme adopted in the proposed system focused on two steps.
 The first step was to enhance cervical cell FE-SEM images in order to show the precancerous characterization indicator.
 A problem arises from the question of how to extract features which characterize cervical precancerous cells.
 For the first step.
This study evaluated a strategy for identifying 3D scapulothoracic orientation using bilateral X-ray scans and 3D scapula models.
 Both subject-specific scapula models and a scaled general model were utilized.
 3D scapulothoracic orientations obtained from X-rays were compared to motion capture data.
 Subjects consisted of a skeletal model of a human torso and ten real bone scapulae.
 Retroreflective markers were placed on the scapulae and a three-marker triad was placed on the trunk.
 Marker positions were recorded using an eight camera motion capture system.
 A biplane X-ray system from EOS Imaging was used to collect two orthogonal 2D images of the skeleton and markers.
 Custom software was created for the 3D to 2D matching process.
 The results indicated that the matched orientations compared favorably to motion capture orientations.
This study examined the effects of display curvature (400.
This study examines whether friendship facilitates or hinders learning in a dyadic instructional setting.
 Working in 80 same-sex pairs.
This study explains the design and implementation of a Virtual Reality (VR) framework for 'fun-based' interactive programming instruction in engineering education courses.
 Students continue to face several difficulties when learning programming [1] and the lack of efficient tools to overcome such difficulties can affect the students' motivation [2].
This study explored the potential of computer vision system (CVS) and hyperspectral imaging (HSI) technique covering spectral range of 900-1700 nm for identifying freezer burnt salmon fillets after frozen storage.
 Local binary pattern (LBP) descriptor was applied for the RGB image classification.
 Reflectance spectra were obtained from various positions surface and pretreated using the standard normal variate (SNV) transformation.
 TreeBagger classifier was used to build classification models for recognition and authentication of the freezer burnt flesh.
 The results suggested that hyperspectral discrimination performed much better than CVS with the correct classification rate (CCR) of 0.
905 in validation and CCR of 0.
945 in cross validation.
 The effective wavelengths were selected based upon the feature importance in the TreeBagger model and the corresponding optimized model yielded CCR of 0.
914 in validation and 0.
978 in cross-validation.
This study explored the use of self-assessments.
This study explores a new security problem existing in various state-ofthe- art quantum private comparison (QPC) protocols.
This study focuses on ant colony optimization (ACO) in decision making of skills and tactics in badminton.
 The model about decision making of skills and tactics in badminton based on ACO is put forward and explained.
This study introduces the Tool for the Automatic Analysis of Cohesion (TAACO).
This study investigates the ignition characteristics of pulverised coal.
This study investigates the success of a multiobjective genetic algorithm (GA) combined with state-of-the-art machine learning (ML) techniques for the feature subset selection (FSS) in binary classification problem (BCP).
 Recent studies have focused on improving the accuracy of BCP by including all of the features.
This study presented a mechanism to automatically identify consecutiveness of textile patterns so as to help classify the ever-growing number of patterns.
 The pattern consecutiveness was first decomposed into two factors.
This study presents a new method to computes analytical fragility curves of a structure subject to tsunami waves.
 The method uses dynamic analysis at each stage of the computation.
This study presents a protection method for fingerprint templates by using fused structures at the feature level.
 The authors compute two transformed features from minutiae points: namely.
This study presents an automatic.
This study presents several meet-in-the-middle attacks on reduced-round Crypton and mCrypton block ciphers.
 Using the generalised -set.
This study presents the results from the rheological measurement of clay suspensions using vane geometry in a wide gap configuration.
 It focuses on how measurement of viscosity cannot be effective for two reasons: the limits of the vane geometry itself and the limits of the material depending on its content of solid particles.
 Image analysis of the flow while shearing the material is carried out to relate the flow behavior.
 Several approaches to compute the shear flow curve from torque-rotational velocity data are used.
 The results demonstrate that the applied setpoint while applying a logarithmic shear rate ramp can be very different from the calculated shear rate from existing theories.
 Depending on the solid volume fraction of the particles in the mixture.
This study represents a continuation of a previously performed study by Sorensen.
This study sought to determine the features of an ideal serve in men's professional tennis.
 A total of 25.
This study started from the perspective of predicting science and technology competition.
This study tested the use of machine learning techniques for the estimation of above-ground biomass (AGB) of Sonneratia caseolaris in a coastal area of Hai Phong city.
This study was carried out to explore the potential of computer vision system (CVS) and two hyper spectral imaging (HSI) systems covering the visible and short-wave near infrared range (400-1000 nm) and the long-wave near infrared range (897-1753 nm).
This work aims design of a system to ensure the availability of IT services in to services public Company in the municipality of Florencia - Caqueta.
This work aims to discriminate among different species of the genus Cistus.
This work analyses the use of parallel processing techniques in synthetic aperture ultrasonic imaging applications.
 In particular.
This work considers a parallel algorithm for solving multidimensional multiextremal optimization problems.
 This algorithm uses Peano-type space filling curves for dimension reduction.
 Conditions of non-redundant parallelization of the algorithm are considered.
 Efficiency of the algorithm on modern computing systems with the use of graphics processing units (GPUs) is investigated.
 Speedup of the algorithm using GPU as compared with the same algorithm implemented on CPU only is demonstrated experimentally.
 Computational experiments are carried out on a series of several hundred multidimensional multiextremal problems.
This work demonstrates use of a smart mobile phone installed with an Android application.
This work describes a system for the automatic generation of full-fledged API layers from RDF schemas.
This work describes a technique for generating parametric surfaces meshes using parallel computing.
This work describes functional and structural transitions of a novel protease isolated from Conidiobolus brefeldianus MTCC 5185 (Cprot).
This work describes modern methods of parallelizing time consuming problems.
 Commonly used parallelization methods.
This work develops a method for calibrating a crystal plasticity model to the results of discrete dislocation (DD) simulations.
 The crystal model explicitly represents junction formation and annihilation mechanisms and applies these mechanisms to describe hardening in hexagonal close packed metals.
 The model treats these dislocation mechanisms separately from elastic interactions among populations of dislocations.
This work focuses on the development of a multiscale computational fluid dynamics (CFD) simulation framework with application to plasma-enhanced chemical vapor deposition of thin film solar cells.
 A macroscopic.
This work is focused on the parallel algorithms in efficient global optimization.
This work is the extended version of Alam.
This work obtains the disguise version of exact solitary wave solutions of the generalized (2+1)-dimensional Zakharov-Kuznetsov Benjamin-Bona-Mahony and the regularized long wave equation with some free parameters via modified simple equation method (MSE).
 Usually the method does not give any solution if the balance number is more than one.
This work presents a low-complexity audio coder-decoder (CODEC) based on fixed-point arithmetic to save the usage of system resources in a low-end embedded system.
 To reduce time complexity and memory usage.
This work presents a method of characterising pipeline defects using a small number of radiographs taken at different angles around the pipe.
 The method relies on knowledge of the setup geometry and use of multiple images.
This work presents a new method of examining the structure of public-transport networks (PTNs) and analyzes their topological properties through a combination of computer programming.
This work presents a new methodology to automate the derivation of Building Energy Models (BEMs) from complex 3D Computer-Aided Design (CAD) geometry.
 The goal is to combine current parametric modeling.
This work presents a noninvasive methodology to obtain biomedical thermal imaging which provide relevant information that may assist in the diagnosis of emotions.
 Biomedical thermal images of the facial expressions of 44 subjects were captured experiencing joy.
This work presents a novel approach for decisionmaking for multi-objective binary classification problems.
 The purpose of the decision process is to select within a set of Pareto-optimal solutions.
This work presents a novel strategy to decipher fragments of Egyptian cartouches identifying the hieroglyphs of which they are composed.
 A cartouche is a drawing.
This work presents a parallel approach of the Kinetic Monte Carlo (KMC) algorithm using a distributed memory architecture.
 The resulting computer software was tested by conducting crystal growth simulations on barite (001) face.
 Execution times.
This work presents a software system designed to track the reproduction of a musical piece with the aim to match the score position into its symbolic representation on a digital sheet.
 Into this system.
This work presents an ensemble of Attractor Neural Networks (ANN) modules.
This work presents an interactive self-learning tool named Technical Drawing Learning Tool-Level 2TDLT-L2for teaching manufacturing dimensioning to engineering students.
 The tool was designed for the students enrolled in the first year of the Bachelor in Management and Mechanical Engineering of the Universities of Brescia and Udine.
 It consists of a simple interactive tool.
This work presents an on-line.
This work presents the application of terahertz imaging to three-dimensional formalin-fixed.
This work presents the development of two free graphical user interfaces (GUIs).
This work presents the Intelligent Trading Architecture (ITA).
This work presents the uncertainty quantification.
This work presents two implementation attacks against cryptographic algorithms.
 Based on these two presented attacks.
This work proposes a clusterization algorithm called k-Morphological Sets (k-MS).
This work proposes a three-wave method with a perturbation parameter to obtain exact multi-soliton solutions of nonlinear evolution equation.
 The ()-dimensional KdV equation is used as an example to illustrate the effectiveness of the suggested method.
 Using this method.
This work proposes to include security and privacy into to context of Linked Open Data (LOD) and the Semantic Web.
This work represents a first attempt to study the effects of financial constraints on firm growth within the business services as it can be argued that firms are different not only in terms of size but also in the way they operate in a specific industry.
This work suggests a method for systematically constructing a software-level environment model for safety checking automotive operating systems by introducing a constraint specification language.
This work uses the 2-D C5G7 benchmark to verify the accuracy and efficiency of the MOCUM code.
This work was undertaken to determine the genesis and role of creativity (Cr) in the formation of mental qualities that gave Homo sapiens (HS) the evolutionary advantages in intra- and interspecific competition during the period of the intraspecific bifurcation of hominids on the border of the Middle and Upper Paleolithic.
 Creativity allowed HS to design the adaptive forms of purposeful behavior corresponding to the conditions and the degree of uncertainty.
Thoth is a standalone software application with a graphical user interface for making it easy to query.
Three dimensional data structures such as batch process data or infra-red spectral measurements usually contain inconsistent trajectories of various durations and quality.
 In the case of batch process data.
Three identical solar tunnel greenhouse dryers under forced convection mode were installed for drying peppermint plants.
 Thermal analysis of the solar tunnel greenhouse peppermint dryer was investigated based on thermal balance equations in order to predict its performance.
 Performance of solar greenhouse dryer was studied as a function of change in plant conditions.
Three key drivers of change in the world of software are identified.
Three persistent common problems in satellite ground control software are obsolescence.
Three-dimensional (3D) building models are essential for 3D Geographic Information Systems and play an important role in various urban management applications.
 Although several light detection and ranging (LiDAR) data-based reconstruction approaches have made significant advances toward the fully automatic generation of 3D building models.
Three-dimensional (3D) computer graphics technology is widely used in various areas and causes profound changes.
 As an information carrier.
Three-dimensional (3-D) representations of urban regions have gained much attention because of recent developments in remote sensing and computer graphics technologies.
 In particular.
Three-dimensional (3D) vision based scanning for metrology and inspection applications is an area that has attracted increasing interest in the industry.
 This interest is driven by the recent advances in 3D technologies.
Three-dimensional modeling is the foundation of graphic expression and virtual reality of digital landscape.
 This article explores key technologies.
Three-factor mutually authenticated key agreement protocols for multi-server environments have gained momentum in recent times due to advancements in wireless technologies and associated constraints.
 Several authors have put forward various authentication protocols for multi-server environment during the past decade.
 Wang et al.
 recently proposed a biometric-based authentication with key agreement protocol for multi-server environment and claimed that their protocol is efficient and resistant to prominent security attacks.
 The careful investigation of this paper shows that Wang et al.
 protocol's users are sharing personal identifiable information with the application servers during the registration and authentication process.
 This nature of disclosing credentials leads to severe threats particularly insider attacks.
Through generating the d-dimensional GHZ state in the Z-basis and measuring it in the X-basis.
Through Hirota bilinear transformation and symbolic computation with Maple.
Through symbolic computation with Maple.
Through symbolic computation with Maple.
Through the analysis and process of visual images of gesture words.
Through the study of various problems in the elimination game.
Throughout its short history.
Tidal notches are a generally accepted sea-level marker and maintain particular interest for palaeoseismic studies since coastal seismic activity potentially displaces them from their genetic position.
 The result of subsequent seismic events is a notch sequence reflecting the cumulative coastal uplift In order to evaluate preserved notch sequences.
Time is pervasive of reality.
Time series of categorical data is not a widely studied research topic.
 Particularly.
Time-triggered architectures form an important component of many distributed computing platforms for safety-critical real-time applications such as avionics and automotive control systems.
Timing is critical when trying to engage students in various engineering career paths.
 While many National Engineers Week programs exist for primary and middle school students.
Tinnitus (ringing in the ear) is characterized by the perception of a sound in the absence of a corresponding acoustic stimulus.
 While many affected people habituate to the phantom sound.
Tissue P systems are distributed parallel computing models inspired by the structure of tissue and the way of communicating substances between two cells or between a cell and the environment.
 In this work.
Tissue P systems with symport/antiport rules are a class of distributed parallel computing models inspired by the cell intercommunication in tissues.
To access the distributed web applications in a computer network.
To accurately construct the topographic information of a six-legged walking robot in real time.
To achieve competitive advantages.
To achieve energy-conservation and prompt responses simultaneously.
To achieve neuronal differentiation of mouse bone mesenchymal stem cells (bMSCs) into neuron-like cells and explore the role of miR-122-5p that may regulate T-box brain 1 (Tbr1) expression during the induction.
 BMSCs were cultured and induced with butylated hydroxyanisole.
To actively promote the deep collaboration and coordinated development between enterprises and universities and research institutes has great importance to building an innovative country.
 In this paper.
To address the issue of internal network security.
To analyze quantitatively the interface of core-shell structural bamboo plastic composites (BPCs) surface.
To assist the rehearsal and planning of robot-assisted partial nephrectomy.
To better understand the fundamental interactions between melt jet and coolant during a core-disruptive accident at a sodium-cooled fast reactor.
To combine the complementary strengths of human vision (HV) and computer vision (CV) in target image retrieval.
To construct interactive graphics such as graphical user interfaces and interactive webpages is an important matter in computer programming.
 For this purpose.
To deal with information incompleteness or error and supervising difficulties caused by hardships in information synergy and sharing among current medical institutions.
To digitize subspaces of the Euclidean nDspace.
To ease the programming burden and to make parallel programs more maintainable.
To enable a prosperous Internet of Things (IoT).
To enable efficiency in stream processing.
To evade the well-known impossibility of unconditionally secure quantum two-party computations.
To evaluate the visual quality in visual secret sharing schemes.
To fully take advantage of external charging conditions and reduce fuel consumption for extended-range electric vehicles.
To gain a better understanding of cellular and molecular processes.
To harness a heterogeneous memory hierarchy.
To harness a heterogeneous memory hierarchy.
To help the clinicians to segment the borders of the left ventricle (LV) efficiently during measurement of the heart.
To implement bilingual teaching is an inevitable choice of higher vocational colleges to be geared to international standards and improve their professional competitive advantages.
 The practice of bilingual teaching of computer courses has been implemented by the School of Information.
To improve practical IT education.
To improve the availability of the software-defined networking (SDN) under distributed denial of service (DDoS) attacks.
To improve the energy conversion ability and well utilize renewable resources.
To improve the flexibility and robustness of the engineering of automated production systems (aPS) in the case of extending.
To improve the inversion accuracy of time-domain airborne electromagnetic data.
To increase software performance.
To investigate the behavior of biochemical systems.
To manage the smart electric grid of the future.
To meet ever increasing load demand in a sustainable way.
To overcome limitations of periodic separations of proteins in batch chromatographic columns Carousel Multi-Column Setup (CMS) has been recently suggested and theoretically analyzed in a previous study (R.
To predict network security situations better using expert knowledge and quantitative data.
To seek Infinite sequence exact solutions to (2+1)-dimensional ZK-MEW equation.
To seek the exact double non-traveling wave solutions of nonlinear partial differential equations.
To simulate welding induced transient thermal stress and deformation of large scale FE models.
To solve the adverse effects brought by resource node transfering the using right to local task and the difficult problem of resource load balancing.
To solve the different time delays that exist in the control device installed on spatial structures.
To solve the problem of throughput decrease caused by spectrum interference in Wireless Local Area Networks (WLAN).
To solve unpunctual delivery.
To support data intensive cluster computing.
To the best of our knowledge.
To understand how root growth responds to temperature.
Today DNS servers run on many different applications and operating systems what means there are many options how to protect DNS server.
 Each regular application has implemented security mechanisms that protect the system from standard attacks.
 DNS service works on application layer.
Today managing data is very critical for all kind of users.
 Some users use Cloud Database Services for managing their data but at the same time for the security reasons they want to keep some crucial private data at their own end so they face problem like how to integrate both kind of data one located at Local RDBMS and the other at Cloud repositories.
 In this paper we have analyzed that Hadoop can be used to solve such kind of problems.
 Hadoop platform is used to integrate data one is located at the Cloud databases and the other located at the Local RDBMS.
 Hadoop components not only process huge amount of data but with the help of inbuilt security mechanisms one can also securely store their data.
Today many different devices and operating systems can be used for InfoVis systems.
 On the one hand.
Today's complex and fast-evolving world necessitates young students to possess design and problem-solving skills more than ever.
 One alternative method of teaching children problem-solving or thinking skills has been using computer programming.
Today's Internet makes hosts and individual networks inherently insecure because permanent addresses turn destinations into permanent attack targets.
 This paper describes an Evasive Internet Protocol (EIP).
Today's modern world requires a digital watermarking technique that takes the redundancy of an image into consideration for embedding a watermark.
 The novel algorithm used in this paper takes into consideration the redundancies of spatial domain and wavelet domain for embedding a watermark.
Today's network is distributed and heterogeneous in nature and has numerous applications which affect day to day life.
Today's Smartphone operating systems frequently fail to provide users with adequate control and visibility into how the third-party applications use their private data.
 With TaintDroid realized on Android system.
Topology recognition and leader election are fundamental tasks in distributed computing in networks.
 The first of them requires each node to find a labeled isomorphic copy of the network.
Traceability supports various activities of the software development process.
Tracing allows the analysis of task interactions with each other and with the operating system.
 Locating performance problems in a trace is not trivial because of their large size.
 Furthermore.
Tracking accuracy plays a critical role in V2V (vehicle-to-vehicle communications) based active safety applications.
 This paper proposed a beacon rate control algorithm to reduce the tracking error.
 By analyzing the vehicle trajectory tracking process.
Traditional approaches for managing enterprise data revolve around a batch driven Extract Transform Load process.
Traditional black-box optimization searches a set of potential solutions for those optimizing the value of a function whose analytical or algebraic form is unknown or inexistent.
Traditional control charts.
Traditional digital computing demands perfectly reliable memory and processing.
Traditional examination by the teacher to choose.
Traditional fluid-structure interaction techniques are based on arbitrary Lagrangian-Eulerian method.
Traditional forensic analysis of hard disks and external media typically involves a dead analysis of a powered down machine.
 Forensic acquisition of hard drives and external media has traditionally been accomplished by one of several means: standalone forensic duplicator; using a hardware write-blocker or dock attached to a laptop.
Traditional k out of n visual cryptography scheme has been proposed to encrypt single secret image into n shares where only k or more shares can decode the secret image.
 Many existing schemes on visual cryptography are restricted to consider only binary images as secret which are not appropriate for many important applications.
 Store-and-Forward telemedicine is one such application where medical images are transmitted from one site to another via electronic medium to analyze the patient's clinical health status.
 The main objective of Store-and-Forward telemedicine is to provide remote clinical services via two-way communication between the patient and the healthcare provider using electronic medical image.
Traditional methods for monitoring the environmental factors of a greenhouse and the growth of Phalaenopsis orchids often suffer from low spatiotemporal resolution.
Traditional missile guidance law is difficult to adapt to the complex operational environment.
Traditional network architecture is inflexible and complex.
 This observation has led to a paradigm shift toward software-defined networks (SDNs).
Traditional PC based operating systems load most of its components during the boot process along with the kernel.
 This mechanism though effective for a broader objective.
Traditionally.
Traditionally.
Traditionally.
Traffic classification.
Traffic jam is one of the hardest problems of the crowded cities.
Traffic sign recognition (TSR) is an integrated part of driver assistance systems and it remains an active research topic in computer vision today.
 This paper proposes a solution for TSR problem which composed of robust traffic sign image descriptor and sparse classifiers.
 Specifically.
Traffic surveillance has become an important topic in intelligent transportation systems (ITSs).
Train operation adjustment is a kind of real-time multi-objective large scale combination optimization problem with multi-restriction.
 A novel immune genetic algorithm based on artificial immune algorithm and genetic algorithm is put forward.
 Then scheduling optimization model for two-way marshalling train is proposed.
Train Rolling stock examination involves visual observation of the moving train around 30Kmph to find defective bogie parts.
 A train coach moves on a couple of bogies consisting of wheels.
Training anatomic and clinical pathology residents in the principles of bioinformatics is a challenging endeavor.
 Most residents receive little to no formal exposure to bioinformatics during medical education.
Training image-based geostatistical methods are increasingly popular in groundwater hydrology even if existing algorithms present limitations that often make real-world applications difficult.
 These limitations include a computational cost that can be prohibitive for high-resolution 3-D applications.
Transfinite barycentric kernels are the continuous version of traditional barycentric coordinates and are used to define interpolants of values given on a smooth planar contour.
 When the data is two-dimensional.
Transglutaminases are calcium-dependent enzymes that catalyze the formation of epsilon-(gamma-glutamyl)lysine isopeptide bonds between specific glutamine and lysine residues.
 Some transglutaminase isoforms are present in the brain and are thought to participate in the protein aggregation characteristic of neurological diseases such as Huntington.
Transient faults in safety-critical computer-based systems represent a major issue for guaranteeing correct system behaviour.
 Fault injection is a commonly used method to evaluate the sensitivity of such systems.
 This paper presents a fault injection tool.
Transition of the precision engineering and instrumentation to the widespread use of nanoscale structures and thin layers requires improved localization methods for measuring the depth of the material.
 Unified standards and the generally accepted methods for measuring the wear resistance and friction coefficient are not currently available.
 The aim of this work was the development of a universal friction machine with the simplified requirements for the preparation and the geometric shape of the sample and the opposing disc.
 An important requirement for the equipment and the method of measurement is the ability to measure the friction coefficient and the determination of the wear resistance of coatings and hardened layers of micron and submicron thicknesses.
 Another important requirement is modeling in the experiment of acyclic friction process.
Transposable elements (TEs) constitute the most dynamic and the largest component of large plant genomes: for example.
Trees are a fundamental structure in algorithmics.
 In this paper.
Triangle meshes are the most common representation of an object in the field of computer graphics.
Trichloroethylene (TCE) and other solvents are found as groundwater contaminants in industrial complexes.
 Pump-andtreat (PAT) systems are commonly selected for remediation.
Tries and patricia tries are two popular data structures for storing strings.
 Let H-n denote the height of the trie (the patricia trie.
Tripartite motif containing 28 (TRIM28) is a transcriptional regulator acting as an essential corepressor for Kruppel-associated box zinc finger domain-containing proteins in multiple tissue and cell types.
 An increasing number of studies have investigated the function of TRIM28; however.
Trust is one of the most challenging issues in the emerging cloud computing era.
 Over the past few years.
Trust is the common factor of any network security.
Trust management has become an emerging security paradigm in various areas such as ad hoc networks and cloud computing.
 One core element of trust management is the indirect trust model that evaluates the trustworthiness of a target based on others' recommendations.
 The research on indirect trust is still at an early stage.
Tuberculosis now exceeds HIV as the top infectious disease cause of mortality.
Tutorials and code puzzles are commonly used in today's novice programming environments to introduce computer programming to children.
 While research has explored the effectiveness of each instructional format at teaching different kinds of information independently.
Tutte's embedding is one of the most popular approaches for computing parameterizations of surface meshes in computer graphics and geometry processing.
 Its popularity can be attributed to its simplicity.
Twitter has grown significantly in the past several years and provides a new vector for data collection.
Twitter spam has become a critical problem nowadays.
 Recent works focus on applying machine learning techniques for Twitter spam detection.
Two algorithms which allow one to take an uneven road surface into account in the vehicle dynamics analysis are presented in the article.
 Their essence is to determine the position of the contact point of the tire model with the uneven road surface.
 According to the concept of the authors.
Two classes of rational solutions to a shallow water wave-like non-linear differential equation are constructed.
 The basic object is a generalized bilinear differential equation based on a prime number.
Two current trends in the real-time and embedded systems are the multiprocessor architectures and the partitioning technology that enables several isolated applications with different criticality levels to share the same computer.
 This paper presents a real-time platform for multiprocessor and partitioned systems.
Two dimensional (2D) vector features.
Two imaginary skew rack cutter curves with stepped triple circular-arc teeth are presented in this paper.
 A helical gear pair that includes the gear and pinion surfaces was generated by using two matched imaginary skew rack cutter surfaces.
 The mathematical models of the helical gear pair with stepped triple circular-arc teeth were developed based on theory of gearing.
 With the use of these models.
Two key factors in dam-break modeling are accuracy and speed.
Two new protocols for quantum binary voting are proposed.
 One of the proposed protocols is designed using a standard scheme for controlled deterministic secure quantum communication (CDSQC).
Two players wishing to communicate are placed each in a room with N telephones connecting the two rooms.
 The players do not know how the telephones are interconnected.
 In each round.
Two purposes of this study are 1) to select a data mining model to predict learners' academic performance in computer programming subject to group learners for cooperative learning by comparing the efficiency of the models created from data mining with classification technique and 2) to develop a model for cooperative learning via web using the selected data mining model to group learners.
 The efficiency of seven models created from data mining with classification technique by using seven algorithms that are Artificial Neural Network.
Two ring signature schemes over number theory research unit (NTRU) lattices are presented.
 The first scheme constructed in the random oracle model is an extension of Ducas.
Type 2 diabetes mellitus (T2DM) is characterized by islet beta-cell dysfunction and insulin resistance.
Typical Internet of Things (IoT) applications involve collecting information automatically from diverse geographically-distributed smart sensors and concentrating the information into more powerful computers.
 The Raspberry Pi platform has become a very interesting choice for IoT applications for several reasons: (1) good computing power/cost ratio; (2) high availability; it has become a de facto hardware standard; and (3) ease of use; it is based on operating systems with a big community of users.
 In IoT applications.
Typing and classification of Escherichia coli (E.
 coli) according to cell wall components.
UDP-based DNS packet is a perfect tool for hackers to launch a well-known type of distributed denial of service (DDoS).
 The purpose of this attack is to saturate the DNS server availability and resources.
 This type of attack usually utilizes a large number of botnet and perform spoofing on the IP address of the targeted victim.
Ultra dense networks are a promising technology enabling high power and spectrum efficiencies in future wireless systems.
 It is well-known that for ultra dense networks inter-cell interference is one of the main bottlenecks prohibiting achieving the promised performance gains.
 In order to effectively coordinate or mitigate interference in paper.
Ultraviolet spectrophotometry has been widely applied in determination of water quality parameters because of its advantagous properties compared to chemical method.
UML has become a de-facto standard for design and development of object-oriented systems.
 On the other hand.
Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations.
 It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes.
 This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory).
Under consideration in this paper is a Volterra lattice system.
 Through symbolic computation.
Under investigation in this article is a (2+1)-dimensional generalised variable-coefficient shallow water wave equation.
Under investigation in this article is a generalised nonlinear Schrodinger-Maxwell-Bloch system for the picosecond optical pulse propagation in an inhomogeneous erbium-doped silica optical fibre.
Under investigation in this letter is a (2+1)-dimensional generalized breaking soliton equation.
Under investigation in this paper are the (1+1)-dimensional and (2+1)-dimensional Ito equations.
 With the help of the Bell polynomials method.
Under investigation in this paper are the coupled higher-order nonlinear Schrodinger equations with variable coefficients.
Under investigation in this paper is a (2 + 1)-dimensional nonlinear Schrodinger equation in the Heisenberg ferromagnetic spin chain.
 Via the symbolic computation and Hirota method.
Under investigation in this paper is a (2+1)-dimensional Broer-Kaup-Kupershmidt system for the nonlinear and dispersive long gravity waves on two horizontal directions in the shallow water of uniform depth.
 Bilinear forms.
Under investigation in this paper is a (2+1)-dimensional Gross-Pitaevskii equation with time-varying trapping potential.
Under investigation in this paper is a (2+1)-dimensional nonlinear evolution equation generated via the Jaulent-Miodek hierarchy for nonlinear water waves.
 With the aid of binary Bell polynomials and symbolic computation.
Under investigation in this paper is a (3 + 1)-dimensional generalized nonlinear Schrodinger equation with the distributed coefficients for the spatiotemporal optical soli tons or light bullets.
 Through the symbolic computation and Hirota method.
Under investigation in this paper is a (3 + 1)-dimensional modified Korteweg-de Vries-Zakharov-Kuznetsov (KdV-ZK) equation.
Under investigation in this paper is a (3 + 1)-dimensional variable-coefficient Kadomtsev-Petviashvili equation.
Under investigation in this paper is a coherently coupled nonlinear Schrodinger system which describes the propagation of polarized optical waves in an isotropic medium.
 By virtue of the Darboux transformation.
Under investigation in this paper is a cubic-quintic nonlinear Schrodinger equation which can describe the propagation of ultrashort pulses in an inhomogeneous optical fibre.
 Lax pair and conservation laws are constructed from which the integrability of the equation can be verified.
 Through a gauge transformation.
Under investigation in this paper is a -dimensional Korteweg-de Vries-type equation.
Under investigation in this paper is a -dimensional variable-coefficient generalized shallow water wave equation.
 Bilinear forms.
Under investigation in this paper is a discrete (2+1)-dimensional Ablowitz-Ladik equation.
Under investigation in this paper is a discrete integrable Ablowitz-Ladik equation.
Under investigation in this paper is a fifth-order nonlinear Schrodinger equation.
Under investigation in this paper is a fourth-order variable-coefficient nonlinear Schrodinger equation.
Under investigation in this paper is a generalized (2 + 1)-dimensional coupled Burger equation with variable coefficients.
Under investigation in this paper is a generalized variable-coefficient fifth-order Kortewegde Vries equation.
Under investigation in this paper is a higher-order nonlinear Schrodinger equation in an optical fiber.
 Lax pair and infinitely-many conservation laws are derived via the symbolic computation.
 By virtue of the Darboux transformation.
Under investigation in this paper is a higher-order nonlinear Schrodinger equation which can describe the propagation of ultrashort pulse in the inhomogeneous optical fiber.
 Lax pair and conservation laws are constructed from which the integrability of the equation can be verified.
 Nonautonomous breathers and rogue waves for the equation are derived based on the Darboux transformation (DT) and generalized DT.
Under investigation in this paper is a Kadomtsev-Petviashvili-Schrodinger system.
Under investigation in this paper is a nonautonomous Kadomtsev-Petviashvili (KP) equation in fluids and plasmas.
 The integrability of this equation is examined via the Painleve analysis and its multi-soliton solutions are constructed.
 A constraint is proposed to ensure the existence of parabola solitons for such KP equation.
 Based on the constructed solutions.
Under investigation in this paper is a nonisospectral and variable-coefficient fifth order Korteweg-de Vries equation in fluids.
 By virtue of the Bell polynomials and symbolic computation.
Under investigation in this paper is a set of the (2 + 1)-dimensional coupled nonlinear Schrodinger equations.
Under investigation in this paper is a set of the (2+1)/dimensional nonlinear Schrodinger-Maxwell-Bloch (NLS-MB) equations.
Under investigation in this paper is a three-dimensional Gross-Pitaevskii equation with the distributed time-dependent coefficients.
Under investigation in this paper is an extended Korteweg-de Vries equation.
 Via Bell polynomial approach and symbolic computation.
Under investigation in this paper is an inhomogeneous higher-order nonlinear Schrodinger equation.
Under investigation in this paper is an inhomogeneous modified nonlinear Schrdinger (MNLS) equation describing the ultrashort pulse dynamics with the distributed dispersion.
Under investigation in this paper is an inhomogeneous nonlinear Schrodinger equation.
Under investigation in this paper is an inhomogeneous nonlinear Schrodinger-Maxwell-Bloch system with variable dispersion.
Under investigation in this paper is an integro-differential nonlinear Schrobinger (IDNLS) equation.
Under investigation in this paper is the (3 + 1)-dimensional coupled nonlinear Schrodinger system for an optical fiber with birefringence.
 With the Hirota method.
Under investigation in this paper is the canonical AB system.
Under investigation in this paper is the N-coupled generalized nonlinear Schrodinger (N-CGNLS) equations with cubic-quintic nonlinearity.
Under investigation in this paper is the N-coupled Hirota system which describes the ultrashort pulse simultaneous propagation of the N-field components in an optical fiber.
 Through the Hirota method and symbolic computation.
Under investigation in this paper is the propagation and interaction of the solitons formed by the incoherently interacting optical beams in the bulk Kerr and saturable media in nonlinear optical fibers.
Understanding basic concepts of electronics and computer programming allows researchers to get the most out of the equipment found in their laboratories.
 Although a number of platforms have been specifically designed for the general public and are supported by a vast array of on-line tutorials.
Understanding beliefs.
Understanding continuous human actions is a non-trivial but important problem in computer vision.
 Although there exists a large corpus of work in the recognition of action sequences.
Understanding how members of a research team cooperate and identifying possible synergies may be crucial for organizational success.
 Using data-driven approaches.
Understanding how to enhance cooperation and coordination in distributed.
Understanding intent and relevance of surrounding agents from video is an essential task for many applications in robotics and computer vision.
 The modeling and evaluation of contextual.
Understanding of images via features like edges plays a vital role in many image processing applications.
Underwater exploration has become an active research area over the past few decades.
 The image enhancement is one of the challenges for those computer vision based underwater researches because of the degradation of the images in the underwater environment.
 The scattering and absorption are the main causes in the underwater environment to make the images decrease their visibility.
Unlike conventional taught learning.
Unmanned aerial vehicles (UAVs) provide a flexible and low-cost solution for the acquisition of high-resolution data.
 The potential of high-resolution UAV imagery to create and update cadastral maps is being increasingly investigated.
 Existing procedures generally involve substantial fieldwork and many manual processes.
Unmanned aerial vehicles have become more widely used for entertainment.
Untargeted metabolomic studies generate information-rich.
Until now hiding methods in network steganography have been described in arbitrary ways.
Urbanisation represents a growing threat to natural communities across the globe.
 Small aquatic habitats such as ponds are especially vulnerable and are often poorly protected by legislation.
 Many ponds are threatened by development and pollution from the surrounding landscape.
Use cases have been widely adopted by software engineers to model user-system interaction.
Use of cloud Database-as-a-Service (DaaS) is gradually increasing in private and government organizations.
 Organizations are now considering outsourcing of their local databases to cloud database servers to minimize their operational and maintenance expenses.
 At the same time.
User authentication in wireless sensor network (WSN) plays a very important role in which a legal registered user is allowed to access the real-time sensing information from the sensor nodes inside WSN.
 To allow such access.
User authentication is crucial in security systems.
User evaluations have gained increasing importance in visualization research over the past years.
User profiles in collaborative filtering (CF) recommendation technique are built based on ratings given by users on a set of items.
 The most eminent shortcoming of the CF technique is the sparsity problem.
 This problem refers to the low ratio of rated items by users to the total number of available items; hence the quality of recommendation will be affected.
 Most researchers use implicit data as a solution for sparsity problem.
Users of heterogeneous computing systems face two problems: first.
Using a computerized symbolic computation technique based on improved Jacobi elliptic function method.
Using a total of 297 segmented sections.
Using dedicated stereo camera systems and structured light is a well-known method for measuring the 3D shape of large surfaces.
 However the problem is not trivial when high accuracy.
Using digital elevation models (DEMs).
Using discrete GPUs for processing very large datasets is challenging.
Using distributed task allocation methods for cooperating multivehicle systems is becoming increasingly attractive.
Using general identities for difference operators.
Using GPUs as general-purpose processors has revolutionized parallel computing by providing.
Using image processing to extract nodular or linear shadows is a key technique of computer-aided diagnosis schemes.
 This study proposes a new method for extracting nodular and linear patterns of various sizes in medical images.
 We have developed a morphology filter bank that creates multiresolution representations of an image.
 Analysis bank of this filter bank produces nodular and linear patterns at each resolution level.
 Synthesis bank can then be used to perfectly reconstruct the original image from these decomposed patterns.
 Our proposed method shows better performance based on a quantitative evaluation using a synthesized image compared with a conventional method based on a Hessian matrix.
Using machine-learning methodologies to analyze EEG signals becomes increasingly attractive for recognizing human emotions because of the objectivity of physiological data and the capability of the learning principles on modeling emotion classifiers from heterogeneous features.
Using real time eye tracking.
Using very high resolution remote sensing images to extracting urban features from very high resolution remote sensing images is a very complex and difficult task.
 The improvement in geospatial technologies brought forward many solutions that can help in improving the process of urban feature extraction.
 Data collection using light detection and ranging (LiDAR) and capturing very high resolution optical images concurrently is one of these solutions.
 This research proves that the fusion of high-resolution optical image with LiDAR data can improve image processing results.
 It is based on increasing urban features extraction success rate by reducing oversegmentation.
 The fusion process relies first on wavelet transform techniques.
Utilization of cloud technologies has recently attained a profile in building information modeling (BIM).
 Many studies have investigated the potential role of cloud in facilitating the use of BIM in the construction domain.
 In addition.
Utilizing complex dynamics of chaotic maps and systems in encryption was studied comprehensively in the pas two and a half decades.
Variable-coefficient nonlinear Schrodinger (NLS)-type models are used to describe certain phenomena in plasma physics.
Varied spatial resolution of isochromatic fringes over the domain influences the accuracy of fringe order estimation using TFP/RGB photoelasticity.
 This has been brought out in the first part of the work.
 The existing scanning schemes do not take this into account.
Vario-scale data structures have been designed to support gradual content zoom and the progressive transfer of vector data.
Various algorithms for optimal control require the explicit determination of switching surfaces.
Various applications of global surface parametrization benefit from the alignment of parametrization isolines with principal curvature directions.
 This is particularly true for recent parametrization-based meshing approaches.
Various applications.
Various model selection methods can be applied to seek sparse subsets of the covariates to explain the response of interest in bioinformatics.
 While such methods often offer very helpful predictive performances.
Various online systems offer a lightweight process for creating accounts (e.
Various security devices which produce a large volume of logs and alerts have been used widely.
 It is such a troublesome and time-consuming task for network managers to analyze and deal with the information.
 This paper presented an improved alerts aggregation method based on grey correlation and attribute similarity method.
 We used grey correlation to ascertain the importance of alert attributes in network security.
Vascular Ehlers-Danlos syndrome (vEDS) is a rare and severe connective tissue disorder caused by mutations in the collagen type III alpha I chain (COL3A1) gene.
 We describe a pathogenetic heterozygous COL3A1 mutation c.
VBF is a collection of C++ classes designed for analyzing vector Boolean functions (functions that map a Boolean vector to another Boolean vector) from a cryptographic perspective.
 This implementation uses the NTL library from Victor Shoup.
Vector and raster are two types of spatial data structures used in a geographic information system (GIS).
 With the development of GIS and remote sensing (RS) technologies.
Vector instructions of modern CPUs are crucially important for the performance of compute-intensive algorithms.
 Auto-vectorisation often fails because of an unfortunate choice of data layout by the programmer.
 This paper proposes a data layout inference for auto-vectorisation that identifies layout transformations that convert single instruction.
Vehicle anti-collision warning system is a key research area of vehicle safety.
 It is also a necessary means to enhance driving safety and reduce traffc accidents.
 The visual location method is presented for this system based on license plate image.
 This method realizes visual positioning with the help of the mounting points of license plate.
 At the same time.
Vehicular network has been recently used to achieve high efficient and flexible traffic scheduling at intersection roads for smart transportation systems.
 Different from existing works.
Vendor lock-in is a major barrier to the adoption of cloud computing.
Venomous animals have developed a huge arsenal of reticulated peptides for defense and predation.
 Based on various scaffolds.
Verification of an ECDSA signature requires a double scalar multiplication on an elliptic curve.
 In this work.
Verizon's Data Breach Investigations Report states that local area network (LAN) access is the top vector for insider threats and misuses.
 In Ethernet.
Very large nonlinear unconstrained binary optimization problems arise in a broad array of applications.
 Several exact or heuristic techniques have proved quite successful for solving many of these problems when the objective function is a quadratic polynomial.
Very little is known about how individual soil particles move over a soil surface as a result of rainfall.
 Specifically there is virtually no information about the pathway a particle takes.
Vessel segmentation is a critical task for various medical applications.
Vibrations with unknown and/or time-varying frequencies significantly affect the achievable performance of control systems.
Video content stored in Video Event Data Recorders (VEDRs) are used as important evidence when certain events such as vehicle collisions occur.
Video games have grown in popularity since the 1980's.
 The largest consumers of video games are youth populations.
 Previous research has shown cognitive development and learning principles in video games.
 As a result.
Video object tracking represents a very important computer vision domain.
 In this paper.
Video on demand (VoD) is a popular application on the Internet.
 In the past few years.
Video surveillance and biomedical research have received a great attention in most of the active application-oriented research areas of computer vision.
Video tracking is a main field of computer vision.
Video tracking simulation platform is very important for algorithm developer.
 In order to improve the efficiency of video tracking algorithm.
Video visualization (VV) is considered to be an essential part of multimedia visual analytics.
 Many challenges have arisen from the enormous video content of cameras which can be solved with the help of data analytics and hence gaining importance.
Video-oculography (VOG) is a tool providing diagnostic information about the progress of the diseases that cause regression of the vergence eye movements.
Views materialisation is well known in the context of relational databases.
Viral infection starts with a virus particle landing on a cell surface followed by penetration of the plasma membrane.
 Due to the difficulty of measuring the rapid motion of small-sized virus particles on the membrane.
Viral myocarditis is a common cardiovascular disease.
Virtual blended learning as the use of virtual 3D environments in education has already delivered utility in a number of cases.
 This article presents a concept and a prototype of a 3D environment used in the current semester to employ the new medium in programming education at universities.
 The concept brings gamification aspects as well as interaction mechanisms from social media to the virtual 3D environment.
 It thereby aims to reduce the large number of student dropouts due to failing the programming modules by motivating more and less capable students alike to an increased participation in the practical exercises.
 It is concluded that the concept is applicable in a much broader spectrum of exercise and tutorial settings.
Virtual clothes try-on has recently come into the researchers' focus owing to its commercial potential.
 It can be used as an efficient tool in online shopping.
Virtual desktop infrastructure (VDI) solutions seek to provide a satisfactory user experience at the client side when accessing remote desktop applications.
Virtual power plant (VPP) concept was developed to integrate distributed energy resources (DERs) into the grid in order that they are seen as a single power plant by the market and power system operator.
Virtual reality (VR) started about 50 years ago in a form we would recognize today [stereo head-mounted display (HMD).
Virtual reality (VR) surgical training and presurgical planning require the creation of 3D virtual models of patient anatomy from medical scans (e.
 CT or MRI).
 Real-time head tracking in VR applications allows users to navigate in the virtual anatomy from any 3D position and orientation.
 The process of interactively rendering highly detailed 3D volumetric data of anatomical models from a dynamically changing observer's perspective is extremely demanding for computational resources.
 We propose a parallel computing solution to this problem.
Virtual reality (VR)/ augmented reality (AR) applications allow users to view artificial content of a surrounding space simulating presence effect with a help of special applications or devices.
 Synthetic contents production is well known process form computer graphics domain and pipeline has been already fixed in the industry.
 However emerging multimedia formats for immersive entertainment applications such as free-viewpoint television (FTV) or spherical panoramic video require different approaches in content management and quality assessment.
 The international standardization on FTV has been promoted by MPEG.
 This paper is dedicated to discussion of immersive media distribution format and quality estimation process.
 Accuracy and reliability of the proposed objective quality estimation method had been verified with spherical panoramic images demonstrating good correlation results with subjective quality estimation held by a group of experts.
Virtual Reality is reaching into the consumer space.
 Mobile virtual reality solutions are nowadays widely available and affordable for many smartphones.
Virtual Reality is used in fields of cognitive sciences to study participants' reactions.
 In such cases.
Virtual reality technology is a kind of cross-integration computer technology which is developed on the basis of multiple disciplines.
Virtual screening.
Visible light positioning (VLP) is widely believed to be a cost-effective answer to the growing demand for real-time indoor positioning.
Vision-based object detection is essential for a multitude of robotic applications.
Visual animal biometrics is an emerging research discipline in computer vision.
Visual Cryptography (VC) has been developed as a significant research arena in media security.
 Despite of its obvious strengths.
Visual cryptography (VC) is a variant form of secret sharing.
 In general threshold setting.
Visual Cryptography Scheme (VCS) is a cryptographic technique which can hide image based secrets.
 Even though VCS has the major advantage that the decoding can be done with the help of Human Visual System (HVS).
Visual data classification.
Visual feature learning.
Visual object tracking technology is one of the key issues in computer vision.
 In this paper.
Visual perception is a fundamental component for most robotics systems operating in human environments.
 Specifically.
Visual presentation of the outputs from marine mammal detectors is key for efficient mining of large acoustic datasets.
 Operators must be able to easily navigate through large time series of detections.
Visual tracking is a challenging computer vision task due to the significant observation changes of the target.
 By contrast.
Visual tracking is a fundamental research topic in computer vision community.
Visual tracking is a popular research area in computer vision.
Visual tracking.
Visualization applications can be performed on distinct platforms.
Visualization workflows are important services for expert users to analyze watersheds when using our HydroTerre end-to-end workflows.
 Analysis is an interactive and iterative process and we demonstrate that the expert user can focus on model results.
Visualizing sets of isosurfaces from 3D scalar ensemble fields is a difficult task due to inherent occlusion effects.
Visualizing very large graphs by edge bundling is a promising method.
VM-based inspection tools generally implement probes in the hypervisor to monitor events and the state of kernel of the guest system.
 The most important function of a probe is to carve information of interest out of the memory of the guest when it is triggered.
 Implementing probes for a closed-source OS demands manually reverse-engineering the undocumented code/data structures in the kernel binary image.
 Furthermore.
Voltage scaling is a fundamental technique in the energy efficient computing field.
 Recent studies tackling this topic show degraded system reliability as frequency scales.
 To address this conflict.
Volume rendering has been a relevant topic in scientific visualization for the last decades.
Volunteer computing is a type of distributed computing in which ordinary people donate processing and storage resources to scientific projects.
 BOINC is the main middleware system for this type of computing.
 The aim of volunteer computing is that organizations be able to attain large computing power thanks to the participation of volunteer clients instead of a high investment in infrastructure.
 There are projects.
V-order is a global order on strings related to Unique Maximal Factorization Families (UMFFs).
Vote by ballot is the feature in a democratic society and the process of decision-making.
VR is a integrated technology.
Water distribution networks are vulnerable to various contamination events that may be accidental or purposeful.
 Sensors are required for online monitoring of water quality to safeguard human health.
 Since sensors are costly.
Water quality planning is complicated itself but further challenged by the existence of uncertainties and nonlinearities in terms of model formulation and solution.
 In this study.
Watermarking protocols are designed for tracing illegal distributors when unauthorized copies are found.
Wave theories of heating of the chromosphere.
Wconfirms the accuracy and robustness of the proposed FFT-BP model.
 The root-mean-square errors of FFT-BP blind predictions of a benchmark set of 100 complexes.
We address a Boundary Integral Equation (BIE) approach for the analysis of gas dissipation in near-vacuum for Micro Electro Mechanical Systems (MEMS).
 Inspired by an analogy with the radiosity equation in computer graphics.
We address the entanglement dynamics of a three-qubit system interacting with a classical fluctuating environment described either by a Gaussian or non-Gaussian noise in three different configurations namely: common.
We address the issue of large scale network security.
 It is known that traditional game theory becomes intractable when considering a large number of players.
We address the particular cyber attack technique known as stack buffer overflow in GNU/Linux operating systems.
We aimed to implement the Extension for Community Healthcare Outcomes (ECHO) tele-mentoring model for hepatitis C and to evaluate its outcomes in the health providers.
 Following the ECHO model.
We analytically study the Schrodinger type nonlinear evolution equations by improved tan(Phi(xi)/2)-expansion method.
 Explicit solutions are derived.
We analyze controllability properties for a class of bilinear interconnected systems.
We analyze the security of the two-way continuous-variable quantum key distribution protocol in reverse reconciliation against general two-mode attacks.
We analyze the state of development practices for Mesh Generation and Mesh Processing (MGMP) software by comparing 27 MGMP projects.
 The analysis employs a reproducible method based on a grading template of 56 questions covering 13 software qualities.
 The software is ranked using the Analytic Hierarchy Process (AHP).
We applied computer-based text analyses of regressive imagery to verbal protocols of individuals engaged in creative problem-solving in two domains: visual art (23 experts.
We are developing a programming education support tool pgtracer utilizing fill-in-the-blank questions.
 Pgtracer operates on Moodle.
We are developing a unified distributed communication environment for processing of spatial data which integrates web-.
We are developing a Web-based programming education support tool pgtracer as a plug-in of well-known Learning Management System Moodle.
 Pgtracer provides fill-in-the-blank questions composed of a C++ program and a trace table to students.
 When a student answers a question by filling the blanks.
We are developing electronic textbook for of basic chemistry-experiment for university students in which reaction mechanisms are shown by computer graphics (CG).
 The CGs of chemical reactions was made based on the empirical molecular orbital calculations.
 The CGs include following reactions as a model of Walden's inversion where drastic change in structure takes place.
We are immerse in a world that becomes more and more mobile every day.
We are on the cusp of the emergence of a new wave of nonvolatile memory technologies that are projected to become the dominant type of main memory in the near future.
 A key property of these new memory technologies is their asymmetric read-write costs: Writes can be an order of magnitude or more higher energy.
We can expect the number of Internet of Things (IoT) devices to rapidly increase.
We complete the computation of the integral homology of the generalized braid group B associated to an arbitrary irreducible complex reflection group W of exceptional type.
 In order to do this we explicitly computed the recursively-defined differential of a resolution of Z as a ZB-module.
We consider a network of agents whose objective is for the aggregate of their states to converge to a solution of a linear program in standard form.
 Each agent has limited information about the problem data and can communicate with other agents at discrete time instants of their choosing.
 Our main contribution is the synthesis of a distributed dynamics and a set of state-based rules.
We consider a new type of attack on a coherent quantum key distribution protocol [coherent one-way (COW) protocol].
 The main idea of the attack consists in measuring individually the intercepted states and sending the rest of them unchanged.
 We have calculated the optimum values of the attack parameters for an arbitrary length of a communication channel and compared this novel attack with a standard beam-splitting attack.
We consider a variable metric linesearch based proximal gradient method for the minimization of the sum of a smooth.
We consider d-dimensional lattice path models restricted to the first orthant whose defining step sets exhibit reflective symmetry across every axis.
 Given such a model.
We consider dynamic group services.
We consider here a natural spline interpolation based on reduced data and the so-called exponential parameterization (depending on parameter lambda is an element of [0.
We consider how to privately determine whether a private set owned by Bob is a subset of another private set owned by Alice.
 This problem has many applications in online collaboration.
 We first propose an encoding method to encode a set to a vector.
We consider one applied global optimization problem where a set of feasible solutions is discrete and very large.
 The goal is to find optimal perfect gratings.
We consider optimal distributed computation of a given function of distributed data.
 The input (data) nodes and the sink node that receives the function form a connected network that is described by an undirected weighted network graph.
 The algorithm to compute the given function is described by a weighted directed acyclic graph and is called the computation graph.
 An embedding defines the computation communication sequence that obtains the function at the sink.
 Two kinds of optimal embeddings are sought.
We consider the generalized fifth-order KdV type nonlinear evolution equation with variable coefficients.
 The system technique has been applied rigorously in order to find new exact solutions of the considered equations.
 The closed-form solutions of the fifth-order KdV type nonlinear evolution equation are expressed by the proposed ansatz in terms of exponential functions.
 We believe that the system technique is effective and stable of finding the exact solutions of nonlinear evolution equations.
We consider the incompressible homogeneous Navier-Stokes (NS) equations on a torus (typically.
We consider the integrable generalization of the nonlinear Schrodinger equation that arises as a model for nonlinear pulse propagation in monomode optical fibers.
 The existent conditions for its modulational instability to form the rogue waves is given from its plane-wave solutions.
 We propose a generalized (n.
We consider the problem of automatically re-identifying a person of interest seen in a probe camera view among several candidate people in a gallery camera view.
 This problem.
We consider the problem of efficient content delivery over networks in which individual nodes are equipped with content caching capabilities.
 We present a flexible methodology for the design of cooperative.
We consider the problem of finding meaningful correspondences between 3D models that are related but not necessarily very similar.
 When the shapes are quite different.
We consider the problem of group testing with sum observations and noiseless answers.
We consider the problem of how to design and implement communication-efficient versions of parallel kernel support vector machines.
We consider the problem of minimizing a linear function over an affine section of the cone of positive semidefinite matrices.
We consider the problem of performing predecessor searches in a bounded universe while achieving query times that depend on the distribution of queries.
 We obtain several data structures with various properties: in particular.
We consider the problem of sampling points from a collection of smooth curves in the plane.
We consider the problem of solving large sparse linear systems where the coefficient matrix is possibly singular but the equations are consistent.
 Block two-stage methods in which the inner iterations are performed using alternating methods are studied.
 These methods are ideal for parallel processing and provide a very general setting to study parallel block methods including overlapping.
 Convergence properties of these methods are established when the matrix in question is either M-matrix or symmetric matrix.
 Different parallel versions of these methods and implementation strategies.
We consider the problems of computing the maximal and the minimal non-empty suffixes of substrings of a longer text of length n.
 For the minimal suffix problem we show that for every tau.
We consider the two-dimensional range maximum query (2D-RMQ) problem: given an array containing elements from an ordered set.
We consider the well known distributed setting of computational mobile entities.
We consider two new matrix spectral problems.
We considered the process of density wave propagation in the logistic equation with diffusion.
We construct team-optimal estimation algorithms over distributed networks for state estimation in the finite-horizon mean-square error (MSE) sense.
We construct the differential invariants of Lie symmetry pseudogroups of the (2+1)-dimensional breaking soliton equation and analyze the structure of the induced differential invariant algebra.
 Their syzygies and recurrence relations are classified.
 In addition.
We deal with data structures and algorithms for bundles of binary images represented by raster data.
 In this paper.
We demonstrate a Model Order Reduction technique for a system of nonlinear equations arising from the Finite Element Method (FEM) discretization of the three-dimensional quasistatic equilibrium equation equipped with a Perzyna viscoplasticity constitutive model.
 The procedure employs the Proper Orthogonal Decomposition-Galerkin (POD-G) in conjunction with the Discrete Empirical Interpolation Method (DEIM).
 For this purpose.
We demonstrate a remote scene viewing system for telepresence purposes.
 The system is based on a pan-tilt stereo camera that captures stereo video and transfers it to a remote user over a network.
 On the user end.
We demonstrate circuits that generate set and integer partitions on a set S of n objects at a rate of one per clock.
 Partitions are ways to group elements of a set together and have been extensively studied by researchers in algorithm design and theory.
 We offer two versions of a hardware set partition generator.
 In the first.
We describe a general framework c2i for generating an invariant inference procedure from an invariant checking procedure.
 Given a checker and a language of possible invariants.
We describe a general framework for adding the values of two approximate counters to produce a new approximate counter value whose expected estimated value is equal to the sum of the expected estimated values of the given approximate counters.
 (To the best of our knowledge.
We describe a new method that interactively extracts salient object surfaces from 3D medical volume data.
 Isosurface extraction has been one of the most popular methods for 3D volume visualization.
We describe a new set of interaction techniques that allow users to interact with physical objects through augmented reality (AR).
 Previously.
We describe a package realized in the Julia programming language which performs symbolic manipulations applied to nonlinear evolution equations.
We describe and evaluate a software-only implementation of a novel mechanism for accessing and streaming GPU-rendered content from the cloud to low-end user devices.
 The unique properties of our implementation enable the trivial cloud-deployment of graphics-intensive applications.
We describe case studies of clinically significant changes in sedentary behavior of older adults captured with a novel computer vision algorithm for depth data.
 An unobtrusive Microsoft Kinect sensor continuously recorded older adults' activity in the primary living spaces of TigerPlace apartments.
 Using the depth data from a period of ten months.
We describe the design and a trial run of an integrated course of instruction in reading.
We describe the evolution of an introductory programming course into an active learning format.
We describe the main features of the developed software tool.
We design an algorithmic framework using matrix exponentials for time-domain simulation of power delivery network (PDN).
 Our framework can reuse factorized matrices to simulate the large-scale linear PDN system with variable step-sizes.
 In contrast.
We detail the design.
We develop a generic programming language for parallel algorithms.
We develop a social group utility maximization (SGUM) framework for cooperative wireless networking that takes into account both social relationships and physical coupling among users.
 Specifically.
We develop a theoretical model of security investments in a network of interconnected agents.
 Network connections introduce the possibility of cascading failures due to an exogenous or endogenous attack depending on the profile of security investments by the agents.
 We provide a tractable decomposition of individual payoffs into an own effect and an externality.
We develop an algorithm for meshing molecular surfaces that is based on patch-wise meshing using an advancing-front method adapted to the particular case of molecular surfaces.
 We focus on the solvent accessible surface (SAS) and the solvent excluded surface (SES).
 The essential ingredient is a newly developed analysis of such surfaces in [18] that allows to describe all SES-singularities a priori and therefore a complete characterization of the SES.
 In addition.
We develop several parallel algorithms for shortest distance queries in planar graphs that use graph partitioning in the preprocessing phase to precompute and store distances between selected pairs of vertices.
 In the query phase.
We developed a fully automated procedure for analyzing data from LED pulses and multilevel bead sets to evaluate backgrounds and photoelectron scales of cytometer fluorescence channels.
 The method improves on previous formulations by fitting a full quadratic model with appropriate weighting and by providing standard errors and peak residuals as well as the fitted parameters themselves.
 Here we describe the details of the methods and procedures involved and present a set of illustrations and test cases that demonstrate the consistency and reliability of the results.
 The automated analysis and fitting procedure is generally quite successful in providing good estimates of the Spe (statistical photoelectron) scales and backgrounds for all the fluorescence channels on instruments with good linearity.
 The precision of the results obtained from LED data is almost always better than that from multilevel bead data.
We developed a novel computer-aided diagnosis (CAD) system that uses feature-ranking and a genetic algorithm to analyze structural magnetic resonance imaging data; using this system.
We devise an efficient algorithm for the symbolic calculation of irreducible angular momentum and spin (LS) eigenspaces within the n-fold antisymmetrized tensor product.
 Lambda V-n(u).
We discuss an architecture and functionality of a multiagent system supporting data integration based on faceted query evaluation in a system of ontology-enhanced graph databases.
 We use an ontology (a subset of OWL 2 RL profile) to support a user in interactive query formulation.
 A set of server agents cooperate one with another in query answering using the ontological knowledge for query rewriting and for deciding about query propagations.
 We propose some algorithms ensuring tractable complexity of query rewriting and query evaluation.
We discuss the dynamics of an inextensible thin Kirchhoff rod used in the modeling of surgical threads.
We discuss the future of massively parallel computing from a fundamental architecture standpoint.
 Our central thesis is that various versions of Moore's Laws will all unavoidably break down over the next two to three decades.
We discuss the way of using keyword search in the case of P2P connected relational databases.
 Each peer is then an agent deciding about translation and propagation of a keyword query.
We discuss two semi-independent calibration techniques used to determine the inflight radiometric calibration for the New Horizons' Multi-spectral Visible Imaging Camera (MVIC).
 The first calibration technique compares the measured number of counts (DN) observed from a number of well calibrated stars to those predicted using the component-level calibration.
 The ratio of these values provides a multiplicative factor that allows a conversation between the preflight calibration to the more accurate inflight one.
We evaluated the underlying causes of differences between latent heat (LE) fluxes measured with two enclosed-path eddy covariance systems (EC) at two measurement levels and independent estimates in an open oak-tree grass savannah over almost one year.
 Estimates of LE of the well-stablished underlying grass by replicated weighable tension-controlled lysimiters (LELye) provided a robust baseline against which to compare EC LE measured at 1.
6 m above ground (LE_1.
 Similarly and at the ecosystem level.
We examine the requirements and the available methods and software to provide (or imitate) uniform random numbers in parallel computing environments.
 In this context.
We explain how to share photons between two distant parties using concatenated entanglement swapping and assess performance according to the two-photon visibility as the figure of merit.
 From this analysis.
We explore the benefits of using online-autotuning to find an optimal configuration for the parallel construction of Surface Area Heuristic (SAH) kD-trees.
 Using a quickly converging autotuning mechanism.
We explored how computer vision techniques can be used to detect engagement while students (N = 22) completed a structured writing activity (draft-feedback-review) similar to activities encountered in educational settings.
 Students provided engagement annotations both concurrently during the writing activity and retrospectively from videos of their faces after the activity.
 We used computer vision techniques to extract three sets of features from videos.
We extended the current density convolution finite-difference time-domain (JEC-FDTD) method to plasma photonic crystals using the Crank-Nicolson -difference scheme and derived the one-dimensional JEC-Crank-Nicolson (CN)-FDTD iterative equation of plasma photonic crystals.
 The method eliminated the Courant-Friedrich-Levy (CFL) stability constraint and became completely unconditional stable form.
 The incomplete Cholesky conjugate gradient (ICCG) algorithm is proposed to solve the equation with a large sparse matrix in the CN-FDTD method as the ICCG method improves the speed of convergence.
We face an increasing need to discover knowledge from data streams in real-time.
 Real-time stream data mining needs a compact data structure to store transactions in the recent sliding-window by one scan.
We give a fully dynamic data structure for maintaining an approximation of the Hausdorff distance between two point sets in a constant dimension d.
We give an optimal-size representation for the elements of the trace zero subgroup of the Picard group of an elliptic or hyperelliptic curve of any genus.
We have been studying and developing a new television system based on a methodology which delivers text-based scripts through the Internet representing visual contents instead of transmitting visual content in a format of video data.
 Such a Text-Generated TV System is achieved by the technology called TVML (TV program Making Language) enabling to create TV-program-like computer graphics animation automatically from script.
 One of the problems in our development is how to establish the scheme of TV program production with TVML to get quality contents like in a real TV broadcast.
 Our approach to the problem is to analyze the real TV program and to mimic it with TVML.
 We first choose a reference TV program then analyze it in many aspects.
 Based on the acquired knowledge.
We have developed a calibration target for use with fluid-immersed endoscopes within the context of the GIFT-Surg (Guided Instrumentation for Fetal Therapy and Surgery) project.
 One of the aims of this project is to engineer novel.
We have developed a process-oriented method for intrusion detection for use on Industrial Control System (ICS) networks.
 Network traffic from an ICS has a much lower volume than that from a typical IT enterprise network.
We have developed a robust sensor for mounting on bridges over rivers and streams.
 These bridge-mounted river stage sensors (BMRSS) make periodic measurements of the distance from the sensor to the water level below.
 Properly interpreted.
We have developed a surgery support system for robot-assisted laparoscopic partial nephrectomy.
 In our system.
We have investigated the femtosecond soliton propagation in inhomogeneous fiber.
We have previously identified a novel intra-tumoral dichotomy in triple-negative breast cancer (TNBC) based on the differential responsiveness to a reporter containing the Sox2 regulatory region-2 (SRR2).
We have proposed a new method of advancing from CS Unplugged through the new process of CS Plugged to full-fledged computer programming languages.
We have proposed an effective machine learning method to analyze multimedia content addressing gesture event detection and recognition.
 Our machine learning method is based on well-studied techniques such that Procrustes Analysis.
We have put into practice the flipped classroom as a way of utilizing e-learning contents in our computer-programming course since 2013.
 The main feature of this practice is to use most of the time usually dedicated to lecture for practicing by assigning the students the e-learning materials as preparation before the class.
 We gave the students homework to learn vocabularies and grammar of the C programming language.
 This decreased the time a teacher spent lecturing and the students were assigned applied-problems to make practical software in addition to conventional basic problems in training.
 Our goal is to maintain the students' motivation to learn the computer programming through the sense of accomplishment that each student obtains by finishing practical assignments in the training.
 We confirmed the effectiveness of this approach by comparing examination scores between last year and this year.
We have witnessed the tremendous momentum of the second spring of parallel computing in recent years.
We identified AOX2 genes in monocot species from Lemnoideae (Spirodela polyrhiza.
We increasingly live in cyber-physical spaces: spaces that are both physical and digital.
We introduce a class of preconditioners for general sparse matrices based on the Birkhoff-von Neumann decomposition of doubly stochastic matrices.
 These preconditioners are aimed primarily at solving challenging linear systems with highly unstructured and indefinite coefficient matrices.
 We present some theoretical results and numerical experiments on linear systems from a variety of applications.
We introduce a framework for simulating the mesoscale self-assembly of block copolymers in arbitrary confined geometries subject to Neumann boundary conditions.
 We employ a hybrid finite difference/volume approach to discretize the mean-field equations on an irregular domain represented implicitly by a level-set function.
 The numerical treatment of the Neumann boundary conditions is sharp.
We introduce a game of trusted computation in which a sensor equipped with limited computing power leverages a central computer to evaluate a specified function over a large dataset.
We introduce a modified and simplified version of the pre-existing fully parallelized three-dimensional Navier-Stokes flow solver known as TPLS.
 We demonstrate how the simplified version can be used as a pedagogical tool for the study of computational fluid dynamics (CFDs) and parallel computing.
 TPLS is at its heart a two-phase flow solver.
We introduce a new approach to the principled design of evolutionary algorithms (EAs) based on kernel methods.
 We demonstrate how kernel functions.
We introduce a new parallel pattern derived from a specific application domain and show how it turns out to have application beyond its domain of origin.
 The pool evolution pattern models the parallel evolution of a population subject to mutations and evolving in such a way that a given fitness function is optimized.
 The pattern has been demonstrated to be suitable for capturing and modeling the parallel patterns underpinning various evolutionary algorithms.
We introduce a new representation of the inverted index that performs faster ranked unions and intersections while using similar space.
 Our index is based on the treap data structure.
We introduce a novel procedure that uses dynamic 3-D computer graphics as a diagnostic tool for assessing disease severity in schizophrenia patients.
We introduce an architecture for undertaking data processing across multiple layers of a distributed computing infrastructure.
We introduce an online anomaly detection algorithm that processes data in a sequential manner.
 At each time.
We introduce from first principles an analysis of the information content of multivariate distributions as information sources.
 Specifically.
We introduce mobile agents for mobile crowdsensing.
 Crowdsensing campaigns are designed through different roles that are implemented as mobile agents.
 The role-based tasks of mobile agents include collecting data.
We introduce NetworKit.
We introduce the quad-kd tree: a general purpose and hierarchical data structure for the storage of multidimensional points.
 Quad-kd trees include point quad trees and kd trees as particular cases and therefore they could constitute a general framework for the study of fundamental properties of trees similar to them.
We introduce transactions into libraries of concurrent data structures; such transactions can be used to ensure atomicity of sequences of data structure operations.
 By focusing on transactional access to a well-defined set of data structure operations.
We investigate a model of a two coupled Ablowitz-Ladik (AL) lattices which admits the integrable AL model.
We investigate implication problems for keys and independence atoms in relational databases.
 For keys and unary independence atoms we show that finite implication is not finitely axiomatizable.
We investigate properties of estimators obtained by minimization of U-processes with the Lasso penalty in the high-dimensional setting.
 Our attention is focused on the ranking problem that is popular in machine learning.
 It is related to guessing the ordering between objects on the basis of their observed predictors.
 We prove the oracle inequality for the excess risk of the considered estimator as well as the bound for the l(1) distance \(theta) over cap-theta*\(1) between the estimator and the oracle.
We investigate semisupervised learning (SL) and pool-based active learning (AL) of a classifier for domains with label-scarce (LS) and unknown categories.
We investigate solitons in optical waveguides and Bose-Einstein condensates (BECs) governed by a (3+1)-dimensional Gross-Pitaevskii system.
We investigate the controllable behavior of nonautonomous soliton in external potentials with variable dispersion and nonlinearity management functions.
We investigate the dynamics of alpha-helical proteins with dipole and quadrupole type molecular excitations.
 Performing the symbolic computation on the iterative algorithm of Darboux transformation.
We investigate the flow dynamics around a rock vane.
We investigate the resonant Davey-Stewartson (DS) system.
 The resonant DS system is a natural (2 +1)-dimensional version of the resonant nonlinear Schrodinger equation.
 Traveling wave solutions were found.
 In this paper.
We investigate the soliton dynamics in tapered parabolic index fibers via symbolic computation for a variety of dispersion profiles to inspect how a specific dispersion profile controls the optical soliton.
 By means of AKNS procedure.
We investigated the soliton solution for N coupled nonlinear Schrodinger (CNLS) equations.
 These equations are coupled due to the cross-phase-modulation (CPM).
 Lax pair of this system is obtained via the Ablowitz-Kaup-Newell-Segur (AKNS) scheme and the corresponding Darboux transformation is constructed to derive the soliton solution.
 One and two soliton solutions are generated.
 Using two soliton solutions of 3 CNLS equation.
We leverage on authenticated data structures to guarantee correctness and completeness of query results over encrypted data.
 Our contribution is in bridging two independent lines of work (searchable encryption.
We live in interesting times - as individuals.
We made use of a special algorithm for compute unified device architecture for NVIDIA graphics cards.
We make use of a kind of distributed semi-quantum computing models to study phase estimation.
 The basic idea is to use distributed micro quantum computers to process respectively a small quantity of quantum states and then communicate with a given classical computer via classical channel to transport the results.
 We study the phase estimation algorithm basing on this idea and provide a distributed semi-quantum algorithm for phase estimation.
 Its time complexity in the first stage will not be worse than the existing quantum algorithm of phase estimation.
We model attacks on a cyberphysical system as a game between two players-the attacker and the system.
 The players may not acquire the complete information about each other.
We observed the open clusters M35 and NGC 2158 during the initial K2 campaign (C0).
 Reducing these data to high-precision photometric timeseries is challenging due to the wide point-spread function (PSF) and the blending of stellar light in such dense regions.
 We developed an image-subtraction-based K2 reduction pipeline that is applicable to both crowded and sparse stellar fields.
 We applied our pipeline to the data-rich C0 K2 super stamp.
We obtain the travelling wave solutions of nonlinear Richard's equation.
We performed a new coupled circuit numerical simulation of eddy currents in an open compact magnetic resonance imaging (MRI) system.
 Following the coupled circuit approach.
We posit that commercial Wi-Fi-based unmanned aerial vehicles (UAV) are vulnerable to common and basic security attacks.
We present a bit-string quantum oblivious transfer protocol based on single-qubit rotations.
 Our protocol is built upon a previously proposed quantum public-key protocol and its practical security relies on the laws of Quantum Mechanics.
 Practical security is reflected in the fact that.
We present a challenging new dataset for autonomous driving: the Oxford RobotCar Dataset.
 Over the period of May 2014 to December 2015 we traversed a route through central Oxford twice a week on average using the Oxford RobotCar platform.
We present a code for generating synthetic spectral energy distributions and intensity maps from smoothed particle hydrodynamics simulation snapshots.
 The code is based on the Lucy Monte Carlo radiative transfer method.
We present a collaborative attempt to build select toolboxes of Scilab using external Free and Open Source Software (FOSS) libraries.
 A C/C++ interface is written for each library.
 Scilab variables are transferred to C/C++ variables.
We present a comprehensive study of the list update problem with locality of reference.
 More specifically.
We present a continuous variable (CV) quantum key distribution (QKD) scheme based on the CV quantum teleportation of coherent states that yields a raw secret key made up of discrete variables for both Alice and Bob.
 This protocol preserves the efficient detection schemes of current CV technology (no single-photon detection techniques) and.
We present a data-driven method for synthesizing 3D indoor scenes by inserting objects progressively into an initial.
We present a data-parallel algorithm for the construction of Delaunay triangulations on the sphere.
 Our method combines a variant of the classical Bowyer-Watson point insertion algorithm with the recently published parallelization technique by Jacobsen et al.
 It resolves a breakdown situation of the latter approach and is suitable for practical implementation because of its compact formulation.
 Some complementary aspects are discussed such as the parallel workload and floating-point arithmetics.
 In a second step.
We present a fast and memory efficient algorithm for generating Compact Precomputed Voxelized Shadows.
 By performing much of the common sub-tree merging before identical nodes are ever created.
We present a fully automated multi-sperm tracking algorithm.
 It has the demonstrated capability to detect and track simultaneously hundreds of sperm cells in recorded videos while accurately measuring motility parameters over time and with minimal operator intervention.
 Algorithms of this kind may help in associating dynamic swimming parameters of human sperm cells with fertility and fertilization rates.
 Specifically.
We present a general computation model inspired in the notion of information hiding in software engineering.
 This model has the form of a game which we call quiz game.
 It allows in a uniform way to prove exponential lower bounds for several complexity problems.
 (C) 2016 Elsevier Inc.
 All rights reserved.
We present a general method for transferring skeletons and skinning weights between characters with distinct mesh topologies.
 Our pipeline takes as inputs a source character rig (consisting of a mesh.
We present a language-independent verification framework that can be instantiated with an operational semantics to automatically generate a program verifier.
 The framework treats both the operational semantics and the program correctness specifications as reachability rules between matching logic patterns.
We present a logic for reasoning about attribute dependencies in data involving degrees such as a degree to which an object is red or a degree to which two objects are similar.
 The dependencies are of the form A a double dagger' B and can be interpreted in two ways: first.
We present a method for computing ambient occlusion (AO) for a stack of images of a Lambertian scene from a fixed viewpoint.
 Ambient occlusion.
We present a method for parallel block-sparse matrix-matrix multiplication on distributed memory clusters.
 By using a quadtree matrix representation.
We present a method of recognizing an object through a multi-mode fiber.
 A number of speckle patterns transmitted through a multi-mode fiber are provided to a classifier based on machine learning.
 We experimentally demonstrated binary classification of face and non-face targets based on the method.
 The measurement process of the experimental setup was random and nonlinear because a multi-mode fiber is a typical strongly scattering medium and any reference light was not used in our setup.
 Comparisons between three supervised learning methods.
We present a method that expands on previous work in learning human perceived style similarity across objects with different structures and functionalities.
 Unlike previous approaches that tackle this problem with the help of hand-crafted geometric descriptors.
We present a method to design the deformation behavior of 3D printed models by an interactive tool.
We present a methodology based on matrix factorization and gradient descent to predict the number of sessions established in the access points of a Wi-Fi network according to the users' behavior.
 As the network considered in this work is monitored and controlled by software in order to manage users and resources in real time.
We present a model checking-based method for verifying list-based concurrent set data structures.
 Concurrent data structures are notorious for being hard to get right and thus.
We present a model for the fast evaluation of the total drag of ship hulls operating in both wet and dry transom stern conditions.
We present a multi-level formation model for complex software systems.
 The previous works extract the software systems to software networks for further studies.
We present a new and efficient algorithm for finding point sources in the photon event data stream from the Fermi Gamma-Ray Space Telescope.
We present a new approach to compute the mass matrix of solid finite elements which allows a significant reduction in the number of integration points.
 The method is based on exploiting information regarding the mathematical form of the integrand.
 This enables higher degree of precision for the same number of integration points compared to standard quadrature use.
 The approach is general and can be applied to both consistent and lumped matrices of all element types.
We present a new approach to lightweight intelligent transportation systems.
 Our approach does not rely on traditional expensive infrastructures.
We present a new framework for analysis and visualization of complex networks based on structural information retrieved from their distance k-graphs and B-matrices.
 The construction of B-matrices for graphs with more than 1 million edges requires massive Breadth-First Search (BFS) computations and is facilitated using new software prepared for distributed environments.
 Our framework benefits from data parallelism inherent to all-pair shortest-path problem and extends Cassovary.
We present a new method for decomposing an image into a set of soft color segments that are analogous to color layers with alpha channels that have been commonly utilized in modern image manipulation software.
 We show that the resulting decomposition serves as an effective intermediate image representation.
We present a new numerical system using classical finite elements with mesh adaptivity for computing stationary solutions of the Gross-Pitaevskii equation.
 The programs are written as a toolbox for FreeFem++ (www.
We present a new scheme on implementing the passive quantum key distribution with thermal distributed parametric down-conversion source.
 In this scheme.
We present a new version of the core structural package of our Application Programming Interface.
We present a newly.
We present a novel image-analysis based method for automatically distinguishing low.
We present a novel natural user interface framework.
We present a novel parallel IP address lookup architecture based on graphics processing unit (GPU) via compute unified device architecture (CUDA).
 Our architecture consists of two functions: 1) host function and 2) device function.
 The host function is executed by a CPU to construct and update the data structure of IP address lookup executed by the device function in a GPU.
 Both host and device functions are executed simultaneously to fully utilize computational resources.
 To shorten the lookup time.
We present a parallel local search approach for obtaining high quality solutions to the Fixed Charge Multicommodity Network Flow problem (FCMNF).
 The approach proceeds by improving a given feasible solution by solving restricted instances of the problem where flows of certain commodities are fixed to those in the solution while the other commodities are locally optimized.
 We derive multiple independent local search neighborhoods from an arc-based mixed integer programming (MIP) formulation of the problem which are explored in parallel.
 Our scalable parallel implementation takes advantage of the hybrid memory architecture in modern platforms and the effectiveness of MIP solvers in solving small problems instances.
 Computational experiments on FCMNF instances from the literature demonstrate the competitiveness of our approach against state of the art MIP solvers and other heuristic methods.
 (C) 2016 Elsevier Ltd.
 All rights reserved.
We present a performance comparison of bounding volume hierarchies and kd-trees for ray tracing on many-core architectures (GPUs).
 The comparison is focused on rendering times and traversal characteristics on the GPU using data structures that were optimized for very high performance of tracing rays.
 To achieve low rendering times.
We present a phase-field model to simulate grain growth occurring in the polysilicon channel process.
We present a pipeline of algorithms that decomposes a given polygon model into parts such that each part can be 3D printed with high (outer) surface quality.
 For this we exploit the fact that most 3D printing technologies have an anisotropic resolution and hence the surface smoothness varies significantly with the orientation of the surface.
 Our pipeline starts by segmenting the input surface into patches such that their normals can be aligned perpendicularly to the printing direction.
 A 3D Voronoi diagram is computed such that the intersections of the Voronoi cells with the surface approximate these surface patches.
 The intersections of the Voronoi cells with the input model's volume then provide an initial decomposition.
 We further present an algorithm to compute an assembly order for the parts and generate connectors between them.
 A post processing step further optimizes the seams between segments to improve the visual quality.
 We run our pipeline on a wide range of 3D models and experimentally evaluate the obtained improvements in terms of numerical.
We present a portable platform.
We present a public key encryption scheme for relational databases (PKDE) that allows the owner to control the execution of cross-relation joins on an outsourced server.
 The scheme allows anyone to deposit encrypted records in a database on the server.
 Thereafter.
We present a Python program.
We present a quantum key distribution (QKD) method that allows us to demonstrate practical quantum cryptography.
 It is used for obtaining the cryptographic key to encode the information communicated between to legitimate parties.
 It generally relates to a secured key sharing method and system between a plurality of transmitters and a plurality of receivers.
We present a refined entanglement concentration protocol (ECP) for an arbitrary unknown less-entangled four-electron-spin cluster state by exploring the optical selection rules derived from the quantum-dot spins in one-sided optical microcavities.
 In our ECP.
We present a review of recent techniques for performing geometric analysis in cultural heritage (CH) applications.
 The survey is aimed at researchers in the areas of computer graphics.
We present a set of Matlab/Octave functions to compute measures of emergence.
We present a simple geometric framework for the relational join.
 Using this framework.
We present a simple method to describe the geometry and topologically classify the intersection of level sets of trilinear interpolants with a reference unit cell.
 The solutions of three quadratic equations are used to correctly triangulate the level set within the cell satisfying the conditions imposed by the asymptotic decider.
 This way the triangulation of isosurfaces across cells borders is manifold and topologically correct.
 The algorithm presented is intuitive and easy to implement.
We present a storage management system that has the ability to adapt to the data access characteristics of the application that uses it based on collection and analysis of runtime statistics.
 This feature is especially useful in the storage management layer of database systems.
We present a supervised machine learning approach for markerless estimation of human full-body kinematics for a cyclist from an unconstrained colour image.
 This approach is motivated by the limitations of existing marker-based approaches restricted by infrastructure.
We present a survey of existing approaches to relational division in rank-aware databases.
We present a symbolic-numeric hybrid method.
We present a system.
We present a technique for representing bounded-degree planar graphs in a succinct fashion while permitting I/O-efficient traversal of paths.
 Using our representation.
We present a top-k algorithm for retrieving tuples according to the order provided by a ranking function that belongs to a subclass of non-monotonic functions.
 The ranking functions are defined with the values where the maximum score is achieved.
 We test the proposed algorithm on various real and artificial data with varying variable ranges and different non-monotonic ranking functions.
We present a vector field approximation for two-dimensional vector fields that preserves their topology and significantly reduces the memory footprint.
 This approximation is based on a segmentation.
 The flow within each segmentation region is approximated by an affine linear function.
 The implementation is driven by four aims: (1) the approximation preserves the original topology; (2) the maximal approximation error is below a user-defined threshold in all regions; (3) the number of regions is as small as possible; and (4) each point has the minimal approximation error.
 The generation of an optimal solution is computationally infeasible.
 We discuss this problem and provide a greedy strategy to efficiently compute a sensible segmentation that considers the four aims.
We present a vision system design for landing an unmanned aerial vehicle on a ship's flight deck autonomously.
 The edge information from the international landing target is used to perform line segment detection.
We present a visual assistive system that features mobile face detection and recognition in an unconstrained environment from a mobile source using convolutional neural networks.
 The goal of the system is to effectively detect individuals that approach facing towards the person equipped with the system.
 We find that face detection and recognition becomes a very difficult task due to the movement of the user which causes camera shakes resulting in motion blur and noise in the input for the visual assistive system.
 Due to the shortage of related datasets.
We present an accurate method for the numerical integration of polynomials over arbitrary polyhedra.
 Using the divergence theorem.
We present an algorithm to efficiently compute accurate volumes and surface areas of macromolecules on graphical processing unit (GPU) devices using an analytic model which represents atomic volumes by continuous Gaussian densities.
 The volume of the molecule is expressed by means of the inclusion-exclusion formula.
We present an approach for scalable long-term preservation of data stored in relational databases (RDBs) as RDF.
We present an efficient Cloud Computing (CC) implementation of the Parallel Small BAseline Subset (P-SBAS) algorithm.
We present an eigenspectrum partitioning scheme without inversion for the recently described real space electronic transport code.
We present an innovative image-based modeling technique.
We present an optimal and efficient algorithm for finding a shortest path in an elastic optical network.
 The algorithm is an adaptation of the Dijkstra shortest path algorithm.
We present Assemble!.
We present BILL2D.
We present efficient locking mechanisms for hierarchical data structures.
 Several applications work on an abstract hierarchy of objects.
We present FabSim.
We present GPUrdma.
We present here a vision of individualized Knowledge Graphs (iKGs) in cardiovascular medicine: a modern informatics platform of exchange and inquiry that comprehensively integrates biological knowledge with medical histories and health outcomes of individual patients.
 We envision that this could transform how clinicians and scientists together discover.
We present HiLLS (High Level Language for System Specification).
We present in this paper a general framework to study issues of effective load balancing and scheduling in highly parallel and distributed environments such as currently built Cloud computing systems.
 We propose a novel approach based on the concept of the Sandpile cellular automaton: a decentralized multi-agent system working in a critical state at the edge of chaos.
 Our goal is providing fairness between concurrent job submissions by minimizing slowdown of individual applications and dynamically rescheduling them to the best suited resources.
 The algorithm design is experimentally validated by a number of numerical experiments showing the effectiveness and scalability of the scheme in the presence of a large number of jobs and resources and its ability to react to dynamic changes in real time.
We present in this paper a new approach for Arabic sign language (ArSL) alphabet recognition using hand gesture analysis.
 This analysis consists in extracting a histogram of oriented gradient (HOG) features from a hand image and then using them to generate an SVM Models.
 Which will be used to recognize the ArSL alphabet in real- time from hand gesture using a Microsoft Kinect camera.
 Our approach involves three steps: (i) Hand detection and localization using a Microsoft Kinect camera.
We present in this paper a new generic approach to variable branching in branch and bound for mixed integer linear problems.
 Our approach consists in imitating the decisions taken by a good branching strategy.
We present in this paper the KO library.
We present Jump.
We present new connections between quantum information and the field of classical cryptography.
 In particular.
We present new efficient data structures for elements of Coxeter groups of type Am and their associated Iwahori Hecke algebras H(A(m)).
We present numerical simulations of a directional coupler based on three-dimensional waveguides made of a nematic liquid crystal.
We present Ontop.
We present optimal deterministic algorithms for constructing shallow cuttings in an arrangement of lines in two dimensions or planes in three dimensions.
 Our results improve the deterministic polynomial-time algorithm of Matouek (Comput Geom 2(3):169-186.
We present our framework for the verification of parameterized infinite-state systems.
 The framework has been successfully applied in the verification of heterogeneous systems.
We present Palirria.
We present preliminary results of work on a low-cost multi-user immersive Virtual Reality system that enables collaborative experiences in large virtual environments.
 In the proposed setup at least three users can walk and interact freely and untethered in a 200 m(2) area.
 The required equipment is worn on the body and rendering is performed locally on each user to minimize latency.
 Inside-out optical head tracking is coupled with a low-cost motion capture suit to track the full body and the head.
 Movements of users.
We present sam(oa)(2).
We present scalable algorithms for the level-set method on dynamic.
We present some new matrix spectral problems.
We present some preliminary results and discussion of our ongoing effort to develop a prototype volumetric atmospheric optical refraction simulator which uses three-dimensional (3D) nonlinear ray-tracing and state of-art physics-based rendering techniques.
 The tool will allow simulation of optical curved-ray propagation through nonlinear refractivity gradient profiles in volumetric atmospheric participating media.
We present the development framework for the distributed control systems.
We present the easy-to-use Sequence Search Tool for Antimicrobial Resistance.
We present the features and results of a newly developed code.
We present the first multi-viewpoint coronal mass ejection (CME) catalog.
 The events are identified visually in simultaneous total brightness observations from the twin SECCHI/COR2 coronagraphs on.
 board the Solar Terrestrial Relations Observatory mission.
 The Multi-View CME Catalog differs from past catalogs in three key aspects: (1) all events between the two viewpoints are cross-linked.
We present the first parallel on-demand spatial hierarchy construction algorithm targeting ray tracing on many-core processors such as GPUs.
 The method performs simultaneous ray traversal and spatial hierarchy construction focused on the parts of the data structure being traversed.
 The method is based on a versatile framework built around a task pool and runs entirely on the GPU.
 We show that the on-demand construction can improve rendering times compared to full hierarchy construction.
 We evaluate our method on both object (BVH) and space (kd-tree) subdivision data structures and compare them mutually.
 The on-demand method is particularly beneficial for rendering large scenes with high occlusion.
 We also present SAH kd-tree builder that outperforms previous state-of-the-art builders running on the GPU.
We present the first public release (v0.
1) of the open-source GADGET Dataframe Library: gadfly.
 The aim of this package is to leverage the capabilities of the broader python scientific computing ecosystem by providing tools for analyzing simulation data from the astrophysical simulation codes GADGET and GIZMO using pandas.
We present the parallel and interacting stochastic approximation annealing (PISAA) algorithm.
We present the R package mnlogit for estimating multinomial logistic regression models.
We present the results from analyzing the physical and morphological properties of 154 dimmings (transient coronal holes) and the associated flares and coronal mass ejections (CMEs).
 Each dimming in our 2013 catalog was processed with the semi-automated Coronal Dimming Tracker using Solar Dynamics Observatory AIA 193 angstrom observations and HMI magnetograms.
 Instead of the typically used difference images.
We present the results obtained by using an evolution of our CUDA-based solution for the exploration.
We present THE-WIZZ.
We present three new matrix spectral problems.
We present TimeFork.
We present two new matrix spectral problems.
We propose a computational framework for the simulation of deformation and fracture in shells that is well suited to situations with widespread damage and fragmentation due to impulsive loading.
 The shell is modeled with a shear-flexible theory and discretized with a discontinuous Galerkin finite element method.
We propose a computational model for shape.
We propose a computationally efficient fully polynomial-time approximation scheme (FPTAS) to compute an approximation with arbitrary precision of the value function of convex stochastic dynamic programs.
We propose a differential versioning based data storage (DiVers) architecture for distributed storage systems.
We propose a distributed local search (DLS) algorithm.
We propose a fast calculation method to synthesize a computer-generated hologram (CGH) of realistic deep threedimensional (3D) scene.
 In our previous study.
We propose a heralded quantum repeater based on the scattering of photons off single emitters in one-dimensional waveguides.
 We show the details by implementing nonlocal entanglement generation.
We propose a many-core GPU implementation of robotic motion planning formulated as a semi-infinite optimization program.
 Our approach computes the constraints and their gradients in parallel.
We propose a new (n.
We propose a new class of profiler for distributed and heterogeneous systems.
 In these systems.
We propose a new method of client-side data caching for relational databases with a central server and distant clients.
 Data is loaded into client cache based on queries executed on the central database at the server.
 These queries have a special form universal relational query.
 The majority of search queries to database can be expressed in such form.
We propose a new sharing analysis of object-oriented programs based on abstract interpretation.
 Two variables share when they are bound to data structures which overlap.
 We show that sharing analysis can greatly benefit from linearity analysis.
 We propose a combined domain including aliasing.
We propose a novel approach for observing cosmic rays at ultra-high energy (>10(18) eV) by repurposing the existing network of smartphones as a ground detector array.
 Extensive air showers generated by cosmic rays produce muons and high-energy photons.
We propose a novel approximation technique.
We propose a novel framework for photometric stereo (PS) under low-light conditions using uncalibrated near-light illumination.
 It operates on free-form video sequences captured with a minimalistic and affordable setup.
 We address issues such as albedo variations.
We propose a novel method conceptualized from the properties of physics where in particular the shape of a flame is determined by temperature that enables a control mechanism for the intuitive shaping of a flame.
 We focused on a trade-off issue from computer graphics whereby the turbulent flow that expresses the characteristics of the flame has a tendency to shift continuously.
We propose a novel parallel computing framework for a nonlinear finite element method (FEM)-based cell model and apply it to simulate avascular tumor growth.
 We derive computation formulas to simplify the simulation and design the basic algorithms.
 With the increment of the proliferation generations of tumor cells.
We propose a novel variational approach to the depth-from-defocus problem.
 The quality of such methods strongly depends on the modelling of the image formation (forward operator) that connects depth with out of-focus blur.
We propose a quantum public-key encryption (QPKE) protocol for an unknown multi-qubit state based on qubit-wise teleportation.
 The private-key is a computational Boolean function.
We propose a quantum secret sharing scheme that uses an orthogonal pair of n-qudit GHZ states and local distinguishability.
 In the proposed protocol.
We propose a spectral turning-bands approach for the simulation of second-order stationary vector Gaussian random fields.
 The approach improves existing spectral methods through coupling with importance sampling techniques.
 A notable insight is that one can simulate any vector random field whose direct and cross-covariance functions are continuous and absolutely integrable.
We propose a support vector regression approach for symbol detection in large-MIMO systems employing spatial multiplexing.
 We explore the applicability of machine learning algorithms.
We propose a two-step approach for the construction of planar smooth collision-free navigation paths.
 Obstacle avoidance techniques that rely on classical data structures are initially considered for the identification of piecewise linear paths having no intersection with the obstacles of a given scenario.
 Variations of the shortest piecewise linear path with angle-based criteria are proposed and discussed.
 In the second part of the scheme we rely on spline interpolation algorithms with tension parameters to provide a smooth planar control strategy.
 In particular.
We propose an active method to rectify the geometric and photometric distortions in a document image.
 This method uses structured beams projected upon the curved document page to recover the spatial curves on it.
 A developable surface is interpolated to the curves by finding the correspondence between them.
 To correct the geometric distortion.
We propose an algebraization of classical and non-classical logics.
We propose an architecture for self-motivated agents allowing them to construct their own knowledge of objects and of geometrical properties of space through interaction with their environment.
 Self-motivation is defined here as a tendency to experiment and to respond to behavioral opportunities afforded by the environment.
 Interactions have predefined valences that specify inborn behavioral preferences.
 The long-term goal is to design agents that construct their own knowledge of their environment through experience.
We propose an efficient entanglement concentration protocol (ECP) for nonlocal three-atom systems in an arbitrary unknown less-entangled W state.
We propose an integrated curriculum to establish essential abilities of computer programming for the freshmen of a physics department.
 The implementation of the graphical-based interfaces from Scratch to LabVIEW then to LabVIEW for Arduino in the curriculum 'Computer-Assisted Instrumentation in the Design of Physics Laboratories' brings rigorous algorithm and syntax protocols together with imagination.
We propose an ultra-lightweight.
We propose and explore the applicability of file timestamps as a steganographic channel.
 We identify an information gap between storage and usage of timestamps in modern operating systems that use high-precision timers.
 Building on this.
We propose and investigate a semantics for peer data exchange systems where different peers are related by data exchange constraints and trust relationships.
 These two elements plus the data at the peers' sites and their local integrity constraints are made compatible via a semantics that characterizes sets of solution instances for the peers.
 They are the intended - possibly virtual - instances for a peer that are obtained through a data repair semantics that we introduce and investigate.
 The semantically correct answers from a peer to a query.
We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces.
 While shape-changing interfaces can dynamically alter the physical appearance of objects.
We propose herein a new portfolio selection method that switches between two distinct asset allocation strategies.
 An important component is a carefully designed adaptive switching rule.
We propose multi-party quantum summation protocols based on single particles.
We propose SSketch.
We propose the use of negative dielectrophoresis (DEP) spectroscopy as a technique to improve the detection limit of rare analytes in biological samples.
 We observe a significant dependence of the negative DEP force on functionalized polystyrene beads at the edges of interdigitated electrodes with respect to the frequency of the electric field.
 We measured this velocity of repulsion for 0% and 0.
8% conjugation of avidin with biotin functionalized polystyrene beads with our automated software through real-time image processing that monitors the Rayleigh scattering from the beads.
 A significant difference in the velocity of the beads was observed in the presence of as little as 80 molecules of avidin per biotin functionalized bead.
 This technology can be applied in the detection and quantification of rare analytes that can be useful in the diagnosis and the treatment of diseases.
We propose to incorporate encryption procedure into the lossy compression of voice data PCM(Pulse Code Modulation) based on the A-law approximation quantization.
 The proposed codec CPCM (Chaotic Pulse Code Modulaion) will join the encryption to the compression of the voice data.
 This scheme provides the same compression ratio given by the PCM codec.
We propose two end-view-based digital image processing algorithms for high-precision rotational angle alignment of specialty optical fibers.
 The first technique is based on the cross-correlation calculation between images of two fiber ends.
 Accurate alignment can be achieved between identical specialty optical fibers with arbitrary end face structures.
 The other one is based on the self-correlation method.
 Two fibers with different refractive index profiles in the fusion splicer can be rotated to preset angles independently with high precision.
 Accurate angle alignment of less than 0.
5 degrees uncertainty and low average splicing loss for seven-core multicore fibers have been achieved in experiment.
 To speed up the alignment process.
We proposed the method that generates ground surface texture by evaluating pressure applied at each point.
 The method standardizes diameter of clod to original size and its 1/2.
We proposed.
We prove identifiability of the tree parameters of the 3-class Jukes-Cantor mixture model.
 The proof uses ideas from algebraic statistics.
We prove that with high probability over the choice of a random graph G from the Erdos-Renyi distribution G(n.
We provide a comprehensive account of recent advances in biomedical image analysis and classification from two complementary imaging modalities: terahertz (THz) pulse imaging and dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI).
 The work aims to highlight underlining commonalities in both data structures so that a common multi-channel data fusion framework can be developed.
 Signal pre-processing in both datasets is discussed briefly taking into consideration advances in multi-resolution analysis and model based fractional order calculus system identification.
 Developments in statistical signal processing using principal component and independent component analysis are also considered.
 These algorithms have been developed independently by the THz-pulse imaging and DCE-MRI communities.
We provide an overview of selected crypto-hardware devices.
We report on the first application of the graphics processing units (GPUs) accelerated computing technology to improve performance of numerical methods used for the optical characterization of evaporating microdroplets.
 Single microdroplets of various liquids with different volatility and molecular weight (glycerine.
We review the relatively immature field of automated image analysis for X-ray cargo imagery.
 There is increasing demand for automated analysis methods that can assist in the inspection and selection of containers.
We revisit the classic problem of spreading a piece of information in a group of fully connected processors.
 By suitably adding a small dose of randomness to the protocol of Gasieniec and Pelc (Parallel Comput 22:903-912.
We revisit the interactive model-based approach to global optimization proposed in Wang and Garcia (J Glob Optim 61(3): 479-495.
We show how the display-map category of finite simplicial complexes can be seen as representing the totality of database schemas and instances in a single mathematical structure.
 We give a sound interpretation of a certain dependent type theory in this model.
We show how the longest common prefix (LCP) array can be generated as a by-product of the suffix array construction algorithm SACA-K (Nong.
We show that it is possible for the so-called weak locking capacity of a quantum channel (Guha et al.
 in Phys Rev X 4:011016.
We show that.
We solve a 40-year-old open problem on depth optimality of sorting networks.
We study empirical metrics for software source code.
We study general techniques for implementing distributed data structures on top of future many-core archi-tectures with non cache-coherent or partially cache-coherent memory.
 With the goal of contributing towards what might become.
We study geometric and topological properties of the image of a smooth submanifold of under a bi-Lipschitz map to .
 In particular.
We study the distributed (Delta + 1)-vertex-coloring and (2 Delta - 1)-edge-coloring problems.
 These problems are among the most important and intensively studied problems in distributed computing.
 Despite very intensive research in the last 30 years.
We study the dynamic scheduling problem for jobs with fixed start and end times on multiple machines.
 The problem is to design efficient data structures that support the update operations: insertions and deletions of jobs.
 Call the period of time in a schedule between two consecutive jobs in a given machine an idle interval.
 We show that for any set of jobs there exists a schedule such that the corresponding set of idle intervals forms a tree under the set-theoretic inclusion.
 We prove that any such schedule is optimal.
 Based on this result.
We study the following questions related to wireless network security: Which jammer placement configuration during a jamming attack results in the largest degradation of network throughput? and Which network design strategies are most effective in mitigating a jamming attack? Although others have studied similar jammer placement problems.
We study the nonlinear dynamics of (2 + 1) dimensional ferromagnetic (FM) spin system with bilinear and biquadratic interactions in the semiclassical limit and the dynamics is found to be governed by a new integrable fourth order nonlinear Schrodinger (NLS) equation in (2 + 1) dimensions.
 The integrability is identified by using Lax pair operators and soliton solutions are obtained using straightforward Darboux transformation (DT) technique.
 The model Hamiltonian representing (2 + 1) dimensional FM spin chain with varying bilinear and biquadratic interactions are also constructed and inhomogeneity effects are studied by performing a perturbation analysis.
We study the problem of designing a data structure that reports the positions of the distinct iota-majorities within any range of an array A[1.
We study the problem of indexing a text T[1.
n] such that whenever a pattern P[1.
p] and an interval [alpha.
We study the Reliable Broadcast problem in incomplete networks against a Byzantine adversary.
 We examine the problem under the locally bounded adversary model of Koo (Proceedings of the 23rd annual ACM symposium on principles of distributed computing.
We survey genetic improvement (GI) of general purpose computing on graphics cards.
 We summarise several experiments which demonstrate four themes.
 Experiments with the gzip program show that genetic programming can automatically port sequential C code to parallel code.
 Experiments with the StereoCamera program show that GI can upgrade legacy parallel code for new hardware and software.
 Experiments with NiftyReg and BarraCUDA show that GI can make substantial improvements to current parallel CUDA applications.
We teach computer programming to students aged 17 through 18 years.
 The course is structured into two units.
We theorize a two-mind model of design thinking.
 Mind 1 is about logical design reasoning.
We turn the Self-organizing Map (SOM) into an Oriented and Scalable Map (OS-Map) by generalizing the neighborhood function and the winner selection.
 The homogeneous Gaussian neighborhood function is replaced with the matrix exponential.
 Thus we can specify the orientation either in the map space or in the data space.
We use cache-sensitive memory layouts to improve search performance in ordinary binary search trees.
We use symbolic computation (SC) and homological perturbation (HPT) to compute a resolution of the integers over the integer group ring of G.
We use the first integral method to construct closed-from exact solutions of the density dependent Fitzhugh-Nagumo telegraph equation.
 While this method is very useful for obtaining formal solutions.
We want to carry out an effectively distributed method to process large power data.
 Large data of electricity is first classified by big industries.
Wearable devices have emerged in the last years with new applications that provide user convenience.
 Healthcare.
Wearable devices monitoring food intake through passive sensing is slowly emerging to complement self-reporting of users' caloric intake and eating behaviors.
 Though the ultimate goal for the passive sensing of eating is to become a reliable gold standard in dietary assessment.
WearTec is an NSF funded project focused on activities related to wearable technologies.
 The goals of the project are to develop an intervention that focuses on solving real world problems and practicing the engineering design process while immersed in the innovative area of wearable technologies.
 Curriculum has been developed focused on youth in grades 4 to 6 to teach engineering design.
Web news articles are generated in continuous.
Web service discovery plays a crucial role in the development of applications based on service-oriented architecture.
Web services allow middleware access to a relational database and require data representation in XML format.
 The XML views obtained from relational databases can be accessed by using XPath queries.
 This article proposes an optimization model for XML data processing based on a heuristic algorithm to extract data from XPath views.
 To this end.
Web services provide functionalities to the users.
 Software products and services require high quality.
 Quality parameters of Web and cloud based applications includes scalability.
Weight and efficiency are conflicting top requirements for all kind of mobile applications of electrical machines.
 Utilizing the design freedom of a novel machine design.
Weighted median filtering is a fundamental operator in a great variety of image processing and computer graphics applications.
 This paper presents a novel real-time weighted median filter which smoothes out high-frequency details while preserving major edges.
 We define a new 4D bilateral grid by incorporating the 3D bilateral grid with an additional range dimension.
 The edge-aware weights and the weighted median values are computed in the 4D space.
 The proposed algorithm is highly parallel.
Wheat is a cereal grain and one of the world's major food crops.
 Recent advances in wheat genome sequencing are by now facilitating its genomic and proteomic analyses.
When a mobile application is supported on multiple major platforms.
When an image is given with only some measurable data.
When an unsymmetric laminated plate is considered.
When compared to earlier programming and data structure experiences that our students might have.
When designing and developing scale selection mechanisms for generating hypotheses about characteristic scales in signals.
When performing electron tomography.
When switching to automatic output-quality testing systems.
When there is a concern about the best way to apply the foldover technique to a Resolution IV fraction.
When traveling virtually through large scenes.
Whenever an intrusion happens.
While all cloud based platforms possess security vulnerabilities.
While cloud computing led the path towards a revolutionary change in the modern day computing aspects.
While criteria for Schenkerian analysis have been much discussed.
While cryptography is used to protect the content of information (e.
While display quality and rendering for Head-Mounted-Displays (HMDs) has increased in quality and performance.
While horizontal partitioning of traditional relational databases is well studied.
While it is usually not difficult to compute principal curvatures of a smooth surface of sufficient differentiability.
While many schemes have been proposed to search encrypted relational databases.
While performing cryptanalysis.
While quantitative structure activity relationship (QSAR) models have been employed successfully for the prediction of small model protein chromatographic behavior.
While Russian roulette (RR) and splitting are considered fundamental importance sampling techniques in neutron transport simulations.
While the data deluge accelerates.
While the quality of products is a competitive advantage for very small software development organizations.
While we are witnessing a transition from petascale to exascale computing.
While weeds in sugar beet farming reduce crop yield and quality.
White-light coronal and heliospheric imagers observe scattering of photospheric light from both dust particles (the F-Corona) and free electrons in the corona (the K-corona).
 The separation of the two coronae is thus vitally important to reveal the faint K-coronal structures (e.
WIA-PA is an industrial wireless networking protocol standard used in process automation.
 One mechanism based on the WIA-PA multi-channel MAC has been proposed to solve industry wireless sensing questions in network channel disturbance and medium turning on conflict.
 The mechanism mainly includes the multi-channel MAC mechanism design proposal and the channel assignment algorithm design about WIA-PA.
Wide spread use of biometric based authentication implies the need to secure biometric reference data.
 Various template protection schemes have been introduced to prevent biometric forgery and identity thefts.
 Cancelable biometrics and visual cryptography are two recent technologies introduced to address the concerns regarding privacy of biometric data.
Widely adumbrated as patterns of parallel computation and communication.
Widespread utilization of mobile ad hoc networks.
Wi-Fi is a local area wireless networking technology that is widely used for different purposes such as data transmission and wireless communication.
 Wi-Fi connection will most often result in faster.
WiFi offloading.
Wind power is a type of clean and renewable energy.
Windows Azure Storage (WAS) is a cloud storage system that provides customers the ability to store seemingly limitless amounts of data for any duration of time.
 WAS customers have access to their data from anywhere at any time and only pay for what they use and store.
Wireless big data significantly challenges the current network management and control architecture.
Wireless body area network (WBAN) provide a mechanism of transmitting a persons physiological data to application providers e.
 Given the limited range of connectivity associated with WBAN.
Wireless communication is widely used for node localization in various automation domains.
 Schemes based on time difference of arrival (TDOA) are a good compromise of achievable accuracy and implementation effort.
 Due to restrictions on possible reference sensor positions in industrial environments.
Wireless communication plays a critical role in determining the lifetime of Internet-of-Things (IoT) systems.
 Data aggregation approaches have been widely used to enhance the performance of IoT applications.
 Such approaches reduce the number of packets that are transmitted by combining multiple packets into one transmission unit.
Wireless Mesh Networks (WMN) are growing rapidly in the research community due to their numerous applications and shared services.
 A feature in this kind of networks is the any-to-any connectivity.
Wireless network is a group of computer network devices using wireless communication.
 There are many types of jamming attacks affecting in wireless communication.
 In order to avoid the jamming attacks in the flow of network communication.
Wireless network security is becoming a great challenge as its popularity is in the high spirit.
 On account of open medium.
Wireless networks have gained immense popularity in recent years.
 Though wireless networks have innumerous advantages over conventional wired networks.
Wireless sensor network (WSN) is widely used in environmental conditions where the systems depend on sensing and monitoring approach.
 Water pollution monitoring system depends on a network of wireless sensing nodes which communicate together depending on a specific topological order.
 The nodes distributed in a harsh environment to detect the polluted zones within the WSN range based on the sensed data.
 WSN exposes several malicious attacks as a consequence of its presence in such open environment.
Wireless sensor network(WSN) contains many specially distributed sensors which collect information for people to analyze appointed objects in real-time.
 WSN is deployed widely in many fields.
Wireless Sensor Networks (WSNs) are a growing subset of the emerging Internet of Things (IoT).
 WSNs reduce the cost of deployment over wired alternatives; consequently.
Wireless sensor networks (WSNs) are characterized by localized interactions.
Wireless sensor networks (WSNs) are susceptible to many security threats and are specifically prone to physical node capture in which the adversary can easily launch the so-called insider attacks such as node compromise.
Wireless Sensor Networks (WSNs) consists of large number of spatially distributed configurable sensors.
Wireless sensor networks (WSNs) contain multiple nodes of the same configuration and type.
 The biggest challenge nowadays is to communicate with heterogeneous nodes of different WSNs.
 To communicate with distinct networks.
Wireless sensor networks (WSNs) have become increasingly popular in many applications across a broad range of fields.
 Securing WSNs poses unique challenges mainly due to their resource constraints.
 Traditional public key cryptography (PKC) for instance is considered to be too computationally expensive for direct implementation in WSNs.
 Elliptic curve cryptography (ECC) allows one to reach the same level of security as traditional PKC using smaller key sizes.
 In this paper.
Wireless sensor networks (WSNs) have been widely used in different fields.
Wireless sensor networks are already employed in numerous applications including military.
Wireless sensor networks are commonly used for critical security tasks such as intrusion or tamper detection.
Wireless Sensor Networks are installed in hostile areas.
 The security issues in wireless sensor networks are very important.
 Getting secure links between nodes is a challenging problem in WSNs.
 They are more vulnerable to security attacks than wired networks.
 In order to protect the sensitive data in WSN can be protected using secret keys to encrypt the exchanged messages between communicating nodes.
 Key management is essential for many security services such as confidentiality and authentication.
 The symmetric or asymmetric key cryptography or Trusted-server schemes are used to solve this problem.
 Asymmetric key cryptography increases network security but it increases computational.
Wireless Sensor Networks are installed in hostile areas.
 The security issues in wireless sensor networks are very important.
 Getting secure links between nodes is a challenging problem in WSNs.
 They are more vulnerable to security attacks than wired networks.
 In order to protect the sensitive data in WSN can be protected using secret keys to encrypt the exchanged messages between communicating nodes.
 Key management is essential for many security services such as confidentiality and authentication.
 The symmetric or asymmetric key cryptography or Trusted-server schemes are used to solve this problem.
 Asymmetric key cryptography increases network security but it increases computational.
Wireless technologies combined with advanced computing are changing industrial communications.
 Industrial wireless networks can improve the monitoring and the control of the entire system by jointly exploiting massively interacting communication and distributed computing paradigms.
 In this paper.
With a large number of scientific literature.
With advances in both medical imaging and computer programming.
With advances in technology.
With an increasing amount of volatile renewable electrical energy.
With cloud computing technology becoming more mature.
With continuous development of science and technology.
With emerging Network Functions Virtualization (NFV) and Software Defined Networking (SDN) paradigms in Network Management (NM).
With existing programming tools.
With high and growing supply of Database-as-a-Service solutions from cloud platform vendors.
With increased use of forensic memory analysis.
With increasing altitude.
With increasing global demand for energy resources.
With information becoming a first-class citizen on the Internet.
With introducing demand response aggregator (DRA) in smart paradigm.
With its relatively small key size.
With larger-scale of railway construction in China.
With location-based social network (LBSN) flourishing.
With new advances in computer hardware and software.
With newer complex multi-core systems.
With over 10 million git repositories.
With promising results in recent treatment trials for Alzheimer's disease (AD).
With rapid urbanization.
With rapidly growing Internet traffic.
With recent advancements.
With reference to a distributed architecture consisting of sensor nodes connected by wireless links in an arbitrary network topology.
With respect to automotive safety.
With security and surveillance.
With simple access interfaces and flexible billing models.
With symbolic computation and Hirota method.
With symbolic computation.
With symbolic computation.
With symbolic computation.
With the acceleration development of the Internet of Things and big data.
With the advance of the World-Wide Web (WWW) technology.
With the advancement of internet.
with the advent of agile software processing methods and needs to write the programs and software with the best quality in the shortest possible time.
With the advent of Component-based software engineering (CBSE).
With the advent of large complex datasets.
With the advent of modern technology.
With the advent of new technologies and various services provided in the context of computer networks.
With the advent of Recommendation ITU-R BT.
 2020) color space for ultra-high definition television (UHDTV).
With the aid of symbolic computation by Maple.
With the aid of symbolic computation Maple.
With the aim of improving detection of novel single-nucleotide polymorphisms (SNPs) in genetic association studies.
With the Amazon EC2 Cloud becoming available as a viable platform for parallel computing.
With the amount of Internet traffic increasing substantially.
With the application of conditional free shipping policy becoming more and more extensive.
With the application of network technology.
With the broad use of business process management technology.
With the coming of the big data era.
With the constant development of economy in our country in recent years.
With the continuing growth of renewable penetration in power systems.
With the continuous development of computer technology and network technology.
With the degree of parallelism increasing.
With the development and popularization of Internet.
With the development of computer graphics technology.
With the development of computer network technology.
With the development of computer network technology.
With the development of digital information technologies.
With the development of information and networking technologies.
With the development of multimedia computer technology.
With the development of network technology.
With the development of science and technology progress.
With the development of the communication technology and the intelligent terminal.
With the development of the design complexity in embedded systems.
With the development of trusted network.
With the dramatic growth of network attacks.
With the emergence of Big Data.
With the enormous genetic plasticity of malaria parasite.
With the ever growing demand of location-independent access to Autonomous Decentralized Systems (ADS).
With the ever increasing human dependency on The Internet for performing various activities such as banking.
With the ever-increasing usage of internet.
With the evolution of the research on network moving target defense (MTD).
With the expansion of wireless network technologies and the emergence of novel mobile applications.
With the explosion of 3D character animation across contemporary screen media.
With the explosive growth of digital data communications in synergistic operating networks and cloud computing service.
With the explosive growth of mobile terminal access to the Network and the shortage of IPv4.
With the fast global adoption of the Materials Genome Initiative (MGI).
With the flood of publicly available data.
With the fully mature and a vast number of available virtualization solutions.
With the great success of the second-generation wireless telephone technology and the third-generation mobile telecommunications technology.
With the growing concern about the safety.
With the growing interest in computational models of visual attention.
With the help of symbolic computation.
With the help of the Boussinesq perturbation expansion.
With the help of the symbolic computation system Maple and Riccati equation (xi'=a(0)+a(1 xi)+a(2)xi(2)) expansion method and a variable separation method.
With the help of the symbolic computation system Maple.
With the help of the symbolic computation system.
With the increase of massive data.
With the increasing amount of interconnections between vehicles.
With the increasing availability of high-resolution images.
With the increasing complexity of both data structures and computer architectures.
With the increasing demand for ultra-low latency services in 5G cellular networks.
With the increasing demand of Machine to Machine (M2M) communications and Internet of Things (IoT) services it is necessary to develop a new network architecture and protocols to support cost effective.
With the increasing deployment of network systems.
With the increasing prevalence of electronic readers (e-readers) for vocational and professional uses.
With the increasing proportion of natural gas in power generation.
With the increasing sizes of digital elevation models (DEMs).
With the increasing use of databases.
With the increasingly growing amount of service requests from the world-wide customers.
With the increasingly serious energy crisis and environmental pollution.
With the installation of synchrophasors widely across the power grid.
With the introduction of more and more random factors in power systems.
With the latest developments in database technologies.
With the network scales rapidly and new network applications emerge frequently.
With the number of satellite sensors and date centers being increased continuously.
With the popularization and application of Internet technology.
With the prevalence of information and communication technologies.
With the proliferation of application specific accelerators.
With the proliferation of unstructured data.
with the rapid advancement of technology today.
With the rapid advancement of wireless technologies.
With the rapid advances in the field of computer vision systems and increasing flexibility in using multimedia applications.
With the rapid development of broadband wireless access technology and mobile terminal.
With the rapid development of computer technology.
With the rapid development of embedded system and Internet of Things technology.
With the rapid development of medical imaging technology.
With the rapid development of mobile data acquisition technology.
With the rapid development of mobile Internet.
With the rapid development of our country's economy.
With the rapid development of remote sensing technology.
With the rapid development of science and technology and the network popularization.
With the rapid development of science and technology.
With the rapid development of the computer network technology.
With the rapid development of the Internet.
With the rapid development of the mobile app market.
With the rapid developments of network technology.
With the rapid proliferation of RFID technologies.
With the recent advances in the programmability and performance of mobile Graphics Processing Units (GPUs).
With the recent widespread adoption of service-oriented architecture.
With the steady growth of linked datasets available on the web.
With the support of cloud computing techniques.
With the tremendous growth of Internet.
With the widespread adoption of mobile devices and explosive growth of spatio-temporal data.
With the widespread adoption of multi-core architectures.
With the widespread deployment of ground.
With the widespread utilization of multicore processors.
With traditional data storage solutions becoming too expensive and cumbersome to support Big Data processing.
Within a digital map service environment.
Within education.
Within the diagnostic process.
Within the policies of an organization is possible find not to allow users to access sites on the Internet for entertainment or social networks.
Women are underrepresented in the field of Computer Science.
 This project aims to help Secondary School girls develop an insight into the role computers play in society and to learn some of the key skills in computing including computer programming.
 Exposure to Computer Science.
Wood is a renewable.
Word clouds are a popular method to visualize the frequency of words in textual data.
 Nowadays many text-based data sets.
Word clouds have been widely used to present the contents and themes in the text for summary and visualization.
 In this paper.
Writing highly performant simulations requires a lot of human effort to optimize for an increasingly diverse set of hardware platforms.
XML is a pervasive technology for representing and accessing semi-structured data.
 XPath is the standard language for navigational queries on XML documents and there is a growing demand for its efficient processing.
 In order to increase the efficiency in executing four navigational XML query primitives.
X-ray screening systems have been used to safeguard environments in which access control is of paramount importance.
 Security checkpoints have been placed at the entrances to many public places to detect prohibited items.
Zebrafish (Danio rerio) is an important vertebrate model organism in biomedical research.
Zernike moments (ZMs) are very useful image descriptors which belong to a family of orthogonal rotation invariant moments.
 Due to their many attractive characteristics.
Zero-difference balanced (ZDB) functions have many applications in coding theory.
Zero-Suppressed Binary Decision Diagrams (ZDDs) are widely used data structures for representing and handling combination sets and Boolean functions.
 In particular.
Zinc is a modeling language in which a conceptual model is automatically mapped into different design models using well-known solving techniques: constraint programming.
Z-Wave is an implementation of home automation.
ZY-3 has been acquiring high quality imagery since its launch in 2012 and its tri-stereo (three-view or three-line-array) imagery has become one of the top choices for extracting DSM (Digital Surface Model) products in China over the past few years.
 The ZY-3 tri-stereo sensors offer users the ability to capture imagery over large regions including an entire territory of a country.
